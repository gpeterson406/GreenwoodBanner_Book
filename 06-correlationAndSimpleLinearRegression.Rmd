---
output:
  pdf_document: 
    keep_tex: yes
  html_document: default
header-includes:
- \usepackage{amsmath}
- \usepackage{color}
---

# Correlation and Simple Linear Regression {#chapter6}

```{r echo=F,warning=F,message=F}
set.seed(3234)
library(pander)
require(mosaic)
```


```{r echo=F}
#Color Format
colFmt = function(x, color){
  outputFormat = opts_knit$get("rmarkdown.pandoc.to")
  if(outputFormat == 'latex')
    paste("\\textcolor{",color,"}{",x,"}",sep="")
  else if(outputFormat == 'html')
    paste("<font color='",color,"'>",x,"</font>",sep="")
  else
    x
}
```

## Relationships between two quantitative variables	{#section6-1}

The independence test in Chapter \@ref(chapter5) provided a technique for assessing evidence of a
relationship between two categorical variables. The terms ***relationship*** and ***association***
are synonyms that, in statistics, imply that values on one variable tend to occur more often with
some other values of the other variable or that knowing
something about the level of one variable provides information about the
patterns of values on the other variable. These terms are not specific to the
"form" of the relationship -- any pattern (strong or weak, negative or positive, 
easily explained or complicated) satisfy the definition. There are two other
aspects to using these terms in a statistical context. First, they are not
directional -- an association between $x$ and $y$ is the same as saying there is an
association between $y$ and $x$. Second, they are not causal unless the levels of
the one of the variables are randomly assigned in an experimental context. We add
to this terminology the idea of correlation between variables $x$ and $y$. 
***Correlation***, in most statistical contexts, is a measure of the specific type 
of relationship between the variables: the **linear relationship between two 
quantitative variables**^[There are measures 
of correlation between categorical variables but when
statisticians say correlation they mean correlation of quantitative variables. 
If they are discussing correlations of other types, they will make that clear.]. 
So as we start to review these ideas from your previous statistics course, 
remember that associations and relationships are more general than correlations
and it is possible to have no correlation where there is a strong relationship
between variables. "Correlation" is used colloquially as a synonym for
relationship but we will work to reserve it for its more specialized usage here
to refer to the linear relationship.

Assessing and then modeling relationships between quantitative variables drives
the rest of the chapters, 
so we should get started with some motivating examples to start to think about
what relationships between quantitative variables "look like"... To motivate
these methods, we will start with a study of the effects of beer consumption on
blood alcohol levels (*BAC*, in grams of alcohol per deciliter of blood). A group 
of $n=16$ student volunteers at The Ohio State University drank a
randomly assigned number of beers^[Some of the details of this study have been lost, 
so we will assume that the
subjects were randomly assigned and that a beer means a can of beer and that
the beer was of regular strength. We don't know if any of that is actually true. 
I have been asking the Dean of Students to repeat this study as an educational
activity at MSU with no success yet.]. 
Thirty minutes later, a police officer measured their *BAC*. Your instincts, especially
as well-educated college students with some chemistry knowledge, should inform
you about the direction of this relationship -- that there is a ***positive 
relationship*** between ``Beers`` and ``BAC``. In other words,  **higher values of
one variable are associated with higher values of the other**. Similarly, 
lower values of one are associated with lower values of the other. 
In fact there are online calculators that tell you how much your *BAC* increases
for each extra beer consumed (for example:
http://www.craftbeer.com/beer-studies/blood-alcohol-content-calculator
if you plug in 1 beer). The increase
in $y$ (``BAC``) for a 1 unit increase in $x$ (here, 1 more beer) is an example of 
***slope coefficient*** that is applicable if the relationship between the 
variables is linear and something that will be
fundamental in what we will call ***simple linear regression model***. In a
simple linear regression model (simple means that there is only one explanatory
variable) the slope is the expected change in the mean response for a one unit
increase in the explanatory variable. You could also use the *BAC* calculator and
the models that we are going to develop to pick a total number of beers you
will consume and get a predicted *BAC*.

Before we get to the specifics of this model and how we measure correlation, we 
should explore the relationship between ``Beers`` and ``BAC`` in a scatterplot. 
Figure \@ref(fig:Figure6-1) shows a ***scatterplot*** of the results that display 
the expected positive relationship. Scatterplots display the response pairs for 
the two quantitative variables with the
explanatory variable on the x-axis and the response variable on the y-axis. The
relationship between ``Beers`` and ``BAC`` appears to be relatively linear but
there is possibly more variability than one might expect. For example, for
students consuming 5 beers, their *BAC*s range from 0.05 to 0.10. If you look
at the online *BAC* calculators, you will see that other factors such as weight,
sex, and beer percent alcohol can impact
the results. We might also be interested in previous alcohol consumption. In
Chapter \@ref(chapter8), we will learn how to estimate the relationship between 
``Beers`` and ``BAC`` after correcting for those "other variables" using 
***multiple linear regression***, where we incorporate more than one
quantitative explanatory variable into the linear model (somewhat like in the
2-Way ANOVA). Some of this variability might be hard or impossible to explain
regardless of the other variables available and is considered unexplained variation
and goes into the residual errors in our models, just like in the ANOVA models. 
To make scatterplots as in Figure \@ref(fig:Figure6-1), we simply use 
``plot(y~x, data=...)``.

```{r echo=F}
set.seed(5678)
```

(ref:fig6-1) Scatterplot of beers consumed versus BAC.

```{r Figure6-1,fig.cap="(ref:fig6-1)",warning=F,message=F}
BB <- read.csv("http://www.math.montana.edu/courses/s217/documents/beersbac.csv")
plot(BAC~Beers, data=BB)
```

There are a few general things to look for in scatterplots:

1. **Assess the** $\underline{\textbf{direction of the relationship}}$ -- is it 
positive or negative? 

2. **Consider the** $\underline{\textbf{strength of the relationship}}$. 
The general idea of assessing strength visually is about how hard or easy it is 
to see the pattern. If it is hard to see a pattern, then it is weak. If it is easy to
see, then it is strong. 

3. **Consider the** $\underline{\textbf{linearity of the relationship}}$. Does it 
appear to curve or does it follow a relatively straight line? Curving relationships are
called ***curvilinear*** or ***nonlinear***  and can be strong or
weak just like linear relationships -- it is all about how tightly the
points follow the pattern you identify. 

4. **Check for** $\underline{\textbf{unusual observations -- outliers}}$ -- by looking 
for points that don't follow the overall pattern. Being large in $x$ or $y$
doesn't mean that the point is an outlier. Being unusual relative to the overall
pattern makes a point an outlier in this setting.

5. **Check for** $\underline{\textbf{changing variability}}$ in one variable based 
on values of the other variable. This will tie into a constant variance assumption 
later in the regression models.

6. **Finally, look for** $\underline{\textbf{distinct groups}}$ in the scatterplot.
This might suggest that observations from two populations, say males and females, 
were combined but the relationship between the two quantitative variables
might be different for the two groups.
 
Going back to Figure \@ref(fig:Figure6-1) it appears that there is a 
moderately strong linear
relationship between ``Beers`` and ``BAC`` -- not weak but with some variability
around what appears to be a straight-line relationship. There might even be a
hint of a nonlinear relationship in the higher beer values. There are no clear
outliers because the observation at 9 beers seems to be following the overall
pattern fairly closely. There is little evidence of non-constant variance
mainly because of the limited size of the data set -- we'll check this with
better plots later. And there are no clearly distinct groups in this plot, mainly
because the # of beers was randomly assigned. These data have one more
interesting feature to be noted -- that subjects managed to consume 8 or 9
beers. This seems to be a large number. I have never been able to trace this
data set to the original study so it is hard to know if (1) they had this study
approved by a human subjects research review board to make sure it was "safe", 
(2) every subject in the study was able to consume their randomly assigned
amount, and (3) whether subjects were asked to show up to the study with *BAC*s
of 0. We also don't know the alcohol concentration of the beer consumed or
volume. But it is still a fun example to start these methods with...

In making scatterplots, there is always a choice of a variable for the 
x-axis and the y-axis. It is our
convention to put explanatory or independent variables (the ones used to
explain or predict the responses) on the x-axis. In studies where the subjects are
randomly assigned to levels of a variable, this is very clearly an explanatory
variable, and we can go as far as making causal inferences with it. In
observational studies, it can be less clear which variable explains which. In
these cases, make the most reasonable choice based on the observed variables but
remember that, when the direction of relationship is unclear, you could have
switched which axes and implication of which variable explains which.
   
## Estimating the correlation coefficient	{#section6-2}

In terms of quantifying relationships between variables, we start with 
the correlation coefficient, a
measure that is the same regardless of your choice of which variable is
considered explanatory or response. We measure the strength and direction of
linear relationships between two quantitative variables using 
***Pearson'?'s r*** or ***Pearson'?'s Product Moment Correlation Coefficient***.
For those who really like acronyms, Wikipedia even suggests calling it 
the PPMCC. However, 
its use is so ubiquitous that the lower case ***r*** or just "correlation
coefficient" are often sufficient to identify that you have used the PPMCC. 
Some of the extra distinctions arise because there are other ways of measuring
correlations in other situations (for example between two categorical
variables), but we will not consider them here. 

The correlation coefficient, ***r***, is calculated as

$$r=\frac{1}{n-1}\Sigma^n_{i=1}\left(\frac{x_i-\bar{x}}{s_x}\right)
\left(\frac{y_i-\bar{y}}{s_y}\right),$$ 

where $s_x$ and $s_y$ are the standard deviations of $x$ and $y$. This 
formula can also be written as

$$r=\frac{1}{n-1}\sum^n_{i=1}z_{x_i}z_{y_i}$$

where $z_{x_i}$ is the z-score (observation minus mean divided by 
standard deviation) for the $i^{th}$ observation on $x$ and $z_{y_i}$
is the z-score for the $i^{th}$ observation on $y$. We won't directly
use this formula, but its contents inform the behavior of ***r***.
First, because it is a sum divided by ($n-1$) it is a bit like
an average -- it combines information across all observations and, like the
mean, is sensitive to outliers. Second, it is a dimension-less measure, meaning
that it has no units attached to it. It is based on z-scores which have units
of standard deviations of $x$ or $y$ so the original units of measurement are
cancelled out going into this calculation. This also means that changing the
original units of measurement, say from Fahrenheit to Celsius or from miles to
km for one or the other variable will have no impact on the correlation. Less
obviously, the formula guarantees that ***r*** is between -1 and 1. It will
attain -1 for a perfect negative linear relationship, 1 for a perfect positive
linear relationship, and 0 for no linear relationship. We are being careful
here to say ***linear relationship*** because you can have a strong nonlinear
relationship with a correlation of 0. For example, consider 
Figure \@ref(fig:Figure6-2). 

(ref:fig6-2) Scatterplot of an amusing relationship that has $r=0$.

```{r Figure6-2,fig.cap="(ref:fig6-2)",echo=F,warning=F,message=F}
x<-seq(from=0,to=20,length.out=20)
y<-(x-mean(x))^2
x<-c(x,5,15)
y<-c(y,220,220)
plot(x,y,col='blue',main="r=0")
```

 
There are some conditions for trusting the results that the 
correlation coefficient provides:

1. Two quantitative variables measured. 

    * This might seem silly, but categorical variables can be coded 
    numerically and a meaningless correlation can be estimated if you 
    are not careful what you correlate. 
    
2. The relationship between the variables is relatively linear.

    * If the relationship is nonlinear, the correlation is meaningless 
    and can be misleading.
    
3. There should be no outliers.

    * The correlation is very sensitive (technically ***not resistant***) 
    to the impacts of certain types of outliers and you should generally 
    avoid reporting the correlation when they are present.
    
    * One option in the presence of outliers is to report the correlation 
    with and without outliers to see how they influence the estimated 
    correlation. 

The correlation coefficient is dimensionless but larger magnitude values
(closer to -1 OR 1) mean stronger linear relationships. A rough interpretation
scale based on experiences working with correlations follows, but this varies
between fields and types of research and variables measured. It depends on the
levels of correlation researchers become used to obtaining, so can even vary
within fields. Use this scale until you develop your own experience:

* $\left|\mathbf{r}\right|<0.3$: weak linear relationship,

* $0.3 < \left|\mathbf{r}\right|<0.7$: moderate linear relationship, 

* $0.7 < \left|\mathbf{r}\right|<0.7$: strong linear relationship, and

* $0.9 < \left|\mathbf{r}\right|<1.0$: very strong linear relationship.  

And again note that this scale only relates to the **linear** aspect of
the relationship between the variables. 

When we have linear relationships between two quantitative variables, 
$x$ and $y$, we can obtain estimated correlations from the ``cor``
function either using ``y~x`` or by running the ``cor`` function^[This 
interface with the ``cor`` function only works after you load the 
``mosaic`` package.] on the entire data set. When you run the ``cor``
function on a data set it produces a ***correlation matrix*** which 
contains a matrix of correlations where you can triangulate the 
variables being correlated by the row and column names, noting
that the correlation between a variable and itself is 1. A matrix of
correlations is useful for comparing more than two variables, discussed below. 

```{r}
require(mosaic)
cor(BAC~Beers, data=BB)
cor(BB)
```

Based on either version of using the function, we find that the correlation
between ``Beers`` and ``BAC`` is estimated to be 0.89. This suggests a 
strong linear relationship between the
two variables. Examples are about the only way to build up enough experience to
become skillful in using the correlation coefficient. Some additional
complications arise in more complicated studies as the next example
demonstrates. 

Gude, Cookson, Greenwood, and Haggerty (2009) explored the relationship 
between average summer
temperature (degrees F) and area burned (natural log of hectares^[The 
natural log ($\log_e$ or $\ln$) is used in statistics so much that the
function in R ``log`` actually takes the natural log and if you want a 
$\log_{10}$ you have to use the function ``log10``. When statisticians 
say log we mean natural log.] = log(hectares)) by wildfires in Montana 
from 1985 to 2007. The ***log-transformation*** is often used to reduce 
the impacts of really large observations on
non-negative (strictly greater than 0) responses with really large observations
(more on ***transformations*** and their impacts on regression models 
in Chapter \@ref(chapter7)). Based on your experiences and before 
analyzing the data, I'm sure
you would assume that summer temperature explains the area burned by wildfires. 
But could it be that more fires are related to having warmer summers? That
second direction is unlikely on a state-wide scale but could apply at a
particular weather station that is near a fire. There is another option -- some
other variable is affecting both variables. For example, drier summers might 
be the real explanatory variable that is related to having both warm summers and lots
of fires. These variables are also being measured over time making them examples
of ***time series***. In statistics, time series data is generally reserved 
for evenly spaced measurements over time, like monthly or yearly measurements. In
this situation, if there are changes over time, they might be attributed to
climate change. So there are really three relationships to explore with the
variables measured here (remembering that the full story might require
measuring even more!): log-area burned versus temperature, temperature versus
year, and log-area burned versus year. 

With more than two variables, we can use the ``cor`` function on all the 
variables and end up getting a matrix of correlations or, simply, the
***correlation matrix***. 

```{r}
mtfires <- read.csv("http://www.math.montana.edu/courses/s217/documents/climateR2.csv")
# natural log transformation of area burned
mtfires$loghectacres <- log(mtfires$hectacres) 

#Cuts the original hectacres data so only log-scale version in data.frame
mtfiresR <- mtfires[,-3] 
cor(mtfiresR)
```

If you triangulate the row and column labels, that cell provides the correlation
between that pair of variables. For example, in the first row (``Year``) 
and the last column (``loghectacres``), you can find that the correlation
coefficient is ***r***=0.362. Note the symmetry in the matrix around the 
diagonal of 1'?'s -- this further illustrates that correlation between 
$x$ and $y$ does not depend on which variable is viewed as the "response".
The estimated correlation
between ``Temperature`` and ``Year`` is -0.004 and the correlation between
``loghectacres`` (*log-hectares burned*) and ``Temperature`` is 0.81. So 
``Temperature`` has almost no linear
change over time. And there is a strong linear relationship between 
``loghectacres`` and ``Temperature``. So it appears that temperatures may 
be related to log-area burned but that the trend over time in both is less 
clear (at least the linear trends). 

The correlation matrix alone is misleading -- we need to explore scatterplots 
to check for nonlinear
relationships, outliers, and clustering of observations that may be distorting
the numerical measure of the linear relationship. The ``pairs.panels``
function from the ``psych`` package (Revelle, 2016) combines the numerical 
correlation information and scatterplots in one display. There are
some options to turn off for the moment but it is an easy function to use to
get lots of information in one place. As in the correlation matrix, you
triangulate the variables for the pairwise relationship. The upper right
diagonal of Figure \@ref(fig:Figure6-3) displays a correlation of 0.36 for 
``Year`` and ``loghectares`` and the lower left panel contains the 
scatterplot with ``Year`` on the x-axis and ``loghectares`` on the y-axis.
The correlation between ``Year`` and ``Temperature`` is really small, both
in magnitude and in display, but appears to be nonlinear (it goes down between
1985 and 1995 and then goes back up), so the correlation coefficient doesn't
mean much here since it just measures the overall linear relationship. We might
say that this is a moderate strength (moderately "clear") curvilinear
relationship. In terms of the underlying climate process, it suggests a
decrease in summer temperatures between 1985 and 1995 and then an increase in
the second half of the data set. 

(ref:fig6-3) Scatterplot matrix of Montana fires data. 

```{r Figure6-3,fig.cap="(ref:fig6-3)",warning=F,message=F}
require(psych) 
pairs.panels(mtfiresR, ellipses=F, scale=T, smooth=F, col=0)
```

As one more example, the Australian Institute of Sport collected data 
on 102 male and 100 female athletes that are available in the ``ais``
data set from the ``alr3`` package (Weisberg, 2005). They measured a 
variety of variables including the athlete'?'s Hematocrit (``Hc``, 
units of percentage of red blood cells in the blood), Body Fat Percentage 
(``Bfat``, units of percentage of total bodyweight), and height (``Ht``,
units of cm). Eventually we might be interested in predicting ``Hc`` 
based on the other variables, but for now the associations are of interest. 

(ref:fig6-4) Scatterplot matrix of athlete data. 

```{r Figure6-4,fig.cap="(ref:fig6-4)",warning=F,message=F}
require(alr3)
data(ais)
aisR <- ais[,c("Ht","Hc","Bfat")]
summary(aisR)
cor(aisR)
pairs.panels(aisR,scale=T,ellipse=F,smooth=F,col=0)
```

``Ht`` (*Height*) and ``Hc`` (*Hematocrit*) have a moderate positive 
relationship that may contain a slightly nonlinearity. It also contains one
clear outlier for a middle height athlete (around 175 cm) with an ``Hc``
of close to 60% (a result that is extremely high). One might wonder about 
whether this athlete has been doping or
if that measurement involved a recording error. We should consider removing
that observation to see how our results might change without it impacting the
results. For the relationship between ``Bfat`` (*bodyfat*) and ``Hc`` 
(*hematocrit*), that same high ``Hc`` value is a clear outlier. There is
also a high ``Bfat`` (*body fat*) athlete (35%) with a somewhat low 
``Hc`` value. This also might be influencing our impressions so we will 
remove both "unusual" values and remake the plot. The two offending
observations were found for individuals numbered 56 and 166 in the data set:

```{r}
aisR[c(56,166),]
```

We can create a reduced version of the data (``aisR2``) by removing those 
two rows using ``[-c(56, 166),]`` and then remake the plot:

(ref:fig6-5) Scatterplot matrix of athlete data with two potential 
outliers removed. 

```{r Figure6-5,fig.cap="(ref:fig6-5)",warning=F,message=F}
aisR2 <- aisR[-c(56,166),] #Removes observations in rows 56 and 166
pairs.panels(aisR2, scale=T, ellipse=F, smooth=F, col=0)
```

After removing these two unusual observations, the relationships between 
the variables are more obvious (Figure \@ref(fig:Figure6-5)). There is a 
moderate strength, relatively linear relationship between *Height* and 
*Hematocrit*. There is almost no relationship between *Height* and 
*Body Fat %* (***r***=-0.20). There is a negative, moderate strength, 
somewhat curvilinear relationship between *Hematocrit* and *Body Fat %*
(***r***=-0.54). As hematocrit increases initially, the body fat 
percentage decreases but at a certain level (around 45% for ``Hc``), the 
body fat percentage seems to
level off. Interestingly, it ended up that removing those two outliers had only
minor impacts on the estimated correlations -- this will not always be the case. 

Sometimes we want to just be able to focus on the correlations, assuming 
we trust that
the correlation is a reasonable description of the results between the
variables. To make it easier to see patterns of positive and negative
correlations, we can employ a different version of the same display from 
the ``corrplot`` package (Wei and Simko, 2016). In this case 
(Figure \@ref(fig:Figure6-6)), it tells much the same story but also allows 
the viewer to easily distinguish both size and direction and read off the 
numerical correlations if desired. 

(ref:fig6-6) Correlation plot of the athlete data with two potential 
outliers removed. Lighter (orange) circle for positive correlations 
and black for negative correlations. 


```{r Figure6-6,fig.cap="(ref:fig6-6)",fig.height=6,fig.width=6}
require(corrplot)
corrplot.mixed(cor(aisR2), col=c("black", "orange"))
```


## Relationships between variables by groups	{#section6-3}

In assessing the relationship
between variables, incorporating information from a third variable can often
enhance the information gathered by either showing that the relationship
between the first two variables is the same across levels of the other variable
or showing that it differs. When the other variable is categorical (or just can
be made categorical), it can be added to scatterplots, changing the symbols and
colors for the points based on the different groups. These techniques are
especially useful if the categorical variable corresponds to potentially
distinct groups in the responses. In the previous example, the data set was
built with male and female athletes. For some characteristics, the
relationships might be the same for both sexes but for others, there are likely
some physiological differences to consider. 

We could continue to use the ``plot`` function here, but it would require 
additional lines of code to add these extra features. The ``scatterplot``
function from the ``car`` package (Fox and Weisberg, 2011) makes it easy 
to incorporate information from an
additional categorical variable. We'll add to our regular formula idea (``y~x``)
the vertical line "|" followed by the categorical variable ``z``, such as 
``y~x|z``. As noted earlier, in statistics, "|" means "to condition on" or, 
here, consider the relationship between $y$ and $x$ by groups in $z$. The other
options are mainly to make it easier to read the information in the plot...
Using this enhanced notation, Figure \@ref(fig:Figure6-7) displays the 
*Height* and *Hematocrit* relationship with information on the sex of the 
athletes where sex was coded 0 for males and 1 for females. 

(ref:fig6-7) Scatterplot of athlete's height and hematocrit by sex of athletes. 
Males were coded as 0s and females as 1s.

```{r Figure6-7,fig.cap="(ref:fig6-7)",message=F,warning=F}
aisR2 <- ais[-c(56,166),c("Ht","Hc","Bfat","Sex")]
require(car)
scatterplot(Hc~Ht|Sex, data=aisR2, pch=c(3,21), reg.line=F, smoother=F,
            boxplots="xy", main="Scatterplot of Height vs Hematocrit by Sex")
```

Adding the grouping information really changes the impressions of the relationship
between *Height* and *Hematocrit* -- within each sex, there is little relationship 
between the two variables. The overall relationship is of
moderate strength and positive but the subgroup relationships are weak at best. 
The overall relationship is created by inappropriately combining two groups
that had different means in both the $x$ and $y$ directions. Men have higher
mean heights and hematocrit values than women and putting them together in one
large group creates the misleading overall relationship. 

To get the correlation coefficients by groups, we can subset the data set using a
logical inquiry on the ``Sex`` variable in the updated ``aisR2`` data set, using 
``Sex==0`` to get the male subjects only and ``Sex==1`` for the female subjects, 
then running the ``cor`` function on each version of the data set:

```{r}
cor(Hc~Ht, data=aisR2[aisR2$Sex==0,]) #Males only
cor(Hc~Ht, data=aisR2[aisR2$Sex==1,]) #Females only
```

These results show that ***r***=-0.05 for *Height* and *Hematocrit* for *males* and
***r***=0.03 for *females*. The first suggests a very weak negative linear
relationship and the second suggests a very weak positive linear relationship. 
The correlation when the two groups were combined (and group information
conclusion here is that correlations on data sets that contain groups can be
very misleading. It also emphasizes the importance of exploring for potential
subgroups in the data set -- these two groups were not obvious in the initial
plot, but with added information the real story became clear. 

For the *Bodyfat* vs *Hematocrit* results in Figure \@ref(fig:Figure6-8), with
an overall correlation of ***r***=-0.54, the subgroup correlations
show weaker relationships that also appear to be in different directions 
(***r***=0.13 for men and ***r***=-0.17 for women). This doubly reinforces the 
dangers of aggregating different groups and
ignoring the group information. 

(ref:fig6-8) Scatterplot of athlete's bodyfat and hematocrit by sex of athletes. Males
were coded as 0s and females as 1s. 

```{r Figure6-8,fig.cap="(ref:fig6-8)",warning=F,message=F}
cor(Hc~Bfat, data=aisR2[aisR2$Sex==0,]) #Males only
cor(Hc~Bfat, data=aisR2[aisR2$Sex==1,]) #Females only
scatterplot(Hc~Bfat|Sex, data=aisR2, pch=c(3,21), reg.line=F, smoother=F,
            boxplots="xy", main="Scatterplot of Bodyfat vs Hematocrit by Sex")
```

One final exploration for these data involves the *body fat *and *height* relationship 
displayed in Figure \@ref(fig:Figure6-9). This relationship shows an even greater
disparity between overall and subgroup results. The overall relationship is characterized
as a weak negative relationship (***r***=-0.20) that is not clearly linear or nonlinear. 
The subgroup relationships are both clearly positive with a stronger relationship for men
that might also be nonlinear (for the linear relationships ***r*** =0.20 for women and
***r*** =0.45 for men). Especially for male athletes, those that are taller seem to have 
higher body fat
percentages. This might be related to the types of sports they compete in --
that would be another categorical variable we could incorporate... Both groups
also seem to demonstrate slightly more variability in *Bodyfat* associated with taller 
athletes (each sort of "fans out"). 

(ref:fig6-9) Scatterplot of athlete's bodyfat and height by sex.

```{r Figure6-9,fig.cap="(ref:fig6-9)",message=F,warning=F}
cor(Bfat~Ht,  data=aisR2[aisR2$Sex==0,]) #Males only
cor(Bfat~Ht,data=aisR2[aisR2$Sex==1,]) #Females only
scatterplot(Bfat~Ht|Sex, data=aisR2, pch=c(3,21), reg.line=F, smoother=F,
            boxplots="xy", main="Scatterplot of Height vs Bodyfat by Sex")
```

In each of these situations, the sex of the athletes has the potential to cause misleading
conclusions if ignored. There are two ways that this could occur -- if we did
not measure it then we would have no hope to account for it OR we could have
measured it but not adjusted for it in our results, as I did initially. We
distinguish between these two situations by defining the impacts of this
additional variable as either a confounding or lurking variable:

* ***Confounding variable:*** affects the response variable and is related to the 
explanatory variable. The impacts of a confounding variable on the response 
variable cannot be separated from the impacts of the explanatory variable.

* ***Lurking variable:*** a potential confounding variable that is not measured 
and is not considered in the interpretation of the study.

Lurking variables show up in studies sometimes due to lack of knowledge of the 
system being studied or a lack of
resources to measure these variables. And sometimes the two variables cannot be
separated even if they are measured... 

To help think about confounding and lurking variables, consider the following 
situation. On many
highways, such as Highway 93 in Montana and north into Canada, recent
construction efforts have been involved in creating safe passages for animals
by adding fencing and animal crossing structures. These structures both can
improve driver safety, save money from costs associated with animal-vehicle
collisions, and increase connectivity of animal populations. Researchers
involved in these projects are interested in which characteristics of
underpasses lead to the most successful structures, mainly measured by rates of
animal usage (number of times they cross under the road). Crossing structures
are typically made using culverts and those tend to be cylindrical. Researchers
are interested in studying the effect of height and width of crossing structures
on animal usage. Unfortunately, all the tallest structures are also the widest
structures. If animals prefer the tall and wide structures, then there is no
way to know if it is due to the height or width of the structure since they are
confounded. If the researcher had only measured width, then they might assume
that it is the important characteristic of the structures but height could be a
lurking variable that really was the factor related to animal usage of the
structures. This is an example where it may not be possible to design a study
that prevents confounding of the two variables *height* and *width*. If the
researchers could control the height and width of the structures, then they
could randomly assign both variables to make sure that some narrow structures
are installed that are tall and some that are short. Additionally, they would
also want to have some wide structures that are short and some are tall. 
Careful design of studies can prevent confounding of variables if they are
known in advance and it is possible to control them, but in observational
studies the observed combinations of variables are uncontrollable. This is why
we need to employ additional caution in interpreting results from observational
studies. 

## Inference for the correlation coefficient (Optional Section)	{#section6-4}

We used bootstrapping briefly in
Chapter \@ref(chapter2) to generate nonparametric confidence intervals based 
on the middle
95% of the bootstrapped version of the statistic. Remember that bootstrapping
involves sampling *with replacement*
from the data set and creates a distribution centered near the statistic from
the real data set. This also mimics sampling under the alternative as opposed
to sampling under the null as in our permutation approaches. Bootstrapping is
particularly useful for making confidence intervals where the distribution of
the statistic may not follow a named distribution. This is the case for the
correlation coefficient which we will see shortly. 

The correlation is an interesting
summary but it is also an estimator of a population parameter called $\rho$
(the symbol rho), which is the ***population correlation coefficient***. When 
$\rho = 1$, we have a perfect positive linear relationship in the population; 
when $\rho=-1$, there is a perfect negative linear relationship in the 
population; and when $\rho=0$, there is no linear relationship in the 
population. Therefore, to test if there is a
linear relationship between two quantitative variables, we use the null
hypothesis $H_0: \rho=0$ (tests if the true correlation, $\rho$, is 0 -- no
linear relationship). The alternative hypothesis is that there is some
(positive or negative) relationship between the variables in the population, 
$H_A: \rho \ne 0$. The distribution of the Pearson correlation coefficient 
can be complicated in some situations, so we will use
bootstrapping methods to generate confidence intervals for $\rho$ based on 
repeated random samples with replacement from the original dataset. If the 
confidence contains 0, then we would fail to reject the null
hypothesis since 0 is in the interval of our likely values for $\rho$. If 
the confidence interval does not contain 0, then we can reject the null 
hypothesis. 

The *beers* and *BAC* example seemed to provide a strong relationship with
***r***=0.89. As correlations approach -1 or 1, the sampling becomes more 
and more skewed. This certainly shows up in the bootstrap distribution that 
the following code produces (Figure \@ref(fig:Figure6-10)). Remember that
bootstrapping utilizes the ``resample`` function applied to the data set to 
create new realizations of the data set by re-sampling
with replacement from those observations. The bold vertical line in 
Figure \@ref(fig:Figure6-10) corresponds to the estimated correlation
***r***=0.89
and the distribution contains a noticeable left skew with a few much smaller
$T^*\text{'s}$ possible in bootstrap samples. The $C\%$ confidence interval is 
found based
on the middle $C\%$ of the distribution or by finding the values that put 
$(100-C)/2$ into each tail of the distribution. 

(ref:fig6-10) Histogram and density curve of the bootstrap distribution of the 
correlation coefficient with
bold vertical line for observed correlation and dashed lines for bounds for 95%
bootstrap confidence interval.

```{r Figure6-10,fig.cap="(ref:fig6-10)",message=F,warning=F}
Tobs <- cor(BAC~Beers, data=BB); Tobs
par(mfrow=c(1,2))
B <- 1000
Tstar<-matrix(NA, nrow=B)
for (b in (1:B)){
  Tstar[b] <- cor(BAC~Beers, data=resample(BB))
}
quantiles <- qdata(Tstar, c(.025,.975)) #95% Confidence Interval
quantiles

hist(Tstar, labels=T)
abline(v=Tobs, col="red", lwd=3)
abline(v=quantiles$quantile, col="blue", lty=2, lwd=3)

plot(density(Tstar), main="Density curve of Tstar")
abline(v=Tobs, col="red", lwd=3)
abline(v=quantiles$quantile, col="blue", lty=2, lwd=3)
```

These results tell us that the bootstrap 95% CI is from 0.75 to 0.96 -- we are 95%
confident that the true correlation between *Beers* and *BAC* in all OSU students 
like those that volunteered is between 0.75 and 0.96. Note that there are no units
on the correlation coefficient. 

We can also use this confidence interval to test for a linear relationship between
these variables. 

* $\boldsymbol{H_0:\rho=0:}$ **There is no linear relationship between** ***Beers**
**and** ***BAC*** **in the population.**

* $\boldsymbol{H_A: \rho \ne 0:}$ **There is a linear relationship between**
***Beers*** **and** ***BAC*** **in the population.**


The 95% confidence level corresponds to a 5% significance level test. The 95% CI is
from 0.75 to 0.96, which does not contain 0, so we can reject the null
hypothesis. We conclude that there is strong evidence to conclude that there is
a linear relationship between *Beers* and *BAC* in OSU students. We'll revisit this
example using the upcoming regression tools to explore the potential for more
specific conclusions about this relationships. For these inferences to be
accurate, we need to be able to trust that the sample correlation is reasonable
for characterizing the relationship between these variables. 

In this situation with randomly assigned levels of $x$ and a rejected null 
hypothesis of no relationship, we can further conclude that changing beer 
consumption **causes** changes in the *BAC*. This is a much stronger conclusion
than we can typically make based on correlation coefficients. Correlations and
scatterplots are enticing for infusing causal interpretations in non-causal
situations. We often repeat the mantra that ***correlation is not causation***
and that generally applies -- except when there is randomization involved in 
the study. It is rarer for
researchers either to assign, or even to be able to assign, levels of
quantitative variables so correlations should be viewed as non-causal unless
the details of the study suggest otherwise. 

## Are tree diameters related to tree heights?	{#section6-5}

In a study at the Upper Flat Creek
study area in the University of Idaho Experimental Forest, a random sample of 
$n=336$ trees was selected from the forest, with measurements recorded on Douglas 
Fir, Grand Fir, Western Red
Cedar, and Western Larch trees. The data set called ``ufc`` is available from the 
``spuRs`` package (Jones, Maillardet, Robinson, Borovkova, and Carnie, 2014) and 
contains ``dbh.cm`` (tree diameter at 1.37 m from the ground, measured in cm) and 
``height.m`` (tree height in meters). The relationship displayed in 
Figure \@ref(fig:Figure6-11) is positive, 
moderately strong with some curvature and increasing variability as the
diameter increases. There do not appear to be groups in the data set but since
this contains four different types of trees, we would want to revisit this plot
by group. 

(ref:fig6-11) Scatterplot of tree heights (m) vs tree diameters (cm).

```{r Figure6-11,fig.cap="(ref:fig6-11)",warning=F,message=F}
require(spuRs) #install.packages("spuRs")
data(ufc)
scatterplot(height.m~dbh.cm, data=ufc, smooth=F, reg.line=F)
```

Of particular interest is an observation with a diameter around 58 cm and a height
of less than 5 m. Observing a tree with a diameter around 60 cm is not unusual
in the data set, but none of the other trees with this diameter had heights
under 15 m. It ends up that the likely outlier is in observation number 168 and
because it is so unusual it likely corresponds to either a damaged tree or a
recording error. 

```{r}
ufc[168,]
```

With the outlier in the data set, the correlation is 0.77 and without it, the
correlation increases to 0.79. The removal does not create a big change because
the data set is relatively large and the *diameter* value is close to the mean of the 
$x\text{'s}$^[Observations at the edge of the $x\text{'s}$ will be called high 
leverage points in Section \@ref(section6-9); this point is a low leverage point 
because it is close to mean of the $x\text{'s}$.] but it has some impact on the 
strength of the correlation. 

```{r}
cor(dbh.cm~height.m, data=ufc)
cor(dbh.cm~height.m, data=ufc[-168,])
```

$$\underline{\textbf{If you skipped Section 6.4, you can skip the rest of this section:}}$$ 

With the outlier included, the bootstrap 95% confidence interval goes from 0.702 to
0.820 -- we are 95% confident that the true correlation between *diameter* and *height*
in the population of trees is between 0.702 and 0.820. When
the outlier is dropped from the data set, the 95% bootstrap CI is 0.752 to 0.827, 
which shifts the lower endpoint of the interval up, reducing the width of the
interval from 0.118 to 0.075. In other words, the uncertainty regarding the
value of the population correlation coefficient is reduced. The reason to
remove the observation is that it is unusual based on the observed pattern, 
which implies an error in data collection or sampling from a population other
than the one used for the other observations and, if the removal is justified, 
it helps us refine our inferences for the population parameter. But measuring
the linear relationship in these data where there is a curve violates one of
our assumptions of using these methods -- we'll see some other ways of detecting
this issue in Section \@ref(section6-10) and we'll try to "fix" this example using
transformations in the Chapter \@ref(chapter7). 

(ref:fig6-12) Bootstrap distributions of the correlation coefficient for the 
full data set (top) and without potential outlier included (bottom) with 
observed correlation (bold line) and bounds for 95% confidence interval 
(dashed lines).

```{r Figure6-12,fig.cap="(ref:fig6-12)",warning=F,message=F,fig.height=7,fig.width=8}
Tobs <- cor(dbh.cm~height.m, data=ufc); Tobs
par(mfrow=c(2,1))
B <- 1000
Tstar <- matrix(NA, nrow=B)
for (b in (1:B)){
  Tstar[b] <- cor(dbh.cm~height.m, data=resample(ufc))
}
quantiles <- qdata(Tstar, c(.025,.975)) #95% Confidence Interval
quantiles
hist(Tstar, labels=T, xlim=c(0.6,0.9),
     main="Bootstrap distribution of correlation with all data")
abline(v=Tobs, col="red", lwd=3)
abline(v=quantiles$quantile, col="blue", lty=2, lwd=3)

Tobs <- cor(dbh.cm~height.m, data=ufc[-168,]); Tobs
Tstar <- matrix(NA, nrow=B)
for (b in (1:B)){
  Tstar[b] <- cor(dbh.cm~height.m, data=resample(ufc[-168,]))
}
quantiles <- qdata(Tstar, c(.025,.975)) #95% Confidence Interval
quantiles
hist(Tstar, labels=T, xlim=c(0.6,0.9),
     main= "Bootstrap distribution of correlation without outlier")
abline(v=Tobs, col="red", lwd=3)
abline(v=quantiles$quantile, col="blue", lty=2, lwd=3)
```



## Describing relationships with a regression model	{#section6-6}

## Least Squares Estimation	{#section6-7}

## Measuring the strength of regressions: R2	{#section6-8}

## Outliers: leverage and influence	{#section6-9}

## Residual diagnostics – setting the stage for inference	{#section6-10}

## Old Faithful discharge and waiting times {#section6-11}

## Chapter summary	{#section6-12}

## Important R code	{#section6-13}

## Practice problems	{#section6-14}

