---
output:
  pdf_document: 
    keep_tex: yes
  html_document: default
header-includes:
- \usepackage{amsmath}
- \usepackage{color}
- \usepackage{ulem}
- \usepackage{amsfonts}
---

# Case studies {#chapter9}

```{r echo=F,warning=F,message=F}
set.seed(3234)
library(pander)
require(mosaic)
library(knitr)
knitr::opts_chunk$set(cache = TRUE)
options(show.signif.stars = FALSE)
```


```{r echo=F}
#Color Format
colFmt = function(x, color){
  outputFormat = opts_knit$get("rmarkdown.pandoc.to")
  if(outputFormat == 'latex')
    paste("\\textcolor{",color,"}{",x,"}",sep="")
  else if(outputFormat == 'html')
    paste("<font color='",color,"'>",x,"</font>",sep="")
  else
    x
}
```


## Overview of material covered	{#section9-1}

At the beginning of the text, we provided a schematic of methods that you would
learn about that was (probably) gibberish. Hopefully, revisiting that same diagram
(Figure \@ref(fig:Figure9-1) will bring back memories of each of the chapters.
Categorical variables create special challenges whether they are explanatory or
response variables. 

(ref:fig9-1) Schematic of methods covered. 

```{r Figure9-1,fig.cap="(ref:fig9-1)",echo=F,warning=F,message=F}
knitr::include_graphics("chapter9_files/image002.png")
```

Every scenario with a quantitative response variable was handled using linear
models. The last material on multiple linear regression modeling tied back to
the One and Two-Way ANOVA models as categorical variables were added to the
models. As both a review and to emphasize the connections, let's connect some 
of the different versions of the general linear model that we considered. 

If we start with the One-Way ANOVA, the referenced-coded model was written out
as:

$$y_{ij}=\alpha + \tau_j + \varepsilon_{ij}.$$

We didn't want to introduce indicator variables at that early stage of the 
material, but we can now write out the same model using our indicator variable
approach from Chapter \@ref(chapter8) for a $J$-level categorical explanatory
variable using $J-1$ indicator variables as:

$$y_i = \beta_0 + \beta_1I_{\text{Level }2,i} + \beta_2I_{\text{Level }3,i} +
\cdots + \beta_{J-1}I_{\text{Level }J,i} + \varepsilon_i.$$

We now know how the indicator variables are either 0 or 1 for each observation
and only one takes in the value 1 (is "turned on") at a time for each response.
We can then equate the general notation from Chapter \@ref(chapter8) with our
specific One-Way ANOVA (Chapter \@ref(chapter3)) notation as follows:

* $\alpha = \beta_0$:

    * The mean for the baseline category was modeled using $\alpha$ which is
    the intercept term in the output that we called $\beta_0$ in the
    regression models. 
    
* For category $j$:

    * From the One-Way ANOVA model:
    
        * $\alpha + \tau_j$
        
    * From the regression model where the only indicator variable that is 1
    is $I_{\text{Level }j,i}$:
    
        * $\beta_0 + \beta_1I_{\text{Level }2,i} + \beta_2I_{\text{Level }3,i} + \cdots + \beta_JI_{\text{Level }J,i} =  \beta_0 + \beta_{j-1}*1$
        
        * $= \beta_0 + \beta_{j-1}$
        
    * So with intercepts being equal, $\beta_{j-1}=\tau_j$.

The ANOVA reference-coding notation was used to focus on the coefficients that
were "turned on" and their interpretation without getting bogged down in the
full power (and notation) of general linear models. The same equivalence is 
possible to equate our work in the Two-Way ANOVA interaction model, 

$$y_{ijk} = \alpha + \tau_j + \gamma_k + \omega_{jk} + \varepsilon_{ijk},$$

with the regression notation from the MLR model with an interaction: 

$$\begin{array}{rc}
y_i=&\beta_0 + \beta_1x_i +\beta_2I_{\text{Level }2,i}+\beta_3I_{\text{Level }3,i}
+\cdots+\beta_JI_{\text{Level }J,i} +\beta_{J+1}I_{\text{Level }2,i}\:x_i \\
&+\beta_{J+2}I_{\text{Level }3,i}\:x_i
+\cdots+\beta_{2J-1}I_{\text{Level }J,i}\:x_i +\varepsilon_i
\end{array}$$

If one of the categorical variables only had two levels, then we could simply
replace $x_i$ with the pertinent indicator variable and be able to equate the
two versions of the notation. That said, we won't attempt that here. And if
both variables have more than 2 levels, the number of coefficients to keep 
track of grows rapidly. The great increase in complexity of notation to fully 
writing out the indicator variables in the regression approach with 
interactions with two categorical variables is the other reason we explored the
Two-Way ANOVA using a "simplified" notation system even though ``lm`` used the
indicator approach to fit the model. The Two-Way ANOVA notation helped us 
distinguish which coefficients related to main effects and the interaction, 
something that the regression notation doesn't make clear. 

In the following three sections, you will have one more chance to see
applications of the methods considered here. The data sets are taken directly 
from published research articles, so you can see the potential utility of the
methods we've been discussing for handling real problems. They are focused on
biological applications because the particular journal (*Biology Letters*) that
all of these were drawn from encourages authors to share their data sets, making
our re-analyses possible. Use these sections to review or to re-inforce methods 
from earlier in the book.

## The impact of simulated chronic nitrogen deposition on the biomass and N2-fixation activity of two boreal feather mossâ€“cyanobacteria associations	{#section9-2}

In a 16-year experiment, @Gundale2013 studied the impacts
of Nitrogen (``N``) additions on the mass of two feather moss species 
(*Pleurozium schreberi *(``PS``) and *Hylocomium* (``HS``) in the Svartberget
Experimental Forest in Sweden. They used a randomized block design: here this 
means that within each of 6 blocks (pre-specified areas that were divided into 
three experimental units or plots of area 0.1 hectacre), one of the three 
treatments were randomly applied. ***Randomized block designs*** involve
randomization of levels within blocks or groups as opposed to ***completely
randomized designs*** where each ***experimental unit*** (the subject or plot
that will be measured) could be randomly assigned to any treatment. This is
done in agricultural studies to control for systematic differences across the
fields by making sure each treatment level is used in each area or ***block***
of the field. 

The three treatments involved different levels of N applied immediately after
snow melt, *Control* (no additional N -- just the naturally deposited 
amount), 12.5 kg N $\text{ha}^{-1}\text{yr}^{-1}$ (*N12.5*), and 
50 kg N $\text{ha}^{-1}\text{yr}^{-1}$ (*N50*). The researchers were 
interested in whether the treatments would have differential impacts on the 
two species of moss growth. They measured a variety of other
variables, but here we focus on the estimated *biomass* per hectare (mg/ha) of
the *species* (*PS* or *HS*), both measured for each plot within each block,
considering differences across the *treatments* (*Control*, *N12.5*, or *N50*).
The beanplot in Figure \@ref(fig:Figure9-2) provides some initial information
about the responses. Initially there seem to be some differences in the
combinations of groups and some differences in variability in the different
groups, especially with much more variability in the *control* treatment level
and more variability in the *PS* responses than for the *HS* responses. 

(ref:fig9-2) Beanplot of biomass responses by treatment and species. 

```{r Figure9-2,fig.cap="(ref:fig9-2)",warning=F,message=F}
gdn <- read.csv("http://www.math.montana.edu/courses/s217/documents/gundalebachnordin_2.csv")
require(beanplot)
beanplot(Massperha~Species+Treatment, data=gdn, side = "b", 
         col=list("white","lightgreen"), xlab="Treatment", ylab="Biomass")
legend("topright", bty="n", c("HS", "PS"), fill=c("white","lightgreen"))
```

The Two-WAY ANOVA model that contains a *species* by *treatment* interaction is
of interest (so this has a quantitative response variable of *biomass* and two
categorical predictors of *species* and *treatment*)^[The researchers did not
do this analysis so never directly addressed this research question although
they did discuss it in general ways. ]. We can make an interaction plot to 
focus on the observed patterns of the means across the combinations of levels
as provided in Figure \@ref(fig:Figure9-3). The interaction
plot suggests a relatively additive pattern of differences between *PS* and 
*HS* across the three treatment levels. However, the variability seems to be 
quite different based on this plot as well. 

(ref:fig9-3) Interaction plot of biomass responses by treatment and species. 

```{r Figure9-3,fig.cap="(ref:fig9-3)",warning=F,message=F}
source("http://www.math.montana.edu/courses/s217/documents/intplot.R")
intplot(Massperha~Species*Treatment, data=gdn, col=c(1,2), lwd=2)
```

Based on the initial plots, we are going to be concerned about the equal
variance assumption initially. We can fit the interaction model and explore
the diagnostic plots to verify that we have a problem. 

(ref:fig9-4) Diagnostic plots of treatment by species interaction model for
Biomass.

```{r Figure9-4,fig.cap="(ref:fig9-4)",warning=F,message=F}
m1 <- lm(Massperha~Species*Treatment, data=gdn)
summary(m1)
par(mfrow=c(2,2), oma=c(0,0,2,0))
plot(m1, sub.caption="Initial Massperha 2-WAY model")
```

There is a clear problem with non-constant variance showing up in a fanning 
shape^[Instructors in this class often get asked what a problem with 
non-constant variance actually looks like -- this is it!]
in the Residuals versus Fitted and Scale-Location plots in Figure 
\@ref(fig:Figure9-4). Interestingly, the normality assumption is not an issue
so hopefully we will not worsen this result by using a transformation to try to
address the non-constant variance issue. The independence assumption is
violated in two ways for this model by this study design -- the blocks create
clusters or groups of observations and the block should be accounted for (they
did this in their models by adding *block* as a categorical variable to their
models). Using blocked designs and accounting for the blocks in the model will
typically give more precise inferences for the effects of interest, the
treatments randomized within the blocks. Additionally, **there are two
measurements on each plot** within block, one for *SP* and one for *HS* and
these might be related (for example, high *HS* biomass might beassociated with
high *SP*) so putting both observations into a model **violates the
independence assumption** at a second level. It takes more advanced statistical
models (called linear mixed models) to see how to fully deal with
this, for now it is important to recognize the issues. The more complicated
models provide similar results here and include the *treatment* by *species*
interaction we are going to explore, they just add to this basic model to
account for these other issues. 

Remember that **before using a *log*-transformation, you always must check
that the responses are strictly greater than 0:**:

```{r warning=F,message=F}
summary(gdn$Massperha)
```

The minimum is 319.1 so it is safe to apply the natural log-transformation to
the response variable (*Biomass*) and repeat the previous plots:

(ref:fig9-5) Beanplot and interaction plot of the log-Biomass responses by 
treatment and species.

```{r Figure9-5,fig.cap="(ref:fig9-5)",warning=F,message=F}
gdn$logMassperha <- log(gdn$Massperha)
par(mfrow=c(1,2))
beanplot(logMassperha~Species+Treatment, data=gdn, side = "b",
         col= list("white","lightgreen"), xlab="Treatment", 
         ylab="log-Biomass", main="(a)")
legend("topright", bty="n", c("HS","PS"), fill=c("white","lightgreen"))
intplot(logMassperha~Species*Treatment, data=gdn, col=c(1,2), lwd=2, main="(b)")
```

The variability in the beanplot in Figure \@ref(fig:Figure9-5)(a) appears to be
more consistent across the groups but the lines appear to be a little less 
parallel in the interaction plot Figure \@ref(fig:Figure9-5)(b) for
the log-scale response. That is not problematic but suggests that we may now
have an interaction present -- it is hard to tell visually sometimes. Again, 
fitting the interaction model and exploring the diagnostics is the best way to
assess the success of the transformation applied. 

The log(Mass per ha) version of the response variable has little issue with
changing variability present in the residuals in Figure \@ref(fig:Figure9-6)
with much more similar variation in the residuals across the fitted values. 

The normality assumption is leaning towards a slight violation with too little
variability in the right tail and so maybe a little bit of a left skew. This
is only a minor issue and fixes the other big issue (clear non-constant 
variance), so this model is at least closer to giving us trustworthy 
inferences than the original model. The model presents some evidence of a 
*Species* by *Treatment* interaction ($F(2,30)=4.2$, p-value$=0.026$). This
suggests that the effects on the log-biomass of the treatments differ between
the two species. The mean log-biomass is lower for *HS* than *PS* with the mean
log-biomass to decrease more rapidly than for *PS*. In other words, increasing
nitrogen has more of an impact on the resulting log-biomass for *HS* than for 
*PS*. The highest mean log-biomass rates were observed under the control 
conditions for both species making nitrogen appear to inhibit growth of these
species. 

(ref:fig9-6) Diagnostic plots of treatment by species interaction model for
log-Biomass. 

```{r Figure9-6,fig.cap="(ref:fig9-6)",warning=F,message=F}
m2 <- lm(logMassperha~Species*Treatment, data=gdn)
summary(m2)
require(car)
Anova(m2)
par(mfrow=c(2,2), oma=c(0,0,2,0))
plot(m2, sub.caption="log-Massperha 2-WAY model")
```

The researchers actually applied a $\log(y+1)$ transformation to all the 
variables. This was used because one of their many variables had a value of 0
and so they added 1 to avoid analyzing a $-\infty$ response. This was not 
needed for most of their variables because most did not attain the value 
of 0. Adding a small value to observations and then log-transforming is a 
common but completely arbitrary practice and the choice of the added value can
impact the results. Sometimes considering a square-root transformation can 
accomplish similar benefits as the log-transform and be applied safely to 
responses that include 0s. Or more complicated statistical models can be used
that allow 0s in responses and still account for the violations of the linear 
model assumptions -- see a statistician or continue exploring more advanced
statistical methods for ideas in this direction. 

The term-plot in Figure \@ref(fig:Figure9-7) provides another display of the
results with some information on the results for each combination of the 
species and treatments. Finding evidence that the treatments caused different
results for the different species is a good first start. And it appears that 
there are some clear differences among certain combinations such as the mean
for *PS-Control* is clearly larger than for *HS*-*N50*. The researchers were 
probably really interested in whether the *N12.5* results differed from 
*Control* for *HS* and whether the *species* differed at *Control* sites. As 
part of performing all pair-wise comparisons, we can assess those sorts of 
detailed questions. This sort of follow-up could be considered in any
Two-Way ANOVA model but will be most interesting in situations where there are
important interactions. 

(ref:fig9-7) Term-plot of the interaction model for log-biomass.

```{r Figure9-7,fig.cap="(ref:fig9-7)",warning=F,message=F}
require(effects)
plot(allEffects(m2), multiline=T, ci.style="bars")
```

**Follow-up Pairwise Comparisons:**

Given strong evidence of an interaction, many researchers would like more
details about the source of the differences. We can re-fit the model with a 
unique mean for each combination of the two predictor variables, fitting a 
One-Way ANOVA model (here with six levels) and using Tukey's HSD to provide 
safe inferences for differences among pairs of the true means. There are six 
groups corresponding to all combinations of *Species* (*HS*, *PS*) and
treatment levels (*Control*, *N12.5*, and *N50*) provided in the new 
variable ``SpTrt`` by the ``interaction`` function with new levels of 
*HS.Control*, *PS.Control*, *HS.N12.5*, *PS.N12.5*, *HS.N50*, and *PS.N50*. The
One-Way ANOVA $F$-test ($F(5,30)=23.96$, p-value$<0.0001$) suggests that there
is evidence of some difference in the true mean log-biomass among the six
treatment combinations. Note that the One-Way ANOVA table contains the test for
at least one of those means being different from the others; the interaction
test above was testing a more refined hypothesis -- does the effect of
treatment differ between the two species? With a small p-value from the overall
One-Way ANOVA test, the pair-wise comparisons should be of interest. 

```{r warning=F,message=F}
#Create new variable:
gdn$SpTrt <- interaction(gdn$Species, gdn$Treatment)
levels(gdn$SpTrt)
newm2 <- lm(logMassperha~SpTrt, data=gdn)
Anova(newm2)
require(multcomp)
PWnewm2 <- glht(newm2, linfct=mcp(SpTrt="Tukey"))
confint(PWnewm2)
```

We can also generate the Compact Letter Display (CLD) to help us group up the
results. 

```{r warning=F,message=F}
cld(PWnewm2)
```

We can also add it to an interaction plot to create Figure 
\@ref(fig:Figure9-8). Researchers often use displays like this to simplify the
presentation of pair-wise comparisons. Sometimes researchers add bars or stars
to provide the same information about pairs that are or are not detectably
different. The following code creates the plot of these results using our 
``intplot`` function and the ``cld=T`` option. 

(ref:fig9-8) Interaction plot for log-biomass with CLD from Tukey's HSD for all
pairwise comparisons.

```{r Figure9-8,fig.cap="(ref:fig9-8)",warning=F,message=F}
intplot(logMassperha~Species*Treatment, cld=T, cldshift=0.15,
        cldcol=c(2,3,4,5,6,8), data=gdn, lwd=2, 
        main="Interaction with CLD from Tukey's HSD on One-Way ANOVA")
```

These results suggest that *HS-N50* is detectably different from all the other
groups (letter "a"). The rest of the story is more complicated since many of
the sets contain overlapping groups in terms of detectable differences. Some 
specific aspects of those results are most interesting. The mean log-biomasses
were not detectably different between the (they share a "d"). In other words,
without treatment, there is no evidence of a difference in how much of the two
species are present in the sites. For *N12.5* and *N50* treatments, there are
detectable differences between the *Species*. These comparisons are probably of
the most interest initially and suggest that the treatments have a different
impact on the two species, remembering that in the control treatments, the
results for the two species were not detectably different. Further explorations
of the sizes of the differences that can be extracted from selected confidence
intervals in the Tukey's HSD results printed above. 

## Ants learn to rely on more informative attributes during decision-making	{#section9-3}

In @Sasaki2013, a set of ant colonies were randomly assigned to one
of two treatments to study whether the ants could be "trained" to have a
preference for or against certain attributes for potential nest sites. The
colonies were either randomly assigned to experience the repeated choice of two
identical colony sites except for having an inferior light or entrance size
attribute. Then the ants were allowed to choose between two nests, one that had
a large entrance but was dark and the other that had a small entrance but was
bright. 54 of the 60 colonies that were randomly assigned to one of the two
treatments completed the experiment by making a choice between the two types of
sites. The data set and some processing code follows. 

The first question of interest is what type of analysis is appropriate here.
Once we recognize that there are two categorical variables being considered
(*Treatment* group with two levels and *After* treatment choice with two levels
*SmallBright* or *LargeDark*), then this is recognized as being within our
Chi-square testing framework. The random assignment of colonies (the subjects 
here) to treatment levels tells us that the ***Chi-square Homogeneity test***
is appropriate here and that we can make causal statements if we find evidence
of differences in the patterns of responses. 

(ref:fig9-9) Stacked bar chart for Ant Colony results.

```{r Figure9-9,fig.cap="(ref:fig9-9)",warning=F,message=F}
sasakipratt <- read.csv("http://www.math.montana.edu/courses/s217/documents/sasakipratt.csv")
sasakipratt$group <- factor(sasakipratt$group)
levels(sasakipratt$group) <- c("Light","Entrance")
sasakipratt$after <- factor(sasakipratt$after)
levels(sasakipratt$after) <- c("SmallBright","LargeDark")
sasakipratt$before <- factor(sasakipratt$before)
levels(sasakipratt$before) <- c("SmallBright","LargeDark")
plot(after~group, data=sasakipratt)
```

```{r warning=F,message=F}
require(mosaic)
tally(~group+after, data=sasakipratt)
table1 <- tally(~group+after, data=sasakipratt, margins=F)
```

The null hypothesis of interest here is that there is no difference in the
distribution of responses on *After* -- the rates of their choice of den types
-- between the two treatment *groups* in the population of all ant colonies
like those studied. The alternative is that there is some difference in the
distributions of *After* between the *groups* in the population. 

To use the Chi-square distribution to find a p-value for the $X^2$ statistic,
we need all the expected cell counts to be larger than 5, so we should check 
that. Note that in the following, the ``correct=F`` option is used to keep the
function from slightly modifying the statistic used that occurs when overall 
sample sizes are small. 

```{r warning=F,message=F}
chisq.test(table1, correct=F)$expected
```

Our expected cell count condition is met, so we can proceed to explore the 
results of the test:

```{r warning=F,message=F}
chisq.test(table1, correct=F)
```

The $X^2$ statistic is 5.97 which, if our assumptions are met, should 
approximately follow a Chi-squared distribution with $(R-1)*(C-1)=1$ degrees of
freedom under the null hypothesis. The p-value is 0.015, suggesting that there
is good evidence against the null hypothesis. We can conclude that there is a
difference in the distribution of the responses between the two treated groups
in the population of all ant colonies that could have been treated. Because of
the random assignment, we can say that the treatments caused differences in the
colony choices. These results cannot be extended to ants beyond those being
studied by these researchers because they were not randomly selected. 

Further exploration of the standardized residuals can provide more insights in
some situations, although here they are similar for all the cells:

```{r warning=F,message=F}
chisq.test(table1,correct=F)$residuals
```

When all the standardized residual contributions are similar, that suggests
that there are differences in all the cells from what we would expect if the
null hypothesis were true. Basically, that means that what we observed is a bit
larger than expected for the *Light* treatment group in the *SmallBright*
choice and lower than expected in *LargeDark* -- those treated ants preferred
the small and bright den. And for the *Entrance* treated group, they preferred
the large entrance, dark den at a higher rate than expected if the null is 
true and lower than expected in the small entrance, bright location. 

The researchers extended this basic result a little further using a statistical
model called ***logistic regression***, which involves using something like a
linear model but with a categorical response variable (well -- it actually only
works for a two-category response variable). They also had measured which of
the two types of dens that each colony chose before treatment and controlled
for that choice. So the actual model used in their paper contained two
predictor variables -- the randomized treatment received that we explored here
and the prior choice of den type. The interpretation of their results related
to the same treatment effect, but they were able to discuss it after adjusting
for the colonies previous selection. Their conclusions were similar to those
found with our simpler analysis. Logistic regression models are a special case
of what are called generalized linear models and are a topic for the next level
of statistics if you continue exploring.

## Multi-variate models are essential for understanding vertebrate diversification in deep time	{#section9-4}

@Benson2012 published a paleontology study that considered
modeling the diversity of *Sauropodomorphs* across $n=26$ "stage-level" time
bins. Diversity is measured by the count of the number of different species
that have been found in a particular level of fossils. Specifically, the counts
in the *Sauropodomorphs* group were obtained fora stage between *Carnian* and
*Maastrichtian*, with the first three stages in the *Triassic*, the next ten in
the *Jurassic*, and the last eleven concerned about variation in sampling
efforts and the ability of paleontologists to find fossils across different
stages creating a false impression of the changes in biodiversity over time.
They first wanted to see if the species counts were related to factors such as
the count of dinosaur-bearing-formations (*DBF*) and the count of
dinosaur-bearing-collections (*DBC*) that have been identified for each period.
The thought is that if there are more formations or collections of fossils from
certain stages, the diversity might be better counted (more found of those
available to find) and those stages with less information available might be
under-counted. They also measured the length of each stage (*Duration*) but did
not consider it in their models since they want to reflect the diversity and
longer stages would likely have higher diversity. 

Their main goal was to develop a model that would **control for** the effects
of sampling efforts and allow them to perform inferences for whether the
diversity was different between the *Triassic/Jurassic* (grouped together) and
considered models that included two different versions of sampling effort
variables and one for the comparisons of periods (an indicator variable 
*TJK*=0 if the observation is in *Triassic* or *Jurassic* or 1 if in 
*Cretaceous*). They *log-e* transformed all their quantitative variables
because the untransformed variables created diagnostic issues including
influential points. They explored a model just based on the *DBC*
predictor^[This was not even close to their top AIC model so they made an odd
choice.] and they analyzed the residuals from that model to see if the
biodiversity was different in the *Cretaceous* or before, finding a 
"p-value **>=** 0.0001" (I think they meant <0.0001^[I had students read this
paper in a class and one decided that this was a reasonable way to report small
p-values -- it is WRONG. We are interested in how small a p-value might be and
saying it is over a value is never useful.]). They were comparing the MLR
models you learned to some extended regression models that incorporated a 
correction for correlation in the responses over time, but we can proceed with
fitting some of their MLR models and using an AIC comparison similar to what
they used. There are some obvious flaws in their analysis and results that we
will avoid^[All too often, I read journal articles that have under-utilized, 
under-reported, mis-applied, or mis-interpreted statistical methods and
results. One of the reasons that I wanted to write this book is to help more
people move from basic statistical knowledge to correct use of intermediate
statistical methods and beginning to see the potential in more advanced
statistical methods. It has taken me many years of being a statistician just to
feel armed for battle when confronted with new applications and two stat
courses are not enough to get you there, but you have to start somewhere. You
are only a couple of hundred hours into your 10,000 hours required for mastery. 
This presentation is intended get you some solid fundamentals to build on or a
few intermediate tools to use if this is your last statistics training
experience.]. 

First, we start with a plot of the log-diversity vs the log-dinosaur bearing
collections by period. We can see fairly strong positive relationships between
the log amounts of collections and species found with potentially similar
slopes for the two periods but what look like different intercepts. Especially
for *TJK* level 1 (*Cretaceous* period) observations, we might need to worry
about a curving relationship. Note that a similar plot can also be made using
the formations version of the quantitative predictor variable and that the
research questions involve whether *DBF* or *DBC* are better predictor
variables. 

(ref:fig9-10) Scatterplot of log-biodiversity vs log-DBCs by TJK.

```{r Figure9-10,fig.cap="(ref:fig9-10)",warning=F,message=F}
bm <- read.csv("http://www.math.montana.edu/courses/s217/documents/bensonmanion.csv")
bm2 <- bm[,-c(9:10)]
require(psych)
pairs.panels(bm2, ellipses=F)
bm$logSpecies <- log(bm$Species)
bm$logDBCs <- log(bm$DBCs)
bm$logDBFs <- log(bm$DBFs)
bm$TJK <- factor(bm$TJK)
scatterplot(logSpecies~StageNumber, data=bm, spread=F)
```

The following results will allow us to explore models similar to theirs. One
"full" model they considered is:

$$\log{(\text{count})}_i=\beta_0 + \beta_1\log{(\text{DBC})}_i 
+ \beta_2I_{\text{TJK},i} + \varepsilon_i$$

which was compared to 

$$\log{(\text{count})}_i=\beta_0 + \beta_1\log{(\text{DBF})}_i 
+ \beta_2I_{\text{TJK},i} + \varepsilon_i$$

as well as the simpler models that each suggests:

$$\begin{array}{rl}
\log{(\text{count})}_i &=\beta_0 + \beta_1\log{(\text{DBC})}_i 
+ \varepsilon_i, \\
\log{(\text{count})}_i &=\beta_0 + \beta_1\log{(\text{DBF})}_i
+ \varepsilon_i, \\
\log{(\text{count})}_i &=\beta_0 + \beta_2I_{\text{TJK},i} 
+ \varepsilon_i, \text{ and} \\
\log{(\text{count})}_i &=\beta_0 + \varepsilon_i.  \\
\end{array}$$

Both versions of the models (based on *DBF* or *DBC*) start with an MLR model
with a quantitative variable and two slopes. We can obtain some of the needed
model selection results from the first full model using:

```{r warning=F,message=F}
bd1 <- lm(logSpecies~logDBCs+TJK, data=bm)
require(MuMIn)
options(na.action = "na.fail")
dredge(bd1, rank="AIC", 
       extra=c("R^2", adjRsq=function(x) summary(x)$adj.r.squared))
```

And from the second model:

```{r warning=F,message=F}
bd2 <- lm(logSpecies~logDBFs+TJK, data=bm)
dredge(bd2, rank="AIC",
       extra=c("R^2", adjRsq=function(x) summary(x)$adj.r.squared))
```

The top AIC model is 
$\log{(\text{count})}_i=\beta_0 + \beta_1\log{(\text{DBC})}_i + \beta_2I_{\text{TJK},i} + \varepsilon_i$ 
with an AIC of 33.3. The next best
model was $\log{(\text{count})}_i=\beta_0 + \beta_1\log{(\text{DBF})}_i + \beta_2I_{\text{TJK},i} + \varepsilon_i$ 
with an AIC of 36.8, so 3.5 AIC units worse than the top model. We put these
two runs of results together in Table \@ref(tab:Table9-1), re-computing all the
AICs based on the top model from the first full model considered. 

(ref:tab9-1) Model comparision table.

```{r Table9-1,echo=F,warning=F,message=F}
columnHeaders <- c("Model","$\\boldsymbol{R^2}$", "adj$\\boldsymbol{R^2}$",
                   "df","logLik","AIC","$\\boldsymbol{\\Delta}$AIC")
cell11 <- "$\\small{\\log(\\text{count})_i=\\beta_0 + \\beta_1\\log(\\text{DBC})_i + \\beta_2I_{\\text{TJK},i} + \\varepsilon_i}$"
cell21 <- "$\\small{\\log(\\text{count})_i=\\beta_0 + \\beta_1\\log(\\text{DBF})_i + \\beta_2I_{\\text{TJK},i} + \\varepsilon_i}$"
cell31 <- "$\\small{\\log(\\text{count})_i=\\beta_0 + \\beta_1\\log(\\text{DBC})_i + \\varepsilon_i}$"
cell41 <- "$\\small{\\log(\\text{count})_i=\\beta_0 + \\beta_1\\log(\\text{DBF})_i + \\varepsilon_i}$"
cell51 <- "$\\log(\\text{count})_i=\\beta_0 + \\varepsilon_i$"
cell61 <- "$\\log(\\text{count})_i=\\beta_0 + \\beta_2I_{\\text{TJK},i} + \\varepsilon_i$"

row1 <- c(cell11,0.5809,0.5444,4,-12.652,33.3,0)
row2 <- c(cell21,0.5199,0.4781,4,-14.418,36.8,3.5)
row3 <- c(cell31,0.3691,0.3428,3,-17.969,41.9,8.6)
row4 <- c(cell41,0.2098,0.1769,3,-20.895,47.8,14.5)
row5 <- c(cell51,0,0,2,-23.956,51.9,18.6)
row6 <- c(cell61,0.0048,-0.03664,3,-23.893,53.8,20.5)

x <- matrix(c(row1,row2,row3,row4,row5,row6),ncol = 7, byrow = T)
kable(x,col.names=columnHeaders,caption="(ref:tab9-1)")
```


Table \@ref(tab:Table9-1) suggests some interesting leads to the worst 
performing model on the AIC measure, ranking below a model with nothing in it
(mean-only) and 20.5 AIC units worse than the top model. But the two top models
distinctly benefit from the inclusion of *TJK*. This suggests that after
controlling for the sampling effort, either through *DBC* or *DBF*, the
differences in the stages captured by *TJK* can be more clearly observed. 

So the top model in our (correct) results^[They also had an error in their AIC
results that is difficult to explain here but was due to an un-careful usage of
the results from the more advanced models that account for autocorrelation,
which seems to provide the proper ranking of models (*that they ignored*) but
did not provide the correct differences among models.] suggests an effect of
*log(DBC)* and different intercepts for the two periods. We can interrogate
this model further but probably should do something we should have done first,
check the diagnostics (Figure \@ref(fig:Figure9-11)) and consider our model
assumptions as AICs are not valid if model assumptions are not met. 

(ref:fig9-11) Diagnostic plots for the top AIC model.

```{r Figure9-11,fig.cap="(ref:fig9-11)",warning=F,message=F}
par(mfrow=c(2,2), oma=c(0,0,2,0))
plot(bd1)
```

The constant variance and assessment of influence do not suggest any real
problems. The normality assumption is possibly violated but shows lighter
tails than expected from a normal distribution and so should cause few
problems with inferences (we would be looking for an answer of "yes, there is a
violation of the normality assumption but, for a bonus point, that problem is 
minor because the pattern is not the problematic type of violation 
because..."). The other assumption that **is violated for all our models** is
that the observations are independent. Between neighboring stages in time,
there would likely be some sort of relationship in the biodiversity so
we should not assume that the observations are independent (this is a
***timeseries*** of observations). The authors acknowledged this issue but
unskillfully attempted to deal with it. Because an interaction was not 
considered in any of the models, there also is an assumption that the results
are parallel enough for the two groups. The scatterplot in Figure 
\@ref(fig:Figure9-10) suggests that using parallel lines for the two
groups is probably reasonable but a full assessment really should also explore
that fully to verify that there is no support for an interaction. 

Ignoring the violation of the independence assumption, we are otherwise ok to
explore the model more and see what it tells us about biodiversity of
*Sauropodomorphs*. The top model is estimated to be
$\log{(\widehat{\text{count}})}_i=-1.089 + 0.724\log{(\text{DBC})}_i -0.75I_{\text{TJK},i}$. 
This suggests that for the early observations (*TJK*=0), the model is
$\log{(\widehat{\text{count}})}_i=-1.089 + 0.724\log{(\text{DBC})}_i$
and for the Cretaceous period (*TJK*=1), the model is
$\log{(\widehat{\text{count}})}_i=-1.089 + -0.75+0.724\log{(\text{DBC})}_i$
which simplifies to $\log{(\widehat{\text{count}})}_i=-1.84 + 0.724\log{(\text{DBC})}_i$. This suggests that the sampling efforts
have the same effect on all observations and having an increase in *logDBCs*
is associated with increases in the mean *log-biodiversity*. Specifically, for 
a 1 log-count increase in the *log-DBCs*, we expect, on average, to have a 
0.724 log-count change in the mean log-biodiversity, after accounting for 
different intercepts for the two periods considered. We could also translate 
this to the original count scale but will leave it as is, because their real
question of interest involves the differences between the periods. The change 
in the y-intercepts of -0.76 suggests that the Cretaceous has a lower average
log-biodiversity by 0.75 log-count, after controlling for the log-sampling 
effort. This suggests that the *Cretaceous* had a lower corrected mean
log-Sauropodomorph biodiversity 
$\require{enclose}(t_{23}=-3.41;\enclose{horizontalstrike}{\text{p-value}=0.0024})$
than the combined
results for the Triassic and Jurassic. On the original count scale, this 
suggests $\exp(-0.76)=0.47$ times (53% drop in) the median biodiversity count 
per stage for Cretaceous versus the prior time period, after correcting for
log-sampling effort in each stage. 

```{r warning=F,message=F}
summary(bd1)
```

Their study shows some interesting contrasts between methods. They tried to use
AIC-based model selection methods across all the models but then used p-values
to really make their final conclusions. This presents a philosophical
inconsistency that bothers some more than others but should bother everyone. 
One thought is whether they needed to use AICs at all since they wanted to use
p-values? The one reason they might have preferred to use AICs is that it allows
the direct comparison of

$$\log{(\text{count})}_i=\beta_0 + \beta_1\log{(\text{DBC})}_i + \beta_2I_{\text{TJK},i} + \varepsilon_i$$

to 
$$\log{(\text{count})}_i=\beta_0 + \beta_1\log{(\text{DBF})}_i + \beta_2I_{\text{TJK},i} + \varepsilon_i,$$

exploring whether *DBC* or *DBF* is the "better" with *TJK* in the model. There
is no hypothesis test to compare these two models because one is not ***nested***
in the other -- **it is not possible to get from one model to the other by 
setting one or more slope coefficients to 0 so we can't test our way from one
model to the other one**. The AICs suggest that the model with *DBC* and *TJK* is
better than the model with *DBF* and *TJK*, so that helps us make that decision.
After that step, we could rely on $t$-tests or ANOVA $F$-tests to decide whether
further refinement is suggested/possible for the model with *DBC* and *TJK*. This
would provide the direct inferences that they probably want and are trying to
obtain from AICs along with p-values in their paper.

Finally, their results would actually be more valid if they had used a set of
statistical methods designed for modeling responses that are counts of events
or things, especially those whose measurements change as a function of sampling
effort; models called ***Poisson rate models*** would be ideal for their
application. The other aspect of the biodiversity that they measured
for each stage was the duration of the stage. They never incorporated that
information and it makes sense given their interests in comparing biodiversity
across stages, not understanding why more or less biodiversity might occur. But
other researchers might want to estimate the biodiversity after also
controlling for the length of time that the stage lasted and the sampling
efforts involved in detecting the biodiversity of each stage, models that are
only a few steps away from those considered here. In general, this study
presents some of the pitfalls of attempting to use advanced statistical methods
as well as hinting at the benefits. The statistical models are the only way to
access the results of interest; inaccurate usage of statistical models can
provide inaccurate conclusions. They seemed to mostly get the right answers
despite a suite of errors in their work. 

## General summary	{#section9-5}

As we wrap up, it is important to remember that these tools are limited by the
quality of the data collected. If you are ever involved in applying these
statistical models, whether in a research or industrial setting, make sure that
the research questions are discussed before data collection. And before data
collection is started, make sure that the methods will provide results that can
address the research questions. And, finally, make sure someone involved in the
project knows how to perform the appropriate statistical analysis. One way to
make sure you know how to analyze a data set and, often, clarify the research
questions and data collection needs, is to **make a fake data set and analyze
it**. With this sort of preparation, many issues can be avoided. Remember to 
think about reasons why assumptions of your proposed method might be violated.

You are now **armed** and a bit **dangerous** with statistical methods. If
you go to use them, remember the fundamentals and find the story in the data. 
After deciding on any research questions of interest, graph the data and make
sure that the statistical methods will give you results that make some sense
based on the graphical results. In the MLR results, it is possible that graphs
will not be able to completely tell you the story, but all the other methods
should follow the pictures you see. Even when (or especially when) you use
sophisticated statistical methods, graphical presentations are critical to
helping others understand the results. We have discussed examples that involve
displaying categorical and quantitative variables and even some displays that
bridge both types of variables. We hope you have enjoyed this material and been
able to continue to develop your interests in statistics. You will see it in
many future situations both in courses in your area of study and in real
problems that need answers. You are also prepared to take more advanced
statistics courses -- if you want to discuss the next options, we are happy to
provide additional information about the best next steps in learning
statistics. 
