<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>A Second Semester Statistics Course with R</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="A Second Semester Statistics Course with R">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="A Second Semester Statistics Course with R" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="gpeterson406/GreenwoodBanner_Book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="A Second Semester Statistics Course with R" />
  
  
  

<meta name="author" content="Mark Greenwood and Katherine Banner">


<meta name="date" content="2017-06-27">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="chapter3.html">
<link rel="next" href="chapter5.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="chapter" data-level="1" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i><b>1</b> Preface</a><ul>
<li class="chapter" data-level="1.1" data-path="preface.html"><a href="preface.html#overview-of-methods"><i class="fa fa-check"></i><b>1.1</b> Overview of methods</a></a></li>
<li class="chapter" data-level="1.2" data-path="preface.html"><a href="preface.html#getting-started-in-r"><i class="fa fa-check"></i><b>1.2</b> Getting started in R</a></li>
<li class="chapter" data-level="1.3" data-path="preface.html"><a href="preface.html#basic-summary-statistics-histograms-and-boxplots-using-r"><i class="fa fa-check"></i><b>1.3</b> Basic summary statistics, histograms, and boxplots using R</a></li>
<li class="chapter" data-level="1.4" data-path="preface.html"><a href="preface.html#chapter-summary"><i class="fa fa-check"></i><b>1.4</b> Chapter summary</a></li>
<li class="chapter" data-level="1.5" data-path="preface.html"><a href="preface.html#important-r-code"><i class="fa fa-check"></i><b>1.5</b> Important R Code</a></li>
<li class="chapter" data-level="1.6" data-path="preface.html"><a href="preface.html#practice-problems"><i class="fa fa-check"></i><b>1.6</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chapter2.html"><a href="chapter2.html"><i class="fa fa-check"></i><b>2</b> (R)e-Introduction to statistics</a><ul>
<li class="chapter" data-level="2.1" data-path="chapter2.html"><a href="chapter2.html#section2-1"><i class="fa fa-check"></i><b>2.1</b> Histograms, boxplots, and density curves</a></li>
<li class="chapter" data-level="2.2" data-path="chapter2.html"><a href="chapter2.html#section2-2"><i class="fa fa-check"></i><b>2.2</b> Beanplots</a></li>
<li class="chapter" data-level="2.3" data-path="chapter2.html"><a href="chapter2.html#section2-3"><i class="fa fa-check"></i><b>2.3</b> Models, hypotheses, and permutations for the 2 sample mean situation</a></li>
<li class="chapter" data-level="2.4" data-path="chapter2.html"><a href="chapter2.html#section2-4"><i class="fa fa-check"></i><b>2.4</b> Permutation testing for the 2 sample mean situation</a></li>
<li class="chapter" data-level="2.5" data-path="chapter2.html"><a href="chapter2.html#section2-5"><i class="fa fa-check"></i><b>2.5</b> Hypothesis testing (general)</a></li>
<li class="chapter" data-level="2.6" data-path="chapter2.html"><a href="chapter2.html#section2-6"><i class="fa fa-check"></i><b>2.6</b> Connecting randomization (nonparametric) and parametric tests</a></li>
<li class="chapter" data-level="2.7" data-path="chapter2.html"><a href="chapter2.html#section2-7"><i class="fa fa-check"></i><b>2.7</b> Second example of permutation tests</a></li>
<li class="chapter" data-level="2.8" data-path="chapter2.html"><a href="chapter2.html#section2-8"><i class="fa fa-check"></i><b>2.8</b> Confidence intervals and bootstrapping</a></li>
<li class="chapter" data-level="2.9" data-path="chapter2.html"><a href="chapter2.html#section2-9"><i class="fa fa-check"></i><b>2.9</b> Bootstrap confidence intervals for difference in GPAs</a></li>
<li class="chapter" data-level="2.10" data-path="chapter2.html"><a href="chapter2.html#section2-10"><i class="fa fa-check"></i><b>2.10</b> Chapter summary</a></li>
<li class="chapter" data-level="2.11" data-path="chapter2.html"><a href="chapter2.html#section2-11"><i class="fa fa-check"></i><b>2.11</b> Summary of important R code</a></li>
<li class="chapter" data-level="2.12" data-path="chapter2.html"><a href="chapter2.html#section2-12"><i class="fa fa-check"></i><b>2.12</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="chapter3.html"><a href="chapter3.html"><i class="fa fa-check"></i><b>3</b> One-Way ANOVA</a><ul>
<li class="chapter" data-level="3.1" data-path="chapter3.html"><a href="chapter3.html#section3-1"><i class="fa fa-check"></i><b>3.1</b> Situation</a></li>
<li class="chapter" data-level="3.2" data-path="chapter3.html"><a href="chapter3.html#section3-2"><i class="fa fa-check"></i><b>3.2</b> Linear model for One-Way ANOVA (cell-means and reference-coding)</a></li>
<li class="chapter" data-level="3.3" data-path="chapter3.html"><a href="chapter3.html#section3-3"><i class="fa fa-check"></i><b>3.3</b> One-Way ANOVA Sums of Squares, Mean Squares, and F-test</a></li>
<li class="chapter" data-level="3.4" data-path="chapter3.html"><a href="chapter3.html#section3-4"><i class="fa fa-check"></i><b>3.4</b> ANOVA model diagnostics including QQ-plots</a></li>
<li class="chapter" data-level="3.5" data-path="chapter3.html"><a href="chapter3.html#section3-5"><i class="fa fa-check"></i><b>3.5</b> Guinea pig tooth growth One-Way ANOVA example</a></li>
<li class="chapter" data-level="3.6" data-path="chapter3.html"><a href="chapter3.html#section3-6"><i class="fa fa-check"></i><b>3.6</b> Multiple (pair-wise) comparisons using Tukey’s HSD and the compact letter display</a></li>
<li class="chapter" data-level="3.7" data-path="chapter3.html"><a href="chapter3.html#section3-7"><i class="fa fa-check"></i><b>3.7</b> Pair-wise comparisons for Prisoner Rating data</a></li>
<li class="chapter" data-level="3.8" data-path="chapter3.html"><a href="chapter3.html#section3-8"><i class="fa fa-check"></i><b>3.8</b> Chapter Summary</a></li>
<li class="chapter" data-level="3.9" data-path="chapter3.html"><a href="chapter3.html#section3-9"><i class="fa fa-check"></i><b>3.9</b> Summary of important R code</a></li>
<li class="chapter" data-level="3.10" data-path="chapter3.html"><a href="chapter3.html#section3-10"><i class="fa fa-check"></i><b>3.10</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="chapter4.html"><a href="chapter4.html"><i class="fa fa-check"></i><b>4</b> Two-Way ANOVA</a><ul>
<li class="chapter" data-level="4.1" data-path="chapter4.html"><a href="chapter4.html#section4-1"><i class="fa fa-check"></i><b>4.1</b> Situation</a></li>
<li class="chapter" data-level="4.2" data-path="chapter4.html"><a href="chapter4.html#section4-2"><i class="fa fa-check"></i><b>4.2</b> Designing a two-way experiment and visualizing results</a></li>
<li class="chapter" data-level="4.3" data-path="chapter4.html"><a href="chapter4.html#section4-3"><i class="fa fa-check"></i><b>4.3</b> Two-Way ANOVA models and hypothesis tests</a></li>
<li class="chapter" data-level="4.4" data-path="chapter4.html"><a href="chapter4.html#section4-4"><i class="fa fa-check"></i><b>4.4</b> Guinea pig tooth growth analysis with Two-Way ANOVA</a></li>
<li class="chapter" data-level="4.5" data-path="chapter4.html"><a href="chapter4.html#section4-5"><i class="fa fa-check"></i><b>4.5</b> Observational study example: The Psychology of Debt</a></li>
<li class="chapter" data-level="4.6" data-path="chapter4.html"><a href="chapter4.html#section4-6"><i class="fa fa-check"></i><b>4.6</b> Pushing Two-Way ANOVA to the limit: Un-replicated designs</a></li>
<li class="chapter" data-level="4.7" data-path="chapter4.html"><a href="chapter4.html#section4-7"><i class="fa fa-check"></i><b>4.7</b> Chapter summary</a></li>
<li class="chapter" data-level="4.8" data-path="chapter4.html"><a href="chapter4.html#section4-8"><i class="fa fa-check"></i><b>4.8</b> Important R code</a></li>
<li class="chapter" data-level="4.9" data-path="chapter4.html"><a href="chapter4.html#section4-9"><i class="fa fa-check"></i><b>4.9</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="chapter5.html"><a href="chapter5.html"><i class="fa fa-check"></i><b>5</b> Chi-square tests</a><ul>
<li class="chapter" data-level="5.1" data-path="chapter5.html"><a href="chapter5.html#section5-1"><i class="fa fa-check"></i><b>5.1</b> Situation, contingency tables, and plots</a></li>
<li class="chapter" data-level="5.2" data-path="chapter5.html"><a href="chapter5.html#section5-2"><i class="fa fa-check"></i><b>5.2</b> Homogeneity Test Hypotheses</a></li>
<li class="chapter" data-level="5.3" data-path="chapter5.html"><a href="chapter5.html#section5-3"><i class="fa fa-check"></i><b>5.3</b> Independence Test Hypotheses</a></li>
<li class="chapter" data-level="5.4" data-path="chapter5.html"><a href="chapter5.html#section5-4"><i class="fa fa-check"></i><b>5.4</b> Models for R by C tables</a></li>
<li class="chapter" data-level="5.5" data-path="chapter5.html"><a href="chapter5.html#section5-5"><i class="fa fa-check"></i><b>5.5</b> Permutation tests for the X2 statistic</a></li>
<li class="chapter" data-level="5.6" data-path="chapter5.html"><a href="chapter5.html#section5-6"><i class="fa fa-check"></i><b>5.6</b> Chi-square distribution for the X2 statistic</a></li>
<li class="chapter" data-level="5.7" data-path="chapter5.html"><a href="chapter5.html#section5-7"><i class="fa fa-check"></i><b>5.7</b> Examining residuals for the source of differences</a></li>
<li class="chapter" data-level="5.8" data-path="chapter5.html"><a href="chapter5.html#section5-8"><i class="fa fa-check"></i><b>5.8</b> General Protocol for X2 tests</a></li>
<li class="chapter" data-level="5.9" data-path="chapter5.html"><a href="chapter5.html#section5-9"><i class="fa fa-check"></i><b>5.9</b> Political Party and Voting results: Complete Analysis</a></li>
<li class="chapter" data-level="5.10" data-path="chapter5.html"><a href="chapter5.html#section5-10"><i class="fa fa-check"></i><b>5.10</b> Is cheating and lying related in students?</a></li>
<li class="chapter" data-level="5.11" data-path="chapter5.html"><a href="chapter5.html#section5-11"><i class="fa fa-check"></i><b>5.11</b> Analyzing a stratified random sample of California schools</a></li>
<li class="chapter" data-level="5.12" data-path="chapter5.html"><a href="chapter5.html#section5-12"><i class="fa fa-check"></i><b>5.12</b> Chapter summary</a></li>
<li class="chapter" data-level="5.13" data-path="chapter5.html"><a href="chapter5.html#section5-13"><i class="fa fa-check"></i><b>5.13</b> Review of Important R commands</a></li>
<li class="chapter" data-level="5.14" data-path="chapter5.html"><a href="chapter5.html#section5-14"><i class="fa fa-check"></i><b>5.14</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="chapter6.html"><a href="chapter6.html"><i class="fa fa-check"></i><b>6</b> Correlation and Simple Linear Regression</a><ul>
<li class="chapter" data-level="6.1" data-path="chapter6.html"><a href="chapter6.html#section6-1"><i class="fa fa-check"></i><b>6.1</b> Relationships between two quantitative variables</a></li>
<li class="chapter" data-level="6.2" data-path="chapter6.html"><a href="chapter6.html#section6-2"><i class="fa fa-check"></i><b>6.2</b> Estimating the correlation coefficient</a></li>
<li class="chapter" data-level="6.3" data-path="chapter6.html"><a href="chapter6.html#section6-3"><i class="fa fa-check"></i><b>6.3</b> Relationships between variables by groups</a></li>
<li class="chapter" data-level="6.4" data-path="chapter6.html"><a href="chapter6.html#section6-4"><i class="fa fa-check"></i><b>6.4</b> Inference for the correlation coefficient (Optional Section)</a></li>
<li class="chapter" data-level="6.5" data-path="chapter6.html"><a href="chapter6.html#section6-5"><i class="fa fa-check"></i><b>6.5</b> Are tree diameters related to tree heights?</a></li>
<li class="chapter" data-level="6.6" data-path="chapter6.html"><a href="chapter6.html#section6-6"><i class="fa fa-check"></i><b>6.6</b> Describing relationships with a regression model</a></li>
<li class="chapter" data-level="6.7" data-path="chapter6.html"><a href="chapter6.html#section6-7"><i class="fa fa-check"></i><b>6.7</b> Least Squares Estimation</a></li>
<li class="chapter" data-level="6.8" data-path="chapter6.html"><a href="chapter6.html#section6-8"><i class="fa fa-check"></i><b>6.8</b> Measuring the strength of regressions: R2</a></li>
<li class="chapter" data-level="6.9" data-path="chapter6.html"><a href="chapter6.html#section6-9"><i class="fa fa-check"></i><b>6.9</b> Outliers: leverage and influence</a></li>
<li class="chapter" data-level="6.10" data-path="chapter6.html"><a href="chapter6.html#section6-10"><i class="fa fa-check"></i><b>6.10</b> Residual diagnostics – setting the stage for inference</a></li>
<li class="chapter" data-level="6.11" data-path="chapter6.html"><a href="chapter6.html#section6-11"><i class="fa fa-check"></i><b>6.11</b> Old Faithful discharge and waiting times</a></li>
<li class="chapter" data-level="6.12" data-path="chapter6.html"><a href="chapter6.html#section6-12"><i class="fa fa-check"></i><b>6.12</b> Chapter summary</a></li>
<li class="chapter" data-level="6.13" data-path="chapter6.html"><a href="chapter6.html#section6-13"><i class="fa fa-check"></i><b>6.13</b> Important R code</a></li>
<li class="chapter" data-level="6.14" data-path="chapter6.html"><a href="chapter6.html#section6-14"><i class="fa fa-check"></i><b>6.14</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="chapter7.html"><a href="chapter7.html"><i class="fa fa-check"></i><b>7</b> Simple linear regression inference</a><ul>
<li class="chapter" data-level="7.1" data-path="chapter7.html"><a href="chapter7.html#section7-1"><i class="fa fa-check"></i><b>7.1</b> Model</a></li>
<li class="chapter" data-level="7.2" data-path="chapter7.html"><a href="chapter7.html#section7-2"><i class="fa fa-check"></i><b>7.2</b> Confidence Interval and Hypothesis tests for the slope and intercept</a></li>
<li class="chapter" data-level="7.3" data-path="chapter7.html"><a href="chapter7.html#section7-3"><i class="fa fa-check"></i><b>7.3</b> Bozeman temperature trend</a></li>
<li class="chapter" data-level="7.4" data-path="chapter7.html"><a href="chapter7.html#section7-4"><i class="fa fa-check"></i><b>7.4</b> Randomizing inferences for the slope coefficient</a></li>
<li class="chapter" data-level="7.5" data-path="chapter7.html"><a href="chapter7.html#section7-5"><i class="fa fa-check"></i><b>7.5</b> Transformations part I: Linearizing relationships</a></li>
<li class="chapter" data-level="7.6" data-path="chapter7.html"><a href="chapter7.html#section7-6"><i class="fa fa-check"></i><b>7.6</b> Transformations part II: Impacts on SLR interpretations: log(y), log(x), &amp; both log(y) &amp; log(x)</a></li>
<li class="chapter" data-level="7.7" data-path="chapter7.html"><a href="chapter7.html#confidence-interval-for-the-mean-and-prediction-intervals-for-a-new-observation-270"><i class="fa fa-check"></i><b>7.7</b> Confidence Interval for the mean and prediction Intervals for a new observation 270</a></li>
<li class="chapter" data-level="7.8" data-path="chapter7.html"><a href="chapter7.html#section7-7"><i class="fa fa-check"></i><b>7.8</b> Chapter summary</a></li>
<li class="chapter" data-level="7.9" data-path="chapter7.html"><a href="chapter7.html#section7-8"><i class="fa fa-check"></i><b>7.9</b> Important R code</a></li>
<li class="chapter" data-level="7.10" data-path="chapter7.html"><a href="chapter7.html#section7-9"><i class="fa fa-check"></i><b>7.10</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="chapter8.html"><a href="chapter8.html"><i class="fa fa-check"></i><b>8</b> Multiple linear regression</a><ul>
<li class="chapter" data-level="8.1" data-path="chapter8.html"><a href="chapter8.html#section8-1"><i class="fa fa-check"></i><b>8.1</b> Going from SLR to MLR</a></li>
<li class="chapter" data-level="8.2" data-path="chapter8.html"><a href="chapter8.html#section8-2"><i class="fa fa-check"></i><b>8.2</b> Validity conditions in MLR</a></li>
<li class="chapter" data-level="8.3" data-path="chapter8.html"><a href="chapter8.html#section8-3"><i class="fa fa-check"></i><b>8.3</b> Interpretation of MLR terms</a></li>
<li class="chapter" data-level="8.4" data-path="chapter8.html"><a href="chapter8.html#section8-4"><i class="fa fa-check"></i><b>8.4</b> Comparing multiple regression models</a></li>
<li class="chapter" data-level="8.5" data-path="chapter8.html"><a href="chapter8.html#section8-5"><i class="fa fa-check"></i><b>8.5</b> General recommendations for MLR interpretations and VIFs</a></li>
<li class="chapter" data-level="8.6" data-path="chapter8.html"><a href="chapter8.html#section8-6"><i class="fa fa-check"></i><b>8.6</b> MLR Inference: Parameter inferences using the t-distribution</a></li>
<li class="chapter" data-level="8.7" data-path="chapter8.html"><a href="chapter8.html#section8-7"><i class="fa fa-check"></i><b>8.7</b> Overall F-test in Multiple Linear Regression</a></li>
<li class="chapter" data-level="8.8" data-path="chapter8.html"><a href="chapter8.html#section8-8"><i class="fa fa-check"></i><b>8.8</b> Case Study: First year college GPA and SATs</a></li>
<li class="chapter" data-level="8.9" data-path="chapter8.html"><a href="chapter8.html#section8-9"><i class="fa fa-check"></i><b>8.9</b> Different intercepts for different groups: MLR with Indicator variables</a></li>
<li class="chapter" data-level="8.10" data-path="chapter8.html"><a href="chapter8.html#section8-10"><i class="fa fa-check"></i><b>8.10</b> Additive MLR with more than two groups: Headache example</a></li>
<li class="chapter" data-level="8.11" data-path="chapter8.html"><a href="chapter8.html#section8-11"><i class="fa fa-check"></i><b>8.11</b> Different slopes and different intercepts</a></li>
<li class="chapter" data-level="8.12" data-path="chapter8.html"><a href="chapter8.html#section8-12"><i class="fa fa-check"></i><b>8.12</b> F-tests for MLR models with quantitative and categorical variables and interactions</a></li>
<li class="chapter" data-level="8.13" data-path="chapter8.html"><a href="chapter8.html#section8-13"><i class="fa fa-check"></i><b>8.13</b> AICs for model selection</a></li>
<li class="chapter" data-level="8.14" data-path="chapter8.html"><a href="chapter8.html#section8-14"><i class="fa fa-check"></i><b>8.14</b> Forced Expiratory Volume model selection using AICs</a></li>
<li class="chapter" data-level="8.15" data-path="chapter8.html"><a href="chapter8.html#section8-15"><i class="fa fa-check"></i><b>8.15</b> Chapter summary</a></li>
<li class="chapter" data-level="8.16" data-path="chapter8.html"><a href="chapter8.html#section8-16"><i class="fa fa-check"></i><b>8.16</b> Important R code</a></li>
<li class="chapter" data-level="8.17" data-path="chapter8.html"><a href="chapter8.html#section8-17"><i class="fa fa-check"></i><b>8.17</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="chapter9.html"><a href="chapter9.html"><i class="fa fa-check"></i><b>9</b> Case studies</a><ul>
<li class="chapter" data-level="9.1" data-path="chapter9.html"><a href="chapter9.html#section9-1"><i class="fa fa-check"></i><b>9.1</b> Overview of material covered</a></li>
<li class="chapter" data-level="9.2" data-path="chapter9.html"><a href="chapter9.html#section9-2"><i class="fa fa-check"></i><b>9.2</b> The impact of simulated chronic nitrogen deposition on the biomass and N2-fixation activity of two boreal feather moss–cyanobacteria associations</a></li>
<li class="chapter" data-level="9.3" data-path="chapter9.html"><a href="chapter9.html#section9-3"><i class="fa fa-check"></i><b>9.3</b> Ants learn to rely on more informative attributes during decision-making</a></li>
<li class="chapter" data-level="9.4" data-path="chapter9.html"><a href="chapter9.html#section9-4"><i class="fa fa-check"></i><b>9.4</b> Multi-variate models are essential for understanding vertebrate diversification in deep time</a></li>
<li class="chapter" data-level="9.5" data-path="chapter9.html"><a href="chapter9.html#section9-5"><i class="fa fa-check"></i><b>9.5</b> General summary</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Second Semester Statistics Course with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chapter4" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> Two-Way ANOVA</h1>
<div id="section4-1" class="section level2">
<h2><span class="header-section-number">4.1</span> Situation</h2>
<p>In this chapter, we extend the One-Way ANOVA to situations with two factors or categorical explanatory variables in a method that is generally called the <strong><em>Two-Way ANOVA</em></strong> . This allows researchers to simultaneously study more than one variable that might explain variability in the responses and explore whether the impacts of one variable change depending on the other variable. In some situations, each observation is so expensive that researchers want to use a single study to explore two different sets of research questions in the same round of data collection. For example, a company might want to study factors that affect the number of defective products per day and are interested in the impacts of two different types of training programs and three different levels of production quotas. These methods would allow engineers to compare the training programs, production quotas, and see if the training programs work differently for different production quotas. In a clinical trials context, it is well known that certain factors can change the performance of certain drugs. For example, different dosages of a drug might have different benefits or side-effects on men, versus women or children. <strong>When the impact of one factor changes on the level of another factor</strong>, we say that they <strong><em>interact</em></strong>. It is also possible for both factors to be related to differences in the mean responses and not interact. For example, suppose there is a difference in the response means between men and women and a difference among various dosages, but the effect of increasing the dosage is the same for the male and female subjects. This is an example of what is called an <strong><em>additive</em></strong> type of model. In general, the world is more complicated than the single factor models we considered in Chapter <a href="chapter3.html#chapter3">3</a> can account for, especially in observational studies, so these models allow us to start to handle more realistic situations.</p>
<p>Consider the following “experiment” where we want to compare the strength of different brands of paper towels when they are wet. The response variable will be the time to failure in seconds (a continuous response variable) when a weight is placed on the towel held at the four corners. We are interested in studying the differences between brands and the impact of different amounts of water applied to the towels.</p>
<ul>
<li><p>Predictors (Explanatory Variables): <strong>A</strong> : <code>Brand</code> (2 brands of interest, named <em>B1</em> and <em>B2</em>) and <strong>B</strong> : Number of <code>Drops</code> of water (10, 20, 30 drops).</p></li>
<li><p>Response: <em>Time</em> to failure (in seconds) of a towel (<span class="math inline">\(y\)</span>) with a weight sitting in the middle of the towel.</p></li>
</ul>
</div>
<div id="section4-2" class="section level2">
<h2><span class="header-section-number">4.2</span> Designing a two-way experiment and visualizing results</h2>
<p>Ideally, we want to randomly assign the levels of each factor so that we can attribute causality to any detected effects and to reduce the chances of <em>confounding</em>. Because there are two factors, we would need to design a random assignment scheme to select the levels of both variables. For example, we could randomly select a brand and then randomly select the number of drops to apply from the levels chosen for each measurement. Or we could decide on how many observations we want at each combination of the two factors (ideally having them all equal so the design is <strong><em>balanced</em></strong>) and then randomize the order of applying the different combinations of levels.</p>
<p>Why might it be important to randomly apply the brand and number of drops in an experiment? There are situations where the order of observations can be related to changes in the responses and we want to be able to eliminate the order of observations from being related to the levels of the factors. For example, suppose that the area where the experiment is being performed becomes wet over time and the later measurements have extra water that gets onto the paper towels and they tend to fail more quickly. If all the observations for the second brand were done later in the study, then the <em>order of observations</em> impacts could make the second brand look worse. If the order of observations is randomized, then even if there is some drift in the responses over the order of observations it should still be possible to see the differences in the randomly assigned effects. If the study incorporates repeated measurements on human subjects, randomizing the order of treatments they are exposed to can alleviate impacts of them “learning” through the study, something that we would not have to worry about with paper towels.</p>
<p>In observational studies, we do not have the luxury of random assignment, that is, we cannot randomly assign levels of the treatment variables to our subjects, so we cannot guarantee that the only difference between the groups are the explanatory variables. As discussed before, because we can’t control which level of the variables are assigned to the subjects, we cannot make causal inferences and have to worry about other variables being the real drivers of the results. Although we can never establish causal inference with observational studies, we can generalize our results to a larger population if we have a representative sample from our population of interest.</p>
<p>It is also possible that we might have studies where some of the variables are randomly assigned and others are not randomly assignable. The most common versions of this are what we sometimes call subject “demographics”, such as sex, income, race, etc. We might be performing a study where we can randomly assign treatments to these subjects but might also want to account for differences based on income level, which we can’t assign. In these cases, the scope of inference gets complicated – differences seen on randomized variables can be causally interpreted but you have to be careful to not say that the demographics caused differences. Suppose that a randomly assigned drug dosage is found to show differences in male patients but not in female patients. We could say that the dosage causes differences in males but does not in females. We are not saying that sex caused the differences but that the causal differences were modified by the sex of the subjects.</p>
<p>Even when we do have random assignment of treatments it is important to think about who/what is included in the sample. To get back to the paper towel example, we are probably interested in more than the sheets of the rolls we have to work with so if we could randomly select the studied paper towels from all paper towels made by each brand, our conclusions could be extended to those populations. That probably would not be practical, but trying to make sure that the towels are representative of all made by each brand by checking for defects and maybe picking towels from a few different rolls would be a good start to being able to extend inferences beyond the tested towels.</p>
<p>Once random assignment and random sampling is settled, the final aspect of study design involves deciding on the number of observations that should be made. The short (glib) answer is to take as many as you can afford. With more observations comes higher power to detect differences if they exist, which is a desired attribute of all studies. It is also important to make sure that you obtain multiple observations at each combination of the treatment levels, which are called <strong><em>replicates</em></strong>. Having replicate measurements allows estimation of the mean for each combination of the treatment levels as well as estimation and testing for an interaction. And we always prefer having balanced designs because they provide resistance to violation of some assumptions as noted in Chapter <a href="chapter3.html#chapter3">3</a>. A <strong><em>balanced design</em></strong> in a Two-Way ANOVA setting involves having the same sample size for every combination of the levels of the treatments.</p>
<p>With two categorical explanatory variables, there are now five possible scenarios for the truth. Different situations are created depending on whether there is an interaction between the two variables, whether both variables are important but do not interact, or whether either of the variables matter at all. Basically, there are five different possible outcomes in a randomized Two-Way ANOVA study, listed in order of increasing model complexity:</p>
<ol style="list-style-type: decimal">
<li><p>Neither A or B has an effect on the responses (nothing causes differences in responses).</p></li>
<li><p>A has an effect, B does not (only A causes differences in responses).</p></li>
<li><p>B has an effect, A does not (only B causes differences in responses).</p></li>
<li><p>Both A and B have effects on response but no interaction (A and B both cause differences in responses but the impacts are additive).</p></li>
<li><p>Effect of A differs based on the levels of B, the opposite is also true (means for levels of A are different for different levels of B, or, simply, A and B interact).</p></li>
</ol>
<p>To illustrate these five potential outcomes, we will consider a fake version of the paper towel example. It ended up being really messy and complicated to actually perform the experiment as we described it so these data were simulated to help us understand the Two-Way ANOVA possibilities in as simple a situation as possible. The first step is to understand what has been observed (number observations at each combination of factors) and look at some summary statistics across all the “groups”. The data set is available from the course website using:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pt&lt;-<span class="kw">read.csv</span>(<span class="st">&quot;http://www.math.montana.edu/courses/s217/documents/pt.csv&quot;</span>)
pt<span class="op">$</span>drops&lt;-<span class="kw">factor</span>(pt<span class="op">$</span>drops)</code></pre></div>
<p>The data set contains five observations per combination of treatment levels as provided by the <code>tally</code> function. To get counts for combinations of the variables, use the general formula of <code>tally(x1~x2, data=...)</code> although the order of <code>x1</code> and <code>x2</code> doesn’t matter:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(mosaic)
<span class="kw">tally</span>(brand <span class="op">~</span><span class="st"> </span>drops, <span class="dt">data=</span>pt)</code></pre></div>
<pre><code>##      drops
## brand 10 20 30
##    B1  5  5  5
##    B2  5  5  5</code></pre>
<p>The sample sizes in each of the six treatment level combinations of <code>Brand</code> and <code>Drops</code> [(<em>B1</em>, 10), (<em>B1</em>, 20), (<em>B1</em>, 30), (<em>B2</em>, 10), (<em>B2</em>, 20), (<em>B2</em>, 30)] are <span class="math inline">\(n_{jk} = 5\)</span> for <span class="math inline">\(j^{th}\)</span> level of <code>Brand</code> (<span class="math inline">\(j=1, 2\)</span>) and <span class="math inline">\(k^{th}\)</span> level of <code>Drops</code> (<span class="math inline">\(k=1, 2, 3\)</span>). The <code>tally</code> function gives us a<br />
<strong><em>contingency table</em></strong> with <span class="math inline">\(R = 2\)</span> rows (<em>B1</em>, <em>B2</em>) and <span class="math inline">\(C = 3\)</span> columns (10, 20, and 30). We’ll have more fun with this sort of summary of <span class="math inline">\(R\)</span> by <span class="math inline">\(C\)</span> tables in Chapter <a href="chapter5.html#chapter5">5</a> – here it helps us see the sample size in each combination of factor levels. The <code>favstats</code> function also helps us dig into the results for all combinations of factor levels. The notation involves putting both variables after the “~” with a “<code>+</code>” between them. In the output, the first row contains summary information for the 5 observations for <code>Brand</code> <em>B1</em> and <code>Drops</code> amount 10. It also contains the sample size in the <code>n</code> column, although here it rolled into a new set of rows with the standard deviations.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">favstats</span>(responses <span class="op">~</span><span class="st"> </span>brand <span class="op">+</span><span class="st"> </span>drops, <span class="dt">data=</span>pt)</code></pre></div>
<pre><code>##   brand.drops       min        Q1   median       Q3      max     mean
## 1       B1.10 0.3892621 1.3158737 1.906436 2.050363 2.333138 1.599015
## 2       B2.10 2.3078095 2.8556961 3.001147 3.043846 3.050417 2.851783
## 3       B1.20 0.3838299 0.7737965 1.516424 1.808725 2.105380 1.317631
## 4       B2.20 1.1415868 1.9382142 2.066681 2.838412 3.001200 2.197219
## 5       B1.30 0.2387500 0.9804284 1.226804 1.555707 1.829617 1.166261
## 6       B2.30 0.5470565 1.1205102 1.284117 1.511692 2.106356 1.313946
##          sd n missing
## 1 0.7714970 5       0
## 2 0.3140764 5       0
## 3 0.7191978 5       0
## 4 0.7509989 5       0
## 5 0.6103657 5       0
## 6 0.5686485 5       0</code></pre>
<p>The next step is to visually explore the results across the combinations of the two explanatory variables. The beanplot can be extended to handle these sorts of two-way situations only if one of the two variables is a two-level variable. This is a pretty serious constraint on this display, so we will show you the plot (Figure <a href="chapter4.html#fig:Figure4-1">4.1</a>) but not focus on the code. The reason beanplots can only handle <span class="math inline">\(2 \times K\)</span> designs is that the beans are split along a vertical line for the <span class="math inline">\(K\)</span> levels of the other variable. In Figure <a href="chapter4.html#fig:Figure4-1">4.1</a>, the <code>Brand</code> B1 density curves are shaded and the B2 curves are not. In reading these plots, look for differences in each level and whether those differences change across the levels of the other variable. Specifically, start with comparing the two brands for different amounts of water. Do the brands seem different? Certainly for 10 drops of water the two look different but not for 30 drops. We can also look for combinations of factors that produce the highest or lower responses in this display. It appears that the time to failure is highest in the low water drop groups but as the water levels increase, the time to failure falls and the differences in the two brands seem to decrease. The fake data seem to have relatively similar amounts of variability and distribution shapes – remembering that there are only 5 observations available for describing the shape of responses for each combination. These data were simulated using a normal distribution and constant variance if that gives you some extra confidence in assessing these model assumptions.</p>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(beanplot)
<span class="kw">beanplot</span>(responses <span class="op">~</span><span class="st"> </span>brand<span class="op">*</span>drops, <span class="dt">data=</span>pt, <span class="dt">side=</span><span class="st">&quot;b&quot;</span>, <span class="dt">col=</span><span class="kw">list</span>(<span class="st">&quot;lightblue&quot;</span>,<span class="st">&quot;white&quot;</span>),
         <span class="dt">xlab=</span><span class="st">&quot;Drops&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;Time&quot;</span>, <span class="dt">method=</span><span class="st">&quot;jitter&quot;</span>,<span class="dt">log=</span><span class="st">&quot;&quot;</span>)
<span class="kw">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="dt">bty=</span><span class="st">&quot;n&quot;</span>, <span class="kw">c</span>(<span class="st">&quot;B1&quot;</span>,<span class="st">&quot;B2&quot;</span>), <span class="dt">fill=</span><span class="kw">c</span>(<span class="st">&quot;lightblue&quot;</span>,<span class="st">&quot;white&quot;</span>))</code></pre></div>
<div class="figure"><span id="fig:Figure4-1"></span>
<img src="GreenwoodBanner_files/figure-html/Figure4-1-1.png" alt="Beanplot of paper towel data by Drops (x-axis) and Brand (side of bean, shaded area for Brand B1." width="672" />
<p class="caption">
Figure 4.1: Beanplot of paper towel data by <code>Drops</code> (x-axis) and <code>Brand</code> (side of bean, shaded area for <code>Brand</code> <em>B1</em>.
</p>
</div>
<p>The beanplots can’t handle situations where both variables have more than two levels – we need a simpler display that just focuses on the means at the combinations of the two explanatory variables. The means for each combination of levels that you can find in the <code>favstats</code> output are more usefully used in what is called an <strong><em>interaction plot</em></strong>. Interaction plots display the mean responses (y-axis) versus levels of one predictor variable on the x-axis, adding points and lines for each level of the other predictor variable. Because we don’t like any of the available functions in R, we wrote our own function, called <code>intplot</code> that you can download using:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">source</span>(<span class="st">&quot;http://www.math.montana.edu/courses/s217/documents/intplot.R&quot;</span>)</code></pre></div>
<p>The function allows a formula interface like <code>Y~X1*X2</code> and provides the means <span class="math inline">\(\pm\)</span> 1 SE (vertical bars) and adds a legend to help make everything clear.</p>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">intplot</span>(responses <span class="op">~</span><span class="st"> </span>brand<span class="op">*</span>drops, <span class="dt">data=</span>pt)</code></pre></div>
<div class="figure"><span id="fig:Figure4-2"></span>
<img src="GreenwoodBanner_files/figure-html/Figure4-2-1.png" alt="Interaction plot of the paper towel data with Drops on the x-axis." width="672" />
<p class="caption">
Figure 4.2: Interaction plot of the paper towel data with <code>Drops</code> on the x-axis.
</p>
</div>
<p>Interaction plots can always be made two different ways by switching the order of the variables. Figure <a href="chapter4.html#fig:Figure4-2">4.2</a> contains <code>Drops</code> on the x-axis and Figure <a href="chapter4.html#fig:Figure4-3">4.3</a> has <code>Brand</code> on the x-axis. Typically putting the variable with more levels on the x-axis will make interpretation easier, but not always. Try both and decide on the one that you like best.</p>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">intplot</span>(responses <span class="op">~</span><span class="st"> </span>drops<span class="op">*</span>brand, <span class="dt">data=</span>pt)</code></pre></div>
<div class="figure"><span id="fig:Figure4-3"></span>
<img src="GreenwoodBanner_files/figure-html/Figure4-3-1.png" alt="Interaction plot of paper towel data with Brand on the x-axis." width="672" />
<p class="caption">
Figure 4.3: Interaction plot of paper towel data with <code>Brand</code> on the x-axis.
</p>
</div>
<p>The formula in this function builds on our previous notation and now we include both predictor variables with an “*&quot; between them. Using an asterisk between explanatory variables is one way of telling R to include an interaction between the variables. While the interaction may or may not be present, the interaction plot helps us to explore those potential differences.</p>
<p>There are a variety of aspects of the interaction plots to pay attention to. Initially, the question to answer is whether it appears that there is an interaction between the predictor variables. When there is an interaction, you will see <strong><em>non-parallel lines</em></strong> in the interaction plot. You want to look from left to right in the plot and assess whether the lines are close to parallel, relative to the amount of variability in the means. If it seems that there is clear visual evidence of non-parallel lines, then the interaction is likely worth considering (we will typically use a hypothesis test to formally assess this – see discussion below). If the lines look to be close to parallel, then there probably isn’t an interaction between the variables. Without an interaction present, that means that the differences across levels of one variable doesn’t change based on the levels of the other variable and vice-versa. This means that we can consider the <strong><em>main effects</em></strong> of each variable on their own<a href="#fn43" class="footnoteRef" id="fnref43"><sup>43</sup></a>. Main effects are much like the results we found in Chapter <a href="chapter3.html#chapter3">3</a> where we can compare means across levels of a single variable except that there are results for two variables to extract from the model. With the presence of an interaction, it is complicated to summarize how each variable is affecting the response variable because their impacts change depending on the level of the other factor. And plots like the interaction plot provide us much useful information.</p>
<p>If the lines are not parallel, then focus in on comparing the levels of one variable as the other variable changes. Remember that the definition of an interaction is that the differences among levels of one variable depends on the level of the other variable being considered. “Visually” this means comparing the size of the differences in the lines from left to right. In Figures <a href="chapter4.html#fig:Figure4-2">4.2</a> and <a href="chapter4.html#fig:Figure4-3">4.3</a>, the effect of amount of water changes based on the brand being considered. In Figure <a href="chapter4.html#fig:Figure4-3">4.3</a>, the three lines represent the three water levels. The difference between the brands (left to right, <em>B1</em> to <em>B2</em>) is different depending on how much water was present. It appears that <code>Brand</code> <em>B2</em> lasted longer at the lower water levels but that the difference between the two brands dropped as the water levels increased. The same story appears in Figure <a href="chapter4.html#fig:Figure4-2">4.2</a>. As the water levels increase (left to right, 10 to 20 to 30 drops), the differences between the two brands decrease. Of the two versions, Figure <a href="chapter4.html#fig:Figure4-2">4.2</a> is probably easier to read here. The interaction plots also are useful for identifying the best and worst mean responses for combinations of the treatment levels. For example, 10 <code>Drops</code> and <code>Brand</code> <em>B2</em> lasts longest, on average, and 30 <code>Drops</code> with <code>Brand</code> <em>B1</em> fails fastest, on average. In this situation, the lines do not appear to be parallel suggesting that further exploration of the interaction appears to be warranted.</p>
<p>Before we get to the hypothesis tests to formally make this assessment (you knew some sort of p-value was coming, right?), we can visualize the 5 different scenarios that could characterize the sorts of results you could observe in a Two-Way ANOVA situation. Figure <a href="chapter4.html#fig:Figure4-4">4.4</a> shows 4 of the 5 scenarios. In panel (a), when there are no differences from either variable (Scenario 1), it provides relatively parallel lines and basically no differences either across <code>Drops</code> levels (x-axis) or <code>Brand</code> (lines). This would result in no evidence related to a difference in brands, water levels, or any interaction between them.</p>

<div class="figure"><span id="fig:Figure4-4"></span>
<img src="GreenwoodBanner_files/figure-html/Figure4-4-1.png" alt="Interaction plots of four possible scenarios in the paper towel study." width="672" />
<p class="caption">
Figure 4.4: Interaction plots of four possible scenarios in the paper towel study.
</p>
</div>
<p>Scenario 2 (Figure <a href="chapter4.html#fig:Figure4-4">4.4</a> panel (b)) incorporates differences based on factor A (here that is <code>Brand</code>) but no real difference based on the <code>Drops</code> or any interaction. This results in a clear shift between the little to no changes in the level of those lines across water levels. These lines are relatively parallel. We can see that <code>Brand</code> <em>B2</em> is better than <code>Brand</code> <em>B1</em> but that is all we can show with these sorts of results.</p>
<p>Scenario 3 (Figure <a href="chapter4.html#fig:Figure4-4">4.4</a> panel (c)) flips the important variable to B (<code>Drops</code>) and shows decreasing average times as the water levels increase. Again, the interaction panels show near parallel-ness in the lines and really just show differences among the levels of the water. In both Scenarios 2 and 3, we could use a single variable and drop the other from the model, getting back to a One-Way ANOVA model, without losing any important information.</p>
<p>Scenario 4 (Figure <a href="chapter4.html#fig:Figure4-4">4.4</a> panel (d)) incorporates effects of A and B, but they are <strong><em>additive</em></strong>. That means that the effect of one variable is the same across the levels of the other variable. In this experiment, that would mean that <code>Drops</code> has the same impact on performance regardless of brand and that the brands differ but each type of difference is the same regardless of levels of the other variable. The interaction plot lines are more or less parallel but now the brands are clearly different from each other. The plot shows the decrease in performance based on increasing water levels and that <code>Brand</code> <em>B2</em> is better than <code>Brand</code> <em>B1</em>. Additive effects show the same difference in lines from left to right in the interaction plots.</p>
<p>Finally, Scenario 5 (Figure <a href="chapter4.html#fig:Figure4-5">4.5</a>) involves an interaction between the two variables (<code>Drops</code> and <code>Brand</code>). There are many ways that interactions can present but the main thing is to look for clearly non-parallel lines. As noted in the previous discussion, the <code>Drops</code> effect appears to change depending on which level of <code>Brand</code> is being considered. Note that the plot here described as Scenario 5 is the same as the initial plot of the results in Figure <a href="chapter4.html#fig:Figure4-2">4.2</a>.</p>

<div class="figure"><span id="fig:Figure4-5"></span>
<img src="GreenwoodBanner_files/figure-html/Figure4-5-1.png" alt="Interaction plot of Scenario 5 where it appears that an interaction is present." width="672" />
<p class="caption">
Figure 4.5: Interaction plot of Scenario 5 where it appears that an interaction is present.
</p>
</div>
<p>The typical modeling protocol is to start with assuming that Scenario 5 is a possible description of the results, related to fitting what is called the <strong><em>interaction model</em></strong>, and then attempt to simplify the model (to the <strong><em>additive model</em></strong>) if warranted. We need a hypothesis test to help decide if the interaction is “real” – if there is sufficient evidence to prove that there is an interaction. We need a test because the lines will never be exactly parallel and, just like in the One-Way ANOVA situation, the amount of variation around the lines impacts the ability of the model to detect differences, in this case of an interaction.</p>
</div>
<div id="section4-3" class="section level2">
<h2><span class="header-section-number">4.3</span> Two-Way ANOVA models and hypothesis tests</h2>
<p>To assess interactions with two variables, we need to fully describe models for the additive and interaction scenarios and then develop a method for assessing evidence of the need for different aspects of the models. First, we need to define the notation for these models:</p>
<ul>
<li><p><span class="math inline">\(y_{ijk}\)</span> is the <span class="math inline">\(i^{th}\)</span> response from the group for level <span class="math inline">\(j\)</span> of factor A and level <span class="math inline">\(k\)</span> of factor B</p>
<ul>
<li><p><span class="math inline">\(j=1,\ldots,J\)</span>     <span class="math inline">\(J\)</span> is the number of levels of A</p></li>
<li><p><span class="math inline">\(k=1,\ldots,K\)</span>     <span class="math inline">\(K\)</span> is the number of levels of B</p></li>
<li><p><span class="math inline">\(i=1,\ldots,n_{jk}\)</span>     <span class="math inline">\(n_{jk}\)</span> is the sample size for level <span class="math inline">\(j\)</span> of factor A and level <span class="math inline">\(k\)</span> of factor B</p></li>
<li><p><span class="math inline">\(N=\Sigma\Sigma n_{jk}\)</span> is the total sample size (sum of the number of observations across all <span class="math inline">\(JK\)</span> groups)</p></li>
</ul></li>
</ul>
<p>We need to extend our previous discussion of reference-coded models to develop a Two-Way ANOVA model. We start with the <strong><em>Two-Way ANOVA interaction model</em></strong>:</p>
<p><span class="math display">\[y_{ijk} = \alpha + \tau_j + \gamma_k + \omega_{jk} + \epsilon_{ijk},\]</span></p>
<p>where <span class="math inline">\(\alpha\)</span> is the baseline group mean (for level 1 of A <strong>and</strong> level 1 of B), <span class="math inline">\(\tau_j\)</span> is the deviation for the <strong><em>main effect</em></strong> of A from the baseline for levels <span class="math inline">\(2,\ldots,J\)</span>, <span class="math inline">\(\gamma_k\)</span> (gamma <span class="math inline">\(k\)</span>) is the deviation for the main effect of B from the baseline for levels <span class="math inline">\(2,\ldots,K\)</span>, and <span class="math inline">\(\omega_{jk}\)</span> (omega <span class="math inline">\(jk\)</span>) is the adjustment for the <strong><em>interaction effect</em></strong> for level <span class="math inline">\(j\)</span> of factor A and level <span class="math inline">\(k\)</span> of factor B for <span class="math inline">\(j=1,\ldots,J\)</span> and <span class="math inline">\(k=1,\ldots,K\)</span>. In this model, <span class="math inline">\(\tau_1\)</span>, <span class="math inline">\(\gamma_1\)</span>, and <span class="math inline">\(\omega_{11}\)</span> are all fixed at 0. As in Chapter <a href="chapter3.html#chapter3">3</a>, R will choose the baseline categories alphabetically but now it is choosing a baseline for both variables and so our detective work will be doubled to sort this out.</p>
<p>If the interaction term is not important, based on the interaction test presented below, the <span class="math inline">\(\omega_{jk}\text{&#39;s}\)</span> can be dropped from the model and we get a model that corresponds to Scenario 4 above. Scenario 4 is where there are two main effects but no interaction between them. The <strong><em>additive Two-Way model</em></strong> is</p>
<p><span class="math display">\[y_{ijk} = \alpha + \tau_j + \gamma_k + \epsilon_{ijk},\]</span></p>
<p>where each component is defined as in the interaction model. The difference between the interaction and additive models is setting all the <span class="math inline">\(\omega_{jk}\text{&#39;s}\)</span> to 0 that are present in the interaction model. When we set parameters to 0 in models it removes them from the model. Setting parameters to 0 is how we will develop our hypotheses to test for an interaction, by testing whether there is evidence enough to reject that all <span class="math inline">\(\omega_{jk}\text{&#39;s}=0\)</span>.</p>
<p>The interaction test hypotheses are</p>
<ul>
<li><p><span class="math inline">\(H_0\)</span>: No interaction between A and B in population <span class="math inline">\(\Leftrightarrow\)</span> All <span class="math inline">\(\omega_{jk}\text{&#39;s}=0\)</span>.</p></li>
<li><p><span class="math inline">\(H_A\)</span>: Interaction between A and B in population <span class="math inline">\(\Leftrightarrow\)</span> At least one <span class="math inline">\(\omega_{jk}\ne 0\)</span></p></li>
</ul>
<p>To perform this test, a new ANOVA <span class="math inline">\(F\)</span>-test is required (presented below) but there are also hypotheses relating to the main effects of A (<span class="math inline">\(\tau_j\text{&#39;s}\)</span>) and B (<span class="math inline">\(\gamma_k\text{&#39;s}\)</span>). If evidence is found to reject the null hypothesis that no interaction is present, then it is dangerous to ignore it and test for the main effects because important main effects can be masked by interactions (examples later). It is important to note that, by definition, <strong>both variables matter if an interaction is found to be important</strong> so the main effect tests may not be very interesting. If the interaction is found to be important based on the test and retained in the model, you should focus on the interaction model (also called the <strong><em>full model</em></strong>) in order to understand and describe the form of the interaction among the variables.</p>
<p>If the interaction test does not return a small p-value, then we have no evidence to suggest that it is needed and it can be dropped from the model. In this situation, we would re-fit the model and focus on the results provided by the additive model – performing tests for the two additive main effects. For the first, but not last time, we encounter a model with more than one variable and test of potential interest. In models with multiple variables at similar levels (here both are main effects), we are interested in the results for each variable given that the other variable is in the model. In many situations, including more than one variable in a model changes the results for the other variable even if those variables do not interact. The reason for this is more clear in Chapter <a href="chapter8.html#chapter8">8</a> and really only matters here if we have unbalanced designs, but we need to start adding a short modifier to our discussions of main effects – they are the results <em>conditional on</em> or <em>adjusting for</em> or, simply, <em>given</em>, the other variable(s) in the model. Specifically, the hypotheses for the two main effects are:</p>
<ul>
<li><p>Main effect test for A:</p>
<ul>
<li><p><span class="math inline">\(H_0\)</span>: No differences in means across levels of A in population, given B in the model <span class="math inline">\(\Leftrightarrow\)</span> All <span class="math inline">\(\tau_j\text{&#39;s} = 0\)</span> in additive model.</p></li>
<li><p><span class="math inline">\(H_A\)</span>: Some difference in means across levels A in population, given B in the model <span class="math inline">\(\Leftrightarrow\)</span> At least one <span class="math inline">\(\tau_j \ne 0\)</span>, in additive model.</p></li>
</ul></li>
<li><p>Main effect test for B:</p>
<ul>
<li><p><span class="math inline">\(H_0\)</span>: No differences in means across levels of B in population, given A in the model <span class="math inline">\(\Leftrightarrow\)</span> All <span class="math inline">\(\gamma_k\text{&#39;s} = 0\)</span> in additive model.</p></li>
<li><p><span class="math inline">\(H_A\)</span>: Some difference in means across levels B in population, given A in the model <span class="math inline">\(\Leftrightarrow\)</span> At least one <span class="math inline">\(\gamma_k \ne 0\)</span>, in additive model.</p></li>
</ul></li>
</ul>
<p>In order to test these effects (interaction in the interaction model and main effects in the additive model), <span class="math inline">\(F\)</span>-tests are developed using Sums of Squares, Mean Squares, and degrees of freedom similar to those in Chapter <a href="chapter3.html#chapter3">3</a>. We won’t worry about the details of the sums of squares formulas but you should remember the sums of squares decomposition, which still applies<a href="#fn44" class="footnoteRef" id="fnref44"><sup>44</sup></a>. Table <a href="#tab:Table4-1"><strong>??</strong></a> summarizes the ANOVA results you will obtain for the interaction model and Table <a href="#tab:Table4-2"><strong>??</strong></a> provides the similar general results for the additive model. As we saw in Chapter <a href="chapter3.html#chapter3">3</a>, the degrees of freedom are the amount of information that is free to vary at a particular level and that rule generally holds here. For example, for factor A with <span class="math inline">\(J\)</span> levels, there are <span class="math inline">\(J-1\)</span> parameters that are free since the baseline is fixed. The residual degrees of freedom for both models are not as easily explained but have simple formula. Note that the sum of the degrees of freedom from the main effects, (interaction if present), and error need to equal <span class="math inline">\(N-1\)</span>, just like in the One-Way ANOVA table.</p>

<table>
<caption><span id="tab:unnamed-chunk-105">Table 4.1: </span>Interaction Model ANOVA Table.</caption>
<thead>
<tr class="header">
<th align="left">Source</th>
<th align="left">DF</th>
<th align="left">SS</th>
<th align="left">MS</th>
<th align="left">F-statistics</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">A</td>
<td align="left"><span class="math inline">\(J-1\)</span></td>
<td align="left"><span class="math inline">\(\text{SS}_A\)</span></td>
<td align="left"><span class="math inline">\(\text{MS}_A=\text{SS}_A/\text{df}_A\)</span></td>
<td align="left"><span class="math inline">\(\text{MS}_A/\text{MS}_E\)</span></td>
</tr>
<tr class="even">
<td align="left">B</td>
<td align="left"><span class="math inline">\(K-1\)</span></td>
<td align="left"><span class="math inline">\(\text{SS}_B\)</span></td>
<td align="left"><span class="math inline">\(\text{MS}_B=\text{SS}_B/\text{df}_B\)</span></td>
<td align="left"><span class="math inline">\(\text{MS}_B/\text{MS}_E\)</span></td>
</tr>
<tr class="odd">
<td align="left">A:B (interaction)</td>
<td align="left"><span class="math inline">\((J-1)(K-1)\)</span></td>
<td align="left"><span class="math inline">\(\text{SS}_{AB}\)</span></td>
<td align="left"><span class="math inline">\(\text{MS}_{AB}=\text{SS}_{AB}/\text{df}_{AB}\)</span></td>
<td align="left"><span class="math inline">\(\text{MS}_{AB}/\text{MS}_E\)</span></td>
</tr>
<tr class="even">
<td align="left">Error</td>
<td align="left"><span class="math inline">\(N-J-K+1\)</span></td>
<td align="left"><span class="math inline">\(\text{SS}_E\)</span></td>
<td align="left"><span class="math inline">\(\text{MS}_E=\text{SS}_E/\text{df}_E\)</span></td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left"><b><font color='red'>Total</font></b></td>
<td align="left"><span class="math inline">\(\color{red}{\mathbf{N-1}}\)</span></td>
<td align="left"><span class="math inline">\(\color{red}{\textbf{SS}_{\textbf{Total}}}\)</span></td>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody>
</table>

<table>
<caption><span id="tab:unnamed-chunk-106">Table 4.2: </span>Additive Model ANOVA Table.</caption>
<thead>
<tr class="header">
<th align="left">Source</th>
<th align="left">DF</th>
<th align="left">SS</th>
<th align="left">MS</th>
<th align="left">F-statistics</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">A</td>
<td align="left"><span class="math inline">\(J-1\)</span></td>
<td align="left"><span class="math inline">\(\text{SS}_A\)</span></td>
<td align="left"><span class="math inline">\(\text{MS}_A=\text{SS}_A/\text{df}_A\)</span></td>
<td align="left"><span class="math inline">\(\text{MS}_A/\text{MS}_E\)</span></td>
</tr>
<tr class="even">
<td align="left">B</td>
<td align="left"><span class="math inline">\(K-1\)</span></td>
<td align="left"><span class="math inline">\(\text{SS}_B\)</span></td>
<td align="left"><span class="math inline">\(\text{MS}_B=\text{SS}_B/\text{df}_B\)</span></td>
<td align="left"><span class="math inline">\(\text{MS}_B/\text{MS}_E\)</span></td>
</tr>
<tr class="odd">
<td align="left">Error</td>
<td align="left"><span class="math inline">\(N-J-K+1\)</span></td>
<td align="left"><span class="math inline">\(\text{SS}_E\)</span></td>
<td align="left"><span class="math inline">\(\text{MS}_E=\text{SS}_E/\text{df}_E\)</span></td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left"><b><font color='red'>Total</font></b></td>
<td align="left"><span class="math inline">\(\color{red}{\mathbf{N-1}}\)</span></td>
<td align="left"><span class="math inline">\(\color{red}{\textbf{SS}_{\textbf{Total}}}\)</span></td>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody>
</table>
<p>The mean squares are formed by taking the sums of squares (we’ll let R find those for us) and dividing by the <span class="math inline">\(df\)</span> in the row. The <span class="math inline">\(F\)</span>-ratios are found by taking the mean squares from the row and dividing by the mean squared error (<span class="math inline">\(\text{MS}_E\)</span>). They follow <span class="math inline">\(F\)</span>-distributions with numerator degrees of freedom from the row and denominator degrees of freedom from the Error row (in R output this the <code>Residuals</code> row). It is possible to develop permutation tests for these methods but some technical issues arise in doing permutation tests for interaction model components so we will not use them here. This means we will have to place even more emphasis on meeting the assumptions since we only have the parametric method available.</p>
<p>With some basic expectations about the ANOVA tables and <span class="math inline">\(F\)</span>-statistic construction in mind, we can get to actually estimating the models and exploring the results. The first example involves the fake paper towel data displayed in Figure <a href="chapter4.html#fig:Figure4-1">4.1</a> and <a href="chapter4.html#fig:Figure4-2">4.2</a>. It appeared that Scenario 5 was the correct story since the lines were not parallel, but we need to know whether there is evidence to suggest that the interaction is “real” and we get that through the interaction hypothesis test. To fit the interaction model using <code>lm</code>, the general formulation is <code>lm(y ~ x1*x2, data=...)</code>. The order of the variables doesn’t matter and the most important part of the model, to start with, relates to the interaction of the variables. The ANOVA table output shows the results for the interaction model obtained by running the <code>anova</code> function on the model called <code>m1</code>. Specifically, the test that <span class="math inline">\(H_0: \text{ All } \omega_{jk}\text{&#39;s} = 0\)</span> has a test statistic of <span class="math inline">\(F(2,24)=1.92\)</span> (in bold in the output from the row with brands:drops) and a p-value of 0.17. So there is insufficient evidence to reject the null hypothesis of no interaction, with a 17% chance we would observe a difference in the <span class="math inline">\(\omega_{jk}\text{&#39;s}\)</span> like we did or more extreme if the <span class="math inline">\(\omega_{jk}\text{&#39;s}\)</span> really were all 0. For the interaction model components, R presents them with a colon, <code>:</code>, between the variable names.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m1&lt;-<span class="kw">lm</span>(responses <span class="op">~</span><span class="st"> </span>brand<span class="op">*</span>drops, <span class="dt">data=</span>pt)
<span class="kw">anova</span>(m1)</code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: responses
##             Df Sum Sq Mean Sq F value   Pr(&gt;F)   
## brand        1 4.3322  4.3322 10.5192 0.003458 **
## drops        2 4.8581  2.4290  5.8981 0.008251 **
## brand:drops  2 1.5801  0.7901  1.9184 0.168695   
## Residuals   24 9.8840  0.4118                    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<table style="width:89%;">
<caption>Interaction ANOVA table with interaction row in bold.</caption>
<colgroup>
<col width="25%" />
<col width="8%" />
<col width="12%" />
<col width="15%" />
<col width="13%" />
<col width="13%" />
</colgroup>
<thead>
<tr class="header">
<th align="center"> </th>
<th align="center">Df</th>
<th align="center">Sum Sq</th>
<th align="center">Mean Sq</th>
<th align="center">F value</th>
<th align="center">Pr(&gt;F)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>brand</strong></td>
<td align="center">1</td>
<td align="center">4.332</td>
<td align="center">4.332</td>
<td align="center">10.52</td>
<td align="center">0.003458</td>
</tr>
<tr class="even">
<td align="center"><strong>drops</strong></td>
<td align="center">2</td>
<td align="center">4.858</td>
<td align="center">2.429</td>
<td align="center">5.898</td>
<td align="center">0.008251</td>
</tr>
<tr class="odd">
<td align="center"><strong>brand:drops</strong></td>
<td align="center"><strong>2</strong></td>
<td align="center"><strong>1.58</strong></td>
<td align="center"><strong>0.7901</strong></td>
<td align="center"><strong>1.918</strong></td>
<td align="center"><strong>0.1687</strong></td>
</tr>
<tr class="even">
<td align="center"><strong>Residuals</strong></td>
<td align="center">24</td>
<td align="center">9.884</td>
<td align="center">0.4118</td>
<td align="center">NA</td>
<td align="center">NA</td>
</tr>
</tbody>
</table>
<p>It is useful to display the estimates from this model and we can utilize <code>plot(allEffects(modelname))</code> to visualize the results for the terms in our models. If we turn on the options for <code>grid=T</code>, <code>multiline=T</code>, and <code>ci. style=&quot;bars&quot;</code> we will get a more useful version of the basic “effect plot” for Two-Way ANOVA models with interaction. The results of the estimated interaction model are displayed in Figure <a href="chapter4.html#fig:Figure4-6">4.6</a>, which looks very similar to our previous interaction plot. The only difference is that this comes from model that assumes equal variance and these plots show 95% confidence intervals for the means instead of the 1 standard error used above.</p>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(effects)
<span class="kw">plot</span>(<span class="kw">allEffects</span>(m1), <span class="dt">grid=</span>T, <span class="dt">multiline=</span>T, <span class="dt">ci.style=</span><span class="st">&quot;bars&quot;</span>)</code></pre></div>
<div class="figure"><span id="fig:Figure4-6"></span>
<img src="GreenwoodBanner_files/figure-html/Figure4-6-1.png" alt="Plot of estimated results of interaction model." width="672" />
<p class="caption">
Figure 4.6: Plot of estimated results of interaction model.
</p>
</div>
<p>In the absence of evidence to include the interaction, the model should be simplified to the additive model and the interpretation focused on each main effect, conditional on having the other variable in the model. To fit an additive model and not include an interaction, the model formula involves a “+” instead of a “*&quot; between the explanatory variables.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m2&lt;-<span class="kw">lm</span>(responses <span class="op">~</span><span class="st"> </span>brand <span class="op">+</span><span class="st"> </span>drops, <span class="dt">data=</span>pt)
<span class="kw">anova</span>(m2)</code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: responses
##           Df  Sum Sq Mean Sq F value   Pr(&gt;F)
## brand      1  4.3322  4.3322  9.8251 0.004236
## drops      2  4.8581  2.4290  5.5089 0.010123
## Residuals 26 11.4641  0.4409</code></pre>
<p>The p-values for the main effects of <code>brand</code> and <code>drops</code> change slightly from the results in the interaction model due to changes in the <span class="math inline">\(\text{MS}_E\)</span> from 0.4118 to 0.4409 (more variability is left over in the simpler model) and the <span class="math inline">\(\text{DF}_{\text{error}}\)</span> that increases from 24 to 26. In both models, the <span class="math inline">\(\text{SS}_{\text{Total}}\)</span> is the same (20.6544). In the interaction model,</p>
<p><span class="math display">\[\begin{array}{rl}
\text{SS}_{\text{Total}} &amp; = \text{SS}_{\text{brand}} + \text{SS}_{\text{drops}}
+ \text{SS}_{\text{brand:drops}} + \text{SS}_{\text{E}}\\
&amp; = 4.3322 + 4.8581 + 1.5801 + 9.8840\\
&amp; = 20.6544\\
\end{array}\]</span></p>
<p>In the additive model, the variability that was attributed to the interaction term in the interaction model (<span class="math inline">\(\text{SS}_{\text{brand:drops}} = 1.5801\)</span>) is pushed into the <span class="math inline">\(\text{SS}_{\text{E}}\)</span>, which increases from 9.884 to 11.4641. The sums of squares decomposition in the additive model is</p>
<p><span class="math display">\[\begin{array}{rl}
\text{SS}_{\text{Total}} &amp; = \text{SS}_{\text{brand}} + \text{SS}_{\text{drops}}
 + \text{SS}_{\text{E}} \\
&amp; = 4.3322 + 4.8581 + 11.4641 \\
&amp; = 20.6544 \\
\end{array}\]</span></p>
<p>This shows that the sums of squares decomposition applies in these more complicated models as it did in the One-Way ANOVA. It also shows that if the interaction is removed from the model, that variability is lumped in with the other unexplained variability that goes in the <span class="math inline">\(\text{SS}_{\text{E}}\)</span> in any model.</p>
<p>The fact that the sums of squares decomposition can be applied here is useful, except that there is a small issue with the main effect tests in the ANOVA table results that follow this decomposition when the design is not balanced. It ends up that the tests in a typical ANOVA table are only conditional on the tests higher up in the table. For example, in the additive model ANOVA table, the <code>Brand</code> test is not conditional on the <code>Drops</code> effect, but the <code>Drops</code> effect is conditional on the <code>Brand</code> effect. To fix this issue, we have to use another type of sums of squares, called <strong><em>Type II sums of squares</em></strong>. They will no longer always follow the rules of the sums of squares decomposition but they will test the desired hypotheses. Specifically, they provide each test conditional on any other terms at the same level of the model and match the hypotheses written out earlier in this section. To get the “correct” ANOVA results, the <code>car</code> (Fox and Weisberg, 2011) package is required. We use the <code>Anova</code> function on our linear models from here forward to get the “right” tests in our ANOVA tables. Note how the case-sensitive nature of R code shows up in the use of the capital-A <code>Anova</code> function instead of the <code>anova</code> function used previously. In this case, because the design was balanced, the results are the same using either function. Observational studies rarely generate balanced designs (some designed studies can result in unbalanced designs) so we will generally just use the Type II version of the sums of squares. The <code>Anova</code> results using the Type II sums of squares are slightly more conservative than the results from <code>anova</code>, which are called Type I sums of squares. The sums of squares decomposition no longer can be applied, but it is a small sacrifice to get each test after adjusting for all other variables<a href="#fn45" class="footnoteRef" id="fnref45"><sup>45</sup></a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(car)
<span class="kw">Anova</span>(m2)</code></pre></div>
<pre><code>## Anova Table (Type II tests)
## 
## Response: responses
##            Sum Sq Df F value   Pr(&gt;F)
## brand      4.3322  1  9.8251 0.004236
## drops      4.8581  2  5.5089 0.010123
## Residuals 11.4641 26</code></pre>
<p>The new output switches the columns around and doesn’t show you the mean squares, but gives the most critical parts of the output. Here, there is no change in results because it is balanced design with equal counts of responses in each combination of the two explanatory variables.</p>
<p>The additive model, when appropriate, provides simpler interpretations for each explanatory variable compared to models with interactions because the effect of one variable is the same regardless of the levels of the other variable and vice versa. There are two tools to aid in understanding the impacts of the two variables in the additive model. First, the model summary provides estimated coefficients with interpretations like those seen in Chapter <a href="chapter3.html#chapter3">3</a> (deviation of group <span class="math inline">\(j\)</span> or <span class="math inline">\(k\)</span> from the baseline group’s mean), except with the additional wording of “controlling for” the other variable added to any of the discussion. Second, the term-plots now show each main effect and how the groups differ with one panel for each of the two explanatory variables in the model. These term-plots are created by holding the other variable constant at one of its levels.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(m2)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = responses ~ brand + drops, data = pt)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.4561 -0.4587  0.1297  0.4434  0.9695 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)   1.8454     0.2425   7.611 4.45e-08
## brandB2       0.7600     0.2425   3.134  0.00424
## drops20      -0.4680     0.2970  -1.576  0.12715
## drops30      -0.9853     0.2970  -3.318  0.00269
## 
## Residual standard error: 0.664 on 26 degrees of freedom
## Multiple R-squared:  0.445,  Adjusted R-squared:  0.3809 
## F-statistic: 6.948 on 3 and 26 DF,  p-value: 0.001381</code></pre>
<p>In the model summary, the baseline combination estimated in the <code>(Intercept)</code> row is for <code>Brand</code> <em>B1</em> and <code>Drops</code> 10 and estimates the mean failure time as 1.85 seconds for this combination. As before, the group labels that do not show up are the baseline but there are two variables’ baselines to identify. Now the “simple” aspects of the additive model show up. The interpretation of the <code>Brands</code> <em>B2</em> coefficient is as a deviation from the baseline but it applies regardless of the level of <code>Drops</code>. Any difference between <em>B1</em> and <em>B2</em> involves a shift up of 0.76 seconds in the estimated mean failure time. Similarly, going from 10 (baseline) to 20 drops results in a drop in the estimated failure mean of 0.47 seconds and going from 10 to 30 drops results in a drop of almost 1 second in the average time to failure, both estimated changes are the same regardless of the brand of paper towel being considered. Sometimes, especially in observational studies, we use the terminology “controlled for” to remind the reader that the other variable was present in the model<a href="#fn46" class="footnoteRef" id="fnref46"><sup>46</sup></a> and also explained some of the variability in the responses. The term-plots for the additive model (Figure <a href="chapter4.html#fig:Figure4-7">4.7</a>) help us visualize the impacts of changes brand and changing water levels, holding the other variable constant. The differences in heights in each panel correspond to the coefficients just discussed.</p>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(effects)
<span class="kw">plot</span>(<span class="kw">allEffects</span>(m2))</code></pre></div>
<div class="figure"><span id="fig:Figure4-7"></span>
<img src="GreenwoodBanner_files/figure-html/Figure4-7-1.png" alt="Term-plots of additive model for paper towel data. Left panel displays results for two brands and right panel for number of drops of water, each after controlling for the other." width="672" />
<p class="caption">
Figure 4.7: Term-plots of additive model for paper towel data. Left panel displays results for two brands and right panel for number of drops of water, each after controlling for the other.
</p>
</div>
</div>
<div id="section4-4" class="section level2">
<h2><span class="header-section-number">4.4</span> Guinea pig tooth growth analysis with Two-Way ANOVA</h2>
<p>The effects of dosage and delivery method of ascorbic acid on Guinea Pig odontoblast growth was analyzed as a One-Way ANOVA in Section <a href="chapter3.html#section3-4">3.4</a> by assessing evidence of any difference in the means of any combinations of dosage method (Vit C capsule vs Orange Juice) and three dosage amounts (0.5, 1, and 2 mg/day). Now we will consider the dosage and delivery methods as two separate variables and explore their potential interaction. A beanplot and interaction plot are provided in Figure <a href="chapter4.html#fig:Figure4-8">4.8</a>.</p>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(ToothGrowth)
<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))
<span class="kw">beanplot</span>(len <span class="op">~</span><span class="st"> </span>supp<span class="op">*</span>dose, <span class="dt">data=</span>ToothGrowth, <span class="dt">side=</span><span class="st">&quot;b&quot;</span>, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">5</span>,<span class="dv">40</span>),
         <span class="dt">main=</span><span class="st">&quot;Beanplot&quot;</span>, <span class="dt">col=</span><span class="kw">list</span>(<span class="st">&quot;white&quot;</span>,<span class="st">&quot;orange&quot;</span>), <span class="dt">xlab=</span><span class="st">&quot;Dosage&quot;</span>,
         <span class="dt">ylab=</span><span class="st">&quot;Tooth Growth&quot;</span>)
<span class="kw">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="dt">bty=</span><span class="st">&quot;n&quot;</span>, <span class="kw">c</span>(<span class="st">&quot;VC&quot;</span>,<span class="st">&quot;OJ&quot;</span>), <span class="dt">fill=</span><span class="kw">c</span>(<span class="st">&quot;white&quot;</span>,<span class="st">&quot;orange&quot;</span>))
<span class="kw">intplot</span>(len <span class="op">~</span><span class="st"> </span>supp<span class="op">*</span>dose, <span class="dt">data=</span>ToothGrowth, <span class="dt">col=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>), 
        <span class="dt">main=</span><span class="st">&quot;Interaction Plot&quot;</span>, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">5</span>,<span class="dv">40</span>))</code></pre></div>
<div class="figure"><span id="fig:Figure4-8"></span>
<img src="GreenwoodBanner_files/figure-html/Figure4-8-1.png" alt="(fig:Figure4-8)" width="672" />
<p class="caption">
Figure 4.8: (fig:Figure4-8)
</p>
</div>
<p>It appears that the effect of method changes based on the dosage as the interaction plot seems to show some evidence of non-parallel lines. Actually, it appears that the effect of delivery method is parallel for doses 0.5 and 1.0 mg/day but that the effect of delivery method changes for 2 mg/day.</p>
<p>We can use the ANOVA <span class="math inline">\(F\)</span>-test for an interaction to assess whether the interaction is “real” relative to the variability in the responses. That is, is it larger than we would expect due to natural variation in the data? If yes, then it is a real effect and we should account for it. The following results fit the interaction model and provide an ANOVA table.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">TG1 &lt;-<span class="st"> </span><span class="kw">lm</span>(len<span class="op">~</span>supp<span class="op">*</span>dose,<span class="dt">data=</span>ToothGrowth)
<span class="kw">Anova</span>(TG1)</code></pre></div>
<pre><code>## Anova Table (Type II tests)
## 
## Response: len
##            Sum Sq Df  F value    Pr(&gt;F)
## supp       205.35  1  12.3170 0.0008936
## dose      2224.30  1 133.4151 &lt; 2.2e-16
## supp:dose   88.92  1   5.3335 0.0246314
## Residuals  933.63 56</code></pre>
<p>The R output is reporting an interaction test result of <span class="math inline">\(F(1,56)=5.3\)</span> with a p-value of 0.025. But this should raise a red flag since the numerator degrees of freedom are not what we should expect of <span class="math inline">\((K-1)*(J-1) = (2-1)*(3-1)=2\)</span>. This brings up an issue in R when working with categorical variables. If the levels of a categorical variable are entered numerically, R will treat them as quantitative variables and not split out the different levels of the categorical variable. To make sure that R treats categorical variables the correct way, we should use the <code>factor</code> function on any variables that are categorical but are coded numerically in the data set. The following code creates a new variable called <code>dosef</code> using the function that will help us obtain correct results from the linear model. The re-run of the ANOVA table provides the correct analysis and the expected <span class="math inline">\(df\)</span> for the two rows of output involving <code>dosef</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ToothGrowth<span class="op">$</span>dosef&lt;-<span class="kw">factor</span>(ToothGrowth<span class="op">$</span>dose)
TG2 &lt;-<span class="st"> </span><span class="kw">lm</span>(len <span class="op">~</span><span class="st"> </span>supp<span class="op">*</span>dosef, <span class="dt">data=</span>ToothGrowth)
<span class="kw">Anova</span>(TG2)</code></pre></div>
<pre><code>## Anova Table (Type II tests)
## 
## Response: len
##             Sum Sq Df F value    Pr(&gt;F)
## supp        205.35  1  15.572 0.0002312
## dosef      2426.43  2  92.000 &lt; 2.2e-16
## supp:dosef  108.32  2   4.107 0.0218603
## Residuals   712.11 54</code></pre>
</div>
<div id="section4-5" class="section level2">
<h2><span class="header-section-number">4.5</span> Observational study example: The Psychology of Debt</h2>
</div>
<div id="section4-6" class="section level2">
<h2><span class="header-section-number">4.6</span> Pushing Two-Way ANOVA to the limit: Un-replicated designs</h2>
</div>
<div id="section4-7" class="section level2">
<h2><span class="header-section-number">4.7</span> Chapter summary</h2>
</div>
<div id="section4-8" class="section level2">
<h2><span class="header-section-number">4.8</span> Important R code</h2>
</div>
<div id="section4-9" class="section level2">
<h2><span class="header-section-number">4.9</span> Practice problems</h2>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="43">
<li id="fn43"><p>We will use “main effects” to refer to the two explanatory variables in the additive model even if they are not randomly assigned to contrast with having those variables interacting in the model. It is the one place where we use “effects” without worrying about random assignment.<a href="chapter4.html#fnref43">↩</a></p></li>
<li id="fn44"><p>In the standard ANOVA table, <span class="math inline">\(\text{SS}_A + \text{SS}_B + \text{SS}_{AB} + \text{SS}_E = \text{SS}_{\text{Total}}\)</span>. However, to get the tests we really desire when our designs are not balanced, a slight modification of the SS is used, using what are called Type II sums of squares and this result doesn’t hold in the output you will see for additive models. This is discussed further below.<a href="chapter4.html#fnref44">↩</a></p></li>
<li id="fn45"><p>Actually, the tests are only conditional on other main effects if Type II Sums of Squares are used for an interaction model.<a href="chapter4.html#fnref45">↩</a></p></li>
<li id="fn46"><p>In Multiple Linear Regression models in Chapter <a href="chapter8.html#chapter8">8</a>, the reasons for this wording will (hopefully) become clearer.<a href="chapter4.html#fnref46">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="chapter3.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="chapter5.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": ["GreenwoodBanner.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
