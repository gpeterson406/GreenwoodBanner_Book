\documentclass[]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={A Second Semester Statistics Course with R},
            pdfauthor={Mark Greenwood and Katherine Banner},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{natbib}
\bibliographystyle{apalike}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}
  \title{A Second Semester Statistics Course with R}
  \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
  \author{Mark Greenwood and Katherine Banner}
  \preauthor{\centering\large\emph}
  \postauthor{\par}
  \predate{\centering\large\emph}
  \postdate{\par}
<<<<<<< HEAD
  \date{2017-07-25}
=======
  \date{2017-07-24}
>>>>>>> origin/chapter0to1_edits

\usepackage{booktabs}
\usepackage[nottoc,numbib]{tocbibind}
\usepackage{amsmath}
\usepackage{color}
\usepackage{amsbsy}
\usepackage[normalem]{ulem}
\usepackage{cancel}
\usepackage[svgnames]{xcolor}
\usepackage{float}
\usepackage{caption}

\definecolor{purple}{RGB}{76,0,153}

% % make code-output smaller
% \DefineVerbatimEnvironment{Highlighting}{Verbatim}{fontsize=\tiny,commandchars=\\\{\}}
% 

% make console-output smaller:
  \makeatletter
\def\verbatim{\small\@verbatim \frenchspacing\@vobeyspaces \@xverbatim}
\makeatother


%\setlength{\parskip}{0pt}


\setlength{\OuterFrameSep}{-4pt}
\makeatletter
\def\preto{\@verbatim}{\topsep=-10pt \partopsep=-10pt }
\makeatother

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\newtheorem{corollary}{Corollary}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\chapter*{Acknowledgments}\label{acknowledgments}
\addcontentsline{toc}{chapter}{Acknowledgments}

We would like to thank all the students and instructors who have
provided input in the development of the current version of STAT 217 and
that have impacted the choice of topics and how we try to teach them.
Dr.~Robison-Cox initially developed this course using R and much of this
work retains his initial ideas. Many years of teaching these topics and
helping researchers use these topics has helped to refine how they are
presented here. Observing students years after the course has also
helped to refine what we try to teach in the course, trying to prepare
these students for the next levels of statistics courses that they might
encounter and the next class where they might need or want to use
statistics.

I (Greenwood) have intentionally taken a first person perspective at
times to be able to include stories from some of those interactions to
try to help you avoid some of their pitfalls in your current or future
usage of statistics. I would like to thank my wife, Teresa Greenwood,
for allowing me the time and support to work on this. I would also like
to acknowledge Dr.~Gordon Bril (Luther College) who introduced me to
statistics while I was an undergraduate and Dr.~Snehalata Huzurbazar
(University of Wyoming) that guided me to completing my Master's and
Ph.D.~in Statistics and still serves as a valued mentor and friend to
me.

The development of this text was initially supported with funding from
Montana State University's Instructional Innovation Grant Program with a
grant titled Towards more active learning in STAT 217. This book was
born with the goal of having a targeted presentation of topics that we
cover (and few that we don't) that minimizes cost to students and
incorporates the statistical software R from day one and every day after
that. The software is a free, open-source platform and so is dynamically
changing over time. This has necessitated frequent revisions of the
text.

This is Version 3.01 of the book. It fixes a problem created with the
digital links in the book that occurred during Spring 2017. Version 3.0
of the book, prepared for Fall 2016, involved edits, a couple of
partially new sections, and updated R code along with a new format for
how the R code is displayed to more easily distinguish it from other
text. Each revision has involved a similar amount of change with Version
2.0 published in January 2015 and Version 1.0 in January 2014 after
using draft chapters that were initially developed during Fall 2013.

We have made every attempt to keep costs as low as possible by making it
possible for most pages to be printed in black and white. The text (in
full color and with dynamic links) is also available as a free digital
download from Montana State University's ScholarWorks repository at
\url{https://scholarworks.montana.edu/xmlui/handle/1/2999}.

Enjoy your journey from introductory to intermediate statistics!

This work is licensed under the Creative Commons
Attribution-NonCommercial-NoDerivatives 4.0 International License. To
view a copy of this license, visit
\url{http://creativecommons.org/licenses/by-nc-nd/4.0/} or send a letter
to Creative Commons, 444 Castro Street, Suite 900, Mountain View,
California, 94041, USA.

<<<<<<< HEAD
\chapter{(R)e-Introduction to statistics}\label{chapter2}

The previous material served to get us started in R and to get a quick
review of same basic descriptive statistics. Now we will begin to engage
some new material and exploit the power of R to do some statistical
inference. Because inference is one of the hardest topics to master in
statistics, we will also review some basic terminology that is required
to move forward in learning more sophisticated statistical methods. To
keep this ``review'' as short as possible, we will not consider every
situation you learned in introductory statistics and instead focus
exclusively on the situation where we have a quantitative response
variable measured on two groups, adding a new graphic called a ``bean
plot'' to help us see the differences in the observations in the groups.

\section{Histograms, boxplots, and density curves}\label{section2-1}

Part of learning statistics is learning to correctly use the
terminology, some of which is used colloquially differently than it is
used in formal statistical settings. The most commonly ``misused'' term
is \textbf{\emph{data}}. In statistical parlance, we want to note the
plurality of data. Specifically, \textbf{\emph{datum}} is a single
measurement, possibly on multiple random variables, and so it is
appropriate to say that ``\textbf{a datum is\ldots{}}''. Once we move to
discussing data, we are now referring to more than one observation,
again on one, or possibly more than one, random variable, and so we need
to use ``\textbf{data are\ldots{}}'' when talking about our
observations. We want to distinguish our use of the term ``data'' from
its more colloquial\footnote{You will more typically hear ``data is''
  but that more often refers to information, sometimes even statistical
  summaries of data sets, than to observations collected as part of a
  study, suggesting the confusion of this term in the general public. We
  will explore a data set in Chapter 4 related to perceptions of this
  issue collected by researchers at \url{http://fivethirtyeight.com/}.}
usage that often involves treating it as singular. In a statistical
setting ``data'' refers to measurements of our cases or units. When we
summarize the results of a study (say providing the mean and SD), that
information is not ``data''. We used our data to generate that
information. Sometimes we also use the term ``data set'' to refer to all
our observations and this is a singular term to refer to the group of
observations and this makes it really easy to make mistakes on the usage
of this term.

It is also really important to note that \textbf{\emph{variables}} have
to vary -- if you measure the sex of your subjects but are only
measuring females, then you do not have a ``variable''. You may not know
if you have real variability in a ``variable'' until you explore the
results you obtained.

The last, but probably most important, aspect of data is the context of
the measurement. The ``who, what, when, and where'' of the collection of
the observations is critical to the sort of conclusions we can make
based on the results. The information on the study design provides
information required to assess the scope of inference of the study.
Generally, remember to think about the research questions the
researchers were trying to answer and whether their study actually would
answer those questions. There are no formulas to help us sort some of
these things out, just critical thinking about the context of the
measurements.

To make this concrete, consider the data collected from a study
\citep{Plaster1989} to investigate whether perceived physical
attractiveness had an impact on the sentences or perceived seriousness
of a crime that male jurors might give to female defendants. The
researchers showed the participants in the study (men who volunteered
from a prison) pictures of one of three young women. Each picture had
previously been decided to be either beautiful, average, or unattractive
by the researchers. Each ``juror'' was randomly assigned to one of three
levels of this factor (which is a categorical predictor or explanatory
variable) and then each rated their picture on a variety of traits such
as how warm or sincere the woman appeared. Finally, they were told the
women had committed a crime (also randomly assigned to either be told
she committed a burglary or a swindle) and were asked to rate the
seriousness of the crime and provide a suggested length of sentence. We
will bypass some aspects of their research and just focus on differences
in the sentence suggested among the three pictures. To get a sense of
these data, let's consider the first and last parts of the data set:



\begin{longtable}[]{@{}ccccccc@{}}
\caption{\label{tab:Table2-1} First 5 and last 6 rows of the Mock Jury data set.}\tabularnewline
\toprule
\begin{minipage}[b]{0.11\columnwidth}\centering\strut
Subject\strut
\end{minipage} & \begin{minipage}[b]{0.11\columnwidth}\centering\strut
Attr\strut
\end{minipage} & \begin{minipage}[b]{0.10\columnwidth}\centering\strut
Crime\strut
\end{minipage} & \begin{minipage}[b]{0.09\columnwidth}\centering\strut
Years\strut
\end{minipage} & \begin{minipage}[b]{0.11\columnwidth}\centering\strut
Serious\strut
\end{minipage} & \begin{minipage}[b]{0.16\columnwidth}\centering\strut
Independent\strut
\end{minipage} & \begin{minipage}[b]{0.10\columnwidth}\centering\strut
Sincere\strut
=======
\chapter{Two-Way ANOVA}\label{chapter4}

\section{Situation}\label{section4-1}

In this chapter, we extend the One-Way ANOVA to situations with two
factors or categorical explanatory variables in a method that is
generally called the \textbf{\emph{Two-Way ANOVA}}. This allows
researchers to simultaneously study more than one variable that might
explain variability in the responses and explore whether the impacts of
one variable change depending on the other variable. In some situations,
each observation is so expensive that researchers want to use a single
study to explore two different sets of research questions in the same
round of data collection. For example, a company might want to study
factors that affect the number of defective products per day and are
interested in the impacts of two different types of training programs
and three different levels of production quotas. These methods would
allow engineers to compare the training programs, production quotas, and
see if the training programs work differently for different production
quotas. In a clinical trials context, it is well known that certain
factors can change the performance of certain drugs. For example,
different dosages of a drug might have different benefits or
side-effects on men, versus women or children. \textbf{When the impact
of one factor changes on the level of another factor}, we say that they
\textbf{\emph{interact}}. It is also possible for both factors to be
related to differences in the mean responses and not interact. For
example, suppose there is a difference in the response means between men
and women and a difference among various dosages, but the effect of
increasing the dosage is the same for the male and female subjects. This
is an example of what is called an \textbf{\emph{additive}} type of
model. In general, the world is more complicated than the single factor
models we considered in Chapter \ref{chapter3} can account for,
especially in observational studies, so these models allow us to start
to handle more realistic situations.

Consider the following ``experiment'' where we want to compare the
strength of different brands of paper towels when they are wet. The
response variable will be the time to failure in seconds (a continuous
response variable) when a weight is placed on the towel held at the four
corners. We are interested in studying the differences between brands
and the impact of different amounts of water applied to the towels.

\begin{itemize}
\item
  Predictors (Explanatory Variables): \textbf{A} : \texttt{Brand} (2
  brands of interest, named \emph{B1} and \emph{B2}) and \textbf{B} :
  Number of \texttt{Drops} of water (10, 20, 30 drops).
\item
  Response: \emph{Time} to failure (in seconds) of a towel (\(y\)) with
  a weight sitting in the middle of the towel.
\end{itemize}

\section{Designing a two-way experiment and visualizing
results}\label{section4-2}

Ideally, we want to randomly assign the levels of each factor so that we
can attribute causality to any detected effects and to reduce the
chances of \emph{confounding}. Because there are two factors, we would
need to design a random assignment scheme to select the levels of both
variables. For example, we could randomly select a brand and then
randomly select the number of drops to apply from the levels chosen for
each measurement. Or we could decide on how many observations we want at
each combination of the two factors (ideally having them all equal so
the design is \textbf{\emph{balanced}}) and then randomize the order of
applying the different combinations of levels.

Why might it be important to randomly apply the brand and number of
drops in an experiment? There are situations where the order of
observations can be related to changes in the responses and we want to
be able to eliminate the order of observations from being related to the
levels of the factors. For example, suppose that the area where the
experiment is being performed becomes wet over time and the later
measurements have extra water that gets onto the paper towels and they
tend to fail more quickly. If all the observations for the second brand
were done later in the study, then the \emph{order of observations}
impacts could make the second brand look worse. If the order of
observations is randomized, then even if there is some drift in the
responses over the order of observations it should still be possible to
see the differences in the randomly assigned effects. If the study
incorporates repeated measurements on human subjects, randomizing the
order of treatments they are exposed to can alleviate impacts of them
``learning'' through the study, something that we would not have to
worry about with paper towels.

In observational studies, we do not have the luxury of random
assignment, that is, we cannot randomly assign levels of the treatment
variables to our subjects, so we cannot guarantee that the only
difference between the groups are the explanatory variables. As
discussed before, because we can't control which level of the variables
are assigned to the subjects, we cannot make causal inferences and have
to worry about other variables being the real drivers of the results.
Although we can never establish causal inference with observational
studies, we can generalize our results to a larger population if we have
a representative sample from our population of interest.

It is also possible that we might have studies where some of the
variables are randomly assigned and others are not randomly assignable.
The most common versions of this are what we sometimes call subject
``demographics'', such as sex, income, race, etc. We might be performing
a study where we can randomly assign treatments to these subjects but
might also want to account for differences based on income level, which
we can't assign. In these cases, the scope of inference gets complicated
-- differences seen on randomized variables can be causally interpreted
but you have to be careful to not say that the demographics caused
differences. Suppose that a randomly assigned drug dosage is found to
show differences in male patients but not in female patients. We could
say that the dosage causes differences in males but does not in females.
We are not saying that sex caused the differences but that the causal
differences were modified by the sex of the subjects.

Even when we do have random assignment of treatments it is important to
think about who/what is included in the sample. To get back to the paper
towel example, we are probably interested in more than the sheets of the
rolls we have to work with so if we could randomly select the studied
paper towels from all paper towels made by each brand, our conclusions
could be extended to those populations. That probably would not be
practical, but trying to make sure that the towels are representative of
all made by each brand by checking for defects and maybe picking towels
from a few different rolls would be a good start to being able to extend
inferences beyond the tested towels.

Once random assignment and random sampling is settled, the final aspect
of study design involves deciding on the number of observations that
should be made. The short (glib) answer is to take as many as you can
afford. With more observations comes higher power to detect differences
if they exist, which is a desired attribute of all studies. It is also
important to make sure that you obtain multiple observations at each
combination of the treatment levels, which are called
\textbf{\emph{replicates}}. Having replicate measurements allows
estimation of the mean for each combination of the treatment levels as
well as estimation and testing for an interaction. And we always prefer
having balanced designs because they provide resistance to violation of
some assumptions as noted in Chapter \ref{chapter3}. A
\textbf{\emph{balanced design}} in a Two-Way ANOVA setting involves
having the same sample size for every combination of the levels of the
treatments.

With two categorical explanatory variables, there are now five possible
scenarios for the truth. Different situations are created depending on
whether there is an interaction between the two variables, whether both
variables are important but do not interact, or whether either of the
variables matter at all. Basically, there are five different possible
outcomes in a randomized Two-Way ANOVA study, listed in order of
increasing model complexity:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Neither A or B has an effect on the responses (nothing causes
  differences in responses).
\item
  A has an effect, B does not (only A causes differences in responses).
\item
  B has an effect, A does not (only B causes differences in responses).
\item
  Both A and B have effects on response but no interaction (A and B both
  cause differences in responses but the impacts are additive).
\item
  Effect of A differs based on the levels of B, the opposite is also
  true (means for levels of A are different for different levels of B,
  or, simply, A and B interact).
\end{enumerate}

To illustrate these five potential outcomes, we will consider a fake
version of the paper towel example. It ended up being really messy and
complicated to actually perform the experiment as we described it so
these data were simulated to help us understand the Two-Way ANOVA
possibilities in as simple a situation as possible. The first step is to
understand what has been observed (number observations at each
combination of factors) and look at some summary statistics across all
the ``groups''. The data set is available from the course website using:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pt<-}\KeywordTok{read.csv}\NormalTok{(}\StringTok{"http://www.math.montana.edu/courses/s217/documents/pt.csv"}\NormalTok{)}
\NormalTok{pt}\OperatorTok{$}\NormalTok{drops<-}\KeywordTok{factor}\NormalTok{(pt}\OperatorTok{$}\NormalTok{drops)}
\end{Highlighting}
\end{Shaded}

The data set contains five observations per combination of treatment
levels as provided by the \texttt{tally} function. To get counts for
combinations of the variables, use the general formula of
\texttt{tally(x1\textasciitilde{}x2,\ data=...)} although the order of
\texttt{x1} and \texttt{x2} doesn't matter:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{require}\NormalTok{(mosaic)}
\KeywordTok{tally}\NormalTok{(brand }\OperatorTok{~}\StringTok{ }\NormalTok{drops, }\DataTypeTok{data=}\NormalTok{pt)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      drops
## brand 10 20 30
##    B1  5  5  5
##    B2  5  5  5
\end{verbatim}

The sample sizes in each of the six treatment level combinations of
\texttt{Brand} and \texttt{Drops} {[}(\emph{B1}, 10), (\emph{B1}, 20),
(\emph{B1}, 30), (\emph{B2}, 10), (\emph{B2}, 20), (\emph{B2}, 30){]}
are \(n_{jk} = 5\) for \(j^{th}\) level of \texttt{Brand} (\(j=1, 2\))
and \(k^{th}\) level of \texttt{Drops} (\(k=1, 2, 3\)). The
\texttt{tally} function gives us a \textbf{\emph{contingency table}}
with \(R = 2\) rows (\emph{B1}, \emph{B2}) and \(C = 3\) columns (10,
20, and 30). We'll have more fun with this sort of summary of \(R\) by
\(C\) tables in Chapter \ref{chapter5} -- here it helps us see the
sample size in each combination of factor levels. The \texttt{favstats}
function also helps us dig into the results for all combinations of
factor levels. The notation involves putting both variables after the
``\textasciitilde{}'' with a ``\texttt{+}'' between them. In the output,
the first row contains summary information for the 5 observations for
\texttt{Brand} \emph{B1} and \texttt{Drops} amount 10. It also contains
the sample size in the \texttt{n} column, although here it rolled into a
new set of rows with the standard deviations.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{favstats}\NormalTok{(responses }\OperatorTok{~}\StringTok{ }\NormalTok{brand }\OperatorTok{+}\StringTok{ }\NormalTok{drops, }\DataTypeTok{data=}\NormalTok{pt)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   brand.drops       min        Q1   median       Q3      max     mean
## 1       B1.10 0.3892621 1.3158737 1.906436 2.050363 2.333138 1.599015
## 2       B2.10 2.3078095 2.8556961 3.001147 3.043846 3.050417 2.851783
## 3       B1.20 0.3838299 0.7737965 1.516424 1.808725 2.105380 1.317631
## 4       B2.20 1.1415868 1.9382142 2.066681 2.838412 3.001200 2.197219
## 5       B1.30 0.2387500 0.9804284 1.226804 1.555707 1.829617 1.166261
## 6       B2.30 0.5470565 1.1205102 1.284117 1.511692 2.106356 1.313946
##          sd n missing
## 1 0.7714970 5       0
## 2 0.3140764 5       0
## 3 0.7191978 5       0
## 4 0.7509989 5       0
## 5 0.6103657 5       0
## 6 0.5686485 5       0
\end{verbatim}

The next step is to visually explore the results across the combinations
of the two explanatory variables. The beanplot can be extended to handle
these sorts of two-way situations only if one of the two variables is a
two-level variable. This is a pretty serious constraint on this display,
so we will show you the plot (Figure \ref{fig:Figure4-1}) but not focus
on the code. The reason beanplots can only handle \(2 \times K\) designs
is that the beans are split along a vertical line for the \(K\) levels
of the other variable. In Figure \ref{fig:Figure4-1}, the \texttt{Brand}
B1 density curves are shaded and the B2 curves are not. In reading these
plots, look for differences in each level and whether those differences
change across the levels of the other variable. Specifically, start with
comparing the two brands for different amounts of water. Do the brands
seem different? Certainly for 10 drops of water the two look different
but not for 30 drops. We can also look for combinations of factors that
produce the highest or lower responses in this display. It appears that
the time to failure is highest in the low water drop groups but as the
water levels increase, the time to failure falls and the differences in
the two brands seem to decrease. The fake data seem to have relatively
similar amounts of variability and distribution shapes -- remembering
that there are only 5 observations available for describing the shape of
responses for each combination. These data were simulated using a normal
distribution and constant variance if that gives you some extra
confidence in assessing these model assumptions.




\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{require}\NormalTok{(beanplot)}
\KeywordTok{beanplot}\NormalTok{(responses }\OperatorTok{~}\StringTok{ }\NormalTok{brand}\OperatorTok{*}\NormalTok{drops, }\DataTypeTok{data=}\NormalTok{pt, }\DataTypeTok{side=}\StringTok{"b"}\NormalTok{, }\DataTypeTok{col=}\KeywordTok{list}\NormalTok{(}\StringTok{"lightblue"}\NormalTok{,}\StringTok{"white"}\NormalTok{),}
         \DataTypeTok{xlab=}\StringTok{"Drops"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"Time"}\NormalTok{, }\DataTypeTok{method=}\StringTok{"jitter"}\NormalTok{,}\DataTypeTok{log=}\StringTok{""}\NormalTok{)}
\KeywordTok{legend}\NormalTok{(}\StringTok{"topright"}\NormalTok{, }\DataTypeTok{bty=}\StringTok{"n"}\NormalTok{, }\KeywordTok{c}\NormalTok{(}\StringTok{"B1"}\NormalTok{,}\StringTok{"B2"}\NormalTok{), }\DataTypeTok{fill=}\KeywordTok{c}\NormalTok{(}\StringTok{"lightblue"}\NormalTok{,}\StringTok{"white"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{04-twoWayAnova_files/figure-latex/Figure4-1-1.pdf}
\caption{\label{fig:Figure4-1}Beanplot of paper towel data by \texttt{Drops} (x-axis) and
\texttt{Brand} (side of bean, shaded area for \texttt{Brand} \emph{B1}.}
\end{figure}

The beanplots can't handle situations where both variables have more
than two levels -- we need a simpler display that just focuses on the
means at the combinations of the two explanatory variables. The means
for each combination of levels that you can find in the
\texttt{favstats} output are more usefully used in what is called an
\textbf{\emph{interaction plot}}. Interaction plots display the mean
responses (y-axis) versus levels of one predictor variable on the
x-axis, adding points and lines for each level of the other predictor
variable. Because we don't like any of the available functions in R, we
wrote our own function, called \texttt{intplot} that you can download
using:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{source}\NormalTok{(}\StringTok{"http://www.math.montana.edu/courses/s217/documents/intplot.R"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The function allows a formula interface like
\texttt{Y\textasciitilde{}X1*X2} and provides the means \(\pm\) 1 SE
(vertical bars) and adds a legend to help make everything clear.




\begin{figure}
\centering
\includegraphics{04-twoWayAnova_files/figure-latex/Figure4-2-1.pdf}
\caption{\label{fig:Figure4-2}Interaction plot of the paper towel data with
\texttt{Drops} on the x-axis.}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{intplot}\NormalTok{(responses }\OperatorTok{~}\StringTok{ }\NormalTok{brand}\OperatorTok{*}\NormalTok{drops, }\DataTypeTok{data=}\NormalTok{pt)}
\end{Highlighting}
\end{Shaded}

Interaction plots can always be made two different ways by switching the
order of the variables. Figure \ref{fig:Figure4-2} contains
\texttt{Drops} on the x-axis and Figure \ref{fig:Figure4-3} has
\texttt{Brand} on the x-axis. Typically putting the variable with more
levels on the x-axis will make interpretation easier, but not always.
Try both and decide on the one that you like best.




\begin{figure}
\centering
\includegraphics{04-twoWayAnova_files/figure-latex/Figure4-3-1.pdf}
\caption{\label{fig:Figure4-3}Interaction plot of paper towel data with \texttt{Brand} on
the x-axis.}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{intplot}\NormalTok{(responses }\OperatorTok{~}\StringTok{ }\NormalTok{drops}\OperatorTok{*}\NormalTok{brand, }\DataTypeTok{data=}\NormalTok{pt)}
\end{Highlighting}
\end{Shaded}

The formula in this function builds on our previous notation and now we
include both predictor variables with an ``*" between them. Using an
asterisk between explanatory variables is one way of telling R to
include an interaction between the variables. While the interaction may
or may not be present, the interaction plot helps us to explore those
potential differences.

There are a variety of aspects of the interaction plots to pay attention
to. Initially, the question to answer is whether it appears that there
is an interaction between the predictor variables. When there is an
interaction, you will see \textbf{\emph{non-parallel lines}} in the
interaction plot. You want to look from left to right in the plot and
assess whether the lines are close to parallel, relative to the amount
of variability in the means. If it seems that there is clear visual
evidence of non-parallel lines, then the interaction is likely worth
considering (we will typically use a hypothesis test to formally assess
this -- see discussion below). If the lines look to be close to
parallel, then there probably isn't an interaction between the
variables. Without an interaction present, that means that the
differences across levels of one variable doesn't change based on the
levels of the other variable and vice-versa. This means that we can
consider the \textbf{\emph{main effects}} of each variable on their
own\footnote{We will use ``main effects'' to refer to the two
  explanatory variables in the additive model even if they are not
  randomly assigned to contrast with having those variables interacting
  in the model. It is the one place where we use ``effects'' without
  worrying about random assignment.}. Main effects are much like the
results we found in Chapter \ref{chapter3} where we can compare means
across levels of a single variable except that there are results for two
variables to extract from the model. With the presence of an
interaction, it is complicated to summarize how each variable is
affecting the response variable because their impacts change depending
on the level of the other factor. And plots like the interaction plot
provide us much useful information.

If the lines are not parallel, then focus in on comparing the levels of
one variable as the other variable changes. Remember that the definition
of an interaction is that the differences among levels of one variable
depends on the level of the other variable being considered.
``Visually'' this means comparing the size of the differences in the
lines from left to right. In Figures \ref{fig:Figure4-2} and
\ref{fig:Figure4-3}, the effect of amount of water changes based on the
brand being considered. In Figure \ref{fig:Figure4-3}, the three lines
represent the three water levels. The difference between the brands
(left to right, \emph{B1} to \emph{B2}) is different depending on how
much water was present. It appears that \texttt{Brand} \emph{B2} lasted
longer at the lower water levels but that the difference between the two
brands dropped as the water levels increased. The same story appears in
Figure \ref{fig:Figure4-2}. As the water levels increase (left to right,
10 to 20 to 30 drops), the differences between the two brands decrease.
Of the two versions, Figure \ref{fig:Figure4-2} is probably easier to
read here. The interaction plots also are useful for identifying the
best and worst mean responses for combinations of the treatment levels.
For example, 10 \texttt{Drops} and \texttt{Brand} \emph{B2} lasts
longest, on average, and 30 \texttt{Drops} with \texttt{Brand} \emph{B1}
fails fastest, on average. In this situation, the lines do not appear to
be parallel suggesting that further exploration of the interaction
appears to be warranted.

Before we get to the hypothesis tests to formally make this assessment
(you knew some sort of p-value was coming, right?), we can visualize the
5 different scenarios that could characterize the sorts of results you
could observe in a Two-Way ANOVA situation. Figure \ref{fig:Figure4-4}
shows 4 of the 5 scenarios. In panel (a), when there are no differences
from either variable (Scenario 1), it provides relatively parallel lines
and basically no differences either across \texttt{Drops} levels
(x-axis) or \texttt{Brand} (lines). This would result in no evidence
related to a difference in brands, water levels, or any interaction
between them.




\begin{figure}
\centering
\includegraphics{04-twoWayAnova_files/figure-latex/Figure4-4-1.pdf}
\caption{\label{fig:Figure4-4}Interaction plots of four possible scenarios in the paper
towel study.}
\end{figure}

Scenario 2 (Figure \ref{fig:Figure4-4} panel (b)) incorporates
differences based on factor A (here that is \texttt{Brand}) but no real
difference based on the \texttt{Drops} or any interaction. This results
in a clear shift between the little to no changes in the level of those
lines across water levels. These lines are relatively parallel. We can
see that \texttt{Brand} \emph{B2} is better than \texttt{Brand}
\emph{B1} but that is all we can show with these sorts of results.

Scenario 3 (Figure \ref{fig:Figure4-4} panel (c)) flips the important
variable to B (\texttt{Drops}) and shows decreasing average times as the
water levels increase. Again, the interaction panels show near
parallel-ness in the lines and really just show differences among the
levels of the water. In both Scenarios 2 and 3, we could use a single
variable and drop the other from the model, getting back to a One-Way
ANOVA model, without losing any important information.

Scenario 4 (Figure \ref{fig:Figure4-4} panel (d)) incorporates effects
of A and B, but they are \textbf{\emph{additive}}. That means that the
effect of one variable is the same across the levels of the other
variable. In this experiment, that would mean that \texttt{Drops} has
the same impact on performance regardless of brand and that the brands
differ but each type of difference is the same regardless of levels of
the other variable. The interaction plot lines are more or less parallel
but now the brands are clearly different from each other. The plot shows
the decrease in performance based on increasing water levels and that
\texttt{Brand} \emph{B2} is better than \texttt{Brand} \emph{B1}.
Additive effects show the same difference in lines from left to right in
the interaction plots.

Finally, Scenario 5 (Figure \ref{fig:Figure4-5}) involves an interaction
between the two variables (\texttt{Drops} and \texttt{Brand}). There are
many ways that interactions can present but the main thing is to look
for clearly non-parallel lines. As noted in the previous discussion, the
\texttt{Drops} effect appears to change depending on which level of
\texttt{Brand} is being considered. Note that the plot here described as
Scenario 5 is the same as the initial plot of the results in Figure
\ref{fig:Figure4-2}.




\begin{figure}
\centering
\includegraphics{04-twoWayAnova_files/figure-latex/Figure4-5-1.pdf}
\caption{\label{fig:Figure4-5}Interaction plot of Scenario 5 where it appears that an
interaction is present.}
\end{figure}

The typical modeling protocol is to start with assuming that Scenario 5
is a possible description of the results, related to fitting what is
called the \textbf{\emph{interaction model}}, and then attempt to
simplify the model (to the \textbf{\emph{additive model}}) if warranted.
We need a hypothesis test to help decide if the interaction is ``real''
-- if there is sufficient evidence to prove that there is an
interaction. We need a test because the lines will never be exactly
parallel and, just like in the One-Way ANOVA situation, the amount of
variation around the lines impacts the ability of the model to detect
differences, in this case of an interaction.

\section{Two-Way ANOVA models and hypothesis tests}\label{section4-3}

To assess interactions with two variables, we need to fully describe
models for the additive and interaction scenarios and then develop a
method for assessing evidence of the need for different aspects of the
models. First, we need to define the notation for these models:

\begin{itemize}
\item
  \(y_{ijk}\) is the \(i^{th}\) response from the group for level \(j\)
  of factor A and level \(k\) of factor B

  \begin{itemize}
  \item
    \(j=1,\ldots,J\) ~~~ \(J\) is the number of levels of A
  \item
    \(k=1,\ldots,K\) ~~~ \(K\) is the number of levels of B
  \item
    \(i=1,\ldots,n_{jk}\) ~~~ \(n_{jk}\) is the sample size for level
    \(j\) of factor A and level \(k\) of factor B
  \item
    \(N=\Sigma\Sigma n_{jk}\) is the total sample size (sum of the
    number of observations across all \(JK\) groups)
  \end{itemize}
\end{itemize}

We need to extend our previous discussion of reference-coded models to
develop a Two-Way ANOVA model. We start with the \textbf{\emph{Two-Way
ANOVA interaction model}}:

\[y_{ijk} = \alpha + \tau_j + \gamma_k + \omega_{jk} + \varepsilon_{ijk},\]

where \(\alpha\) is the baseline group mean (for level 1 of A
\textbf{and} level 1 of B), \(\tau_j\) is the deviation for the
\textbf{\emph{main effect}} of A from the baseline for levels
\(2,\ldots,J\), \(\gamma_k\) (gamma \(k\)) is the deviation for the main
effect of B from the baseline for levels \(2,\ldots,K\), and
\(\omega_{jk}\) (omega \(jk\)) is the adjustment for the
\textbf{\emph{interaction effect}} for level \(j\) of factor A and level
\(k\) of factor B for \(j=1,\ldots,J\) and \(k=1,\ldots,K\). In this
model, \(\tau_1\), \(\gamma_1\), and \(\omega_{11}\) are all fixed at 0.
As in Chapter \ref{chapter3}, R will choose the baseline categories
alphabetically but now it is choosing a baseline for both variables and
so our detective work will be doubled to sort this out.

If the interaction term is not important, based on the interaction test
presented below, the \(\omega_{jk}\text{'s}\) can be dropped from the
model and we get a model that corresponds to Scenario 4 above. Scenario
4 is where there are two main effects but no interaction between them.
The \textbf{\emph{additive Two-Way model}} is

\[y_{ijk} = \alpha + \tau_j + \gamma_k + \varepsilon_{ijk},\]

where each component is defined as in the interaction model. The
difference between the interaction and additive models is setting all
the \(\omega_{jk}\text{'s}\) to 0 that are present in the interaction
model. When we set parameters to 0 in models it removes them from the
model. Setting parameters to 0 is how we will develop our hypotheses to
test for an interaction, by testing whether there is evidence enough to
reject that all \(\omega_{jk}\text{'s}=0\).

The interaction test hypotheses are

\begin{itemize}
\item
  \(H_0\): No interaction between A and B in population
  \(\Leftrightarrow\) All \(\omega_{jk}\text{'s}=0\).
\item
  \(H_A\): Interaction between A and B in population \(\Leftrightarrow\)
  At least one \(\omega_{jk}\ne 0\)
\end{itemize}

To perform this test, a new ANOVA \(F\)-test is required (presented
below) but there are also hypotheses relating to the main effects of A
(\(\tau_j\text{'s}\)) and B (\(\gamma_k\text{'s}\)). If evidence is
found to reject the null hypothesis that no interaction is present, then
it is dangerous to ignore it and test for the main effects because
important main effects can be masked by interactions (examples later).
It is important to note that, by definition, \textbf{both variables
matter if an interaction is found to be important} so the main effect
tests may not be very interesting. If the interaction is found to be
important based on the test and retained in the model, you should focus
on the interaction model (also called the \textbf{\emph{full model}}) in
order to understand and describe the form of the interaction among the
variables.

If the interaction test does not return a small p-value, then we have no
evidence to suggest that it is needed and it can be dropped from the
model. In this situation, we would re-fit the model and focus on the
results provided by the additive model -- performing tests for the two
additive main effects. For the first, but not last time, we encounter a
model with more than one variable and test of potential interest. In
models with multiple variables at similar levels (here both are main
effects), we are interested in the results for each variable given that
the other variable is in the model. In many situations, including more
than one variable in a model changes the results for the other variable
even if those variables do not interact. The reason for this is more
clear in Chapter \ref{chapter8} and really only matters here if we have
unbalanced designs, but we need to start adding a short modifier to our
discussions of main effects -- they are the results \emph{conditional
on} or \emph{adjusting for} or, simply, \emph{given}, the other
variable(s) in the model. Specifically, the hypotheses for the two main
effects are:

\begin{itemize}
\item
  Main effect test for A:

  \begin{itemize}
  \item
    \(H_0\): No differences in means across levels of A in population,
    given B in the model \(\Leftrightarrow\) All \(\tau_j\text{'s} = 0\)
    in additive model.
  \item
    \(H_A\): Some difference in means across levels A in population,
    given B in the model \(\Leftrightarrow\) At least one
    \(\tau_j \ne 0\), in additive model.
  \end{itemize}
\item
  Main effect test for B:

  \begin{itemize}
  \item
    \(H_0\): No differences in means across levels of B in population,
    given A in the model \(\Leftrightarrow\) All
    \(\gamma_k\text{'s} = 0\) in additive model.
  \item
    \(H_A\): Some difference in means across levels B in population,
    given A in the model \(\Leftrightarrow\) At least one
    \(\gamma_k \ne 0\), in additive model.
  \end{itemize}
\end{itemize}

In order to test these effects (interaction in the interaction model and
main effects in the additive model), \(F\)-tests are developed using
Sums of Squares, Mean Squares, and degrees of freedom similar to those
in Chapter \ref{chapter3}. We won't worry about the details of the sums
of squares formulas but you should remember the sums of squares
decomposition, which still applies\footnote{In the standard ANOVA table,
  \(\text{SS}_A + \text{SS}_B + \text{SS}_{AB} + \text{SS}_E = \text{SS}_{\text{Total}}\).
  However, to get the tests we really desire when our designs are not
  balanced, a slight modification of the SS is used, using what are
  called Type II sums of squares and this result doesn't hold in the
  output you will see for additive models. This is discussed further
  below.}. Table \ref{tab:Table4-1} summarizes the ANOVA results you
will obtain for the interaction model and Table \ref{tab:Table4-2}
provides the similar general results for the additive model. As we saw
in Chapter \ref{chapter3}, the degrees of freedom are the amount of
information that is free to vary at a particular level and that rule
generally holds here. For example, for factor A with \(J\) levels, there
are \(J-1\) parameters that are free since the baseline is fixed. The
residual degrees of freedom for both models are not as easily explained
but have simple formula. Note that the sum of the degrees of freedom
from the main effects, (interaction if present), and error need to equal
\(N-1\), just like in the One-Way ANOVA table.



\begin{longtable}[]{@{}lllll@{}}
\caption{\label{tab:Table4-1} Interaction Model ANOVA Table.}\tabularnewline
\toprule
\begin{minipage}[b]{0.16\columnwidth}\raggedright\strut
\textbf{Source}~~~~\strut
\end{minipage} & \begin{minipage}[b]{0.17\columnwidth}\raggedright\strut
\textbf{DF}~~~~~\strut
\end{minipage} & \begin{minipage}[b]{0.19\columnwidth}\raggedright\strut
\textbf{SS}\strut
\end{minipage} & \begin{minipage}[b]{0.21\columnwidth}\raggedright\strut
\textbf{MS}\strut
\end{minipage} & \begin{minipage}[b]{0.12\columnwidth}\raggedright\strut
\textbf{F-statistics}\strut
>>>>>>> origin/chapter0to1_edits
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
<<<<<<< HEAD
\begin{minipage}[b]{0.11\columnwidth}\centering\strut
Subject\strut
\end{minipage} & \begin{minipage}[b]{0.11\columnwidth}\centering\strut
Attr\strut
\end{minipage} & \begin{minipage}[b]{0.10\columnwidth}\centering\strut
Crime\strut
\end{minipage} & \begin{minipage}[b]{0.09\columnwidth}\centering\strut
Years\strut
\end{minipage} & \begin{minipage}[b]{0.11\columnwidth}\centering\strut
Serious\strut
\end{minipage} & \begin{minipage}[b]{0.16\columnwidth}\centering\strut
Independent\strut
\end{minipage} & \begin{minipage}[b]{0.10\columnwidth}\centering\strut
Sincere\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.11\columnwidth}\centering\strut
1\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering\strut
Beautiful\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering\strut
Burglary\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\centering\strut
10\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering\strut
8\strut
\end{minipage} & \begin{minipage}[t]{0.16\columnwidth}\centering\strut
9\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering\strut
8\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.11\columnwidth}\centering\strut
2\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering\strut
Beautiful\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering\strut
Burglary\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\centering\strut
3\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering\strut
8\strut
\end{minipage} & \begin{minipage}[t]{0.16\columnwidth}\centering\strut
9\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering\strut
3\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.11\columnwidth}\centering\strut
3\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering\strut
Beautiful\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering\strut
Burglary\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\centering\strut
5\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering\strut
5\strut
\end{minipage} & \begin{minipage}[t]{0.16\columnwidth}\centering\strut
6\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering\strut
3\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.11\columnwidth}\centering\strut
4\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering\strut
Beautiful\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering\strut
Burglary\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\centering\strut
1\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering\strut
3\strut
\end{minipage} & \begin{minipage}[t]{0.16\columnwidth}\centering\strut
9\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering\strut
8\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.11\columnwidth}\centering\strut
5\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering\strut
Beautiful\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering\strut
Burglary\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\centering\strut
7\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering\strut
9\strut
\end{minipage} & \begin{minipage}[t]{0.16\columnwidth}\centering\strut
5\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering\strut
1\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.11\columnwidth}\centering\strut
\ldots{}\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering\strut
\ldots{}\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering\strut
\ldots{}\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\centering\strut
\ldots{}\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering\strut
\ldots{}\strut
\end{minipage} & \begin{minipage}[t]{0.16\columnwidth}\centering\strut
\ldots{}\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering\strut
\ldots{}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.11\columnwidth}\centering\strut
108\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering\strut
Average\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering\strut
Swindle\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\centering\strut
3\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering\strut
3\strut
\end{minipage} & \begin{minipage}[t]{0.16\columnwidth}\centering\strut
5\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering\strut
4\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.11\columnwidth}\centering\strut
109\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering\strut
Average\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering\strut
Swindle\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\centering\strut
3\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering\strut
2\strut
\end{minipage} & \begin{minipage}[t]{0.16\columnwidth}\centering\strut
9\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering\strut
9\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.11\columnwidth}\centering\strut
110\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering\strut
Average\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering\strut
Swindle\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\centering\strut
2\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering\strut
1\strut
\end{minipage} & \begin{minipage}[t]{0.16\columnwidth}\centering\strut
8\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering\strut
8\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.11\columnwidth}\centering\strut
111\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering\strut
Average\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering\strut
Swindle\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\centering\strut
7\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering\strut
4\strut
\end{minipage} & \begin{minipage}[t]{0.16\columnwidth}\centering\strut
9\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering\strut
1\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.11\columnwidth}\centering\strut
112\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering\strut
Average\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering\strut
Swindle\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\centering\strut
6\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering\strut
3\strut
\end{minipage} & \begin{minipage}[t]{0.16\columnwidth}\centering\strut
5\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering\strut
2\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.11\columnwidth}\centering\strut
113\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering\strut
Average\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering\strut
Swindle\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\centering\strut
12\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering\strut
9\strut
\end{minipage} & \begin{minipage}[t]{0.16\columnwidth}\centering\strut
9\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering\strut
1\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.11\columnwidth}\centering\strut
114\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering\strut
Average\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering\strut
Swindle\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\centering\strut
8\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering\strut
8\strut
\end{minipage} & \begin{minipage}[t]{0.16\columnwidth}\centering\strut
1\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering\strut
5\strut
=======
\begin{minipage}[b]{0.16\columnwidth}\raggedright\strut
\textbf{Source}~~~~\strut
\end{minipage} & \begin{minipage}[b]{0.17\columnwidth}\raggedright\strut
\textbf{DF}~~~~~\strut
\end{minipage} & \begin{minipage}[b]{0.19\columnwidth}\raggedright\strut
\textbf{SS}\strut
\end{minipage} & \begin{minipage}[b]{0.21\columnwidth}\raggedright\strut
\textbf{MS}\strut
\end{minipage} & \begin{minipage}[b]{0.12\columnwidth}\raggedright\strut
\textbf{F-statistics}\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.16\columnwidth}\raggedright\strut
A\strut
\end{minipage} & \begin{minipage}[t]{0.17\columnwidth}\raggedright\strut
\(J-1\)\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\raggedright\strut
\(\text{SS}_A\)\strut
\end{minipage} & \begin{minipage}[t]{0.21\columnwidth}\raggedright\strut
\(\text{MS}_A=\text{SS}_A/\text{df}_A\)\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedright\strut
\(\text{MS}_A/\text{MS}_E\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.16\columnwidth}\raggedright\strut
B\strut
\end{minipage} & \begin{minipage}[t]{0.17\columnwidth}\raggedright\strut
\(K-1\)\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\raggedright\strut
\(\text{SS}_B\)\strut
\end{minipage} & \begin{minipage}[t]{0.21\columnwidth}\raggedright\strut
\(\text{MS}_B=\text{SS}_B/\text{df}_B\)\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedright\strut
\(\text{MS}_B/\text{MS}_E\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.16\columnwidth}\raggedright\strut
A:B (interaction)\strut
\end{minipage} & \begin{minipage}[t]{0.17\columnwidth}\raggedright\strut
\((J-1)(K-1)\)\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\raggedright\strut
\(\text{SS}_{AB}\)\strut
\end{minipage} & \begin{minipage}[t]{0.21\columnwidth}\raggedright\strut
\(\text{MS}_{AB}=\text{SS}_{AB}/\text{df}_{AB}\)\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedright\strut
\(\text{MS}_{AB}/\text{MS}_E\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.16\columnwidth}\raggedright\strut
Error\strut
\end{minipage} & \begin{minipage}[t]{0.17\columnwidth}\raggedright\strut
\(N-J-K+1\)\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\raggedright\strut
\(\text{SS}_E\)\strut
\end{minipage} & \begin{minipage}[t]{0.21\columnwidth}\raggedright\strut
\(\text{MS}_E=\text{SS}_E/\text{df}_E\)\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedright\strut
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.16\columnwidth}\raggedright\strut
\textcolor{red}{\textbf{Total}}\strut
\end{minipage} & \begin{minipage}[t]{0.17\columnwidth}\raggedright\strut
\(\color{red}{\mathbf{N-1}}\)\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\raggedright\strut
\(\color{red}{\textbf{SS}_{\textbf{Total}}}\)\strut
\end{minipage} & \begin{minipage}[t]{0.21\columnwidth}\raggedright\strut
\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedright\strut
\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}



\begin{longtable}[]{@{}lllll@{}}
\caption{\label{tab:Table4-2} Additive Model ANOVA Table.}\tabularnewline
\toprule
\begin{minipage}[b]{0.17\columnwidth}\raggedright\strut
\textbf{Source}~~~~\strut
\end{minipage} & \begin{minipage}[b]{0.18\columnwidth}\raggedright\strut
\textbf{DF}~~~~~\strut
\end{minipage} & \begin{minipage}[b]{0.21\columnwidth}\raggedright\strut
\textbf{SS}\strut
\end{minipage} & \begin{minipage}[b]{0.18\columnwidth}\raggedright\strut
\textbf{MS}\strut
\end{minipage} & \begin{minipage}[b]{0.12\columnwidth}\raggedright\strut
\textbf{F-statistics}\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.17\columnwidth}\raggedright\strut
\textbf{Source}~~~~\strut
\end{minipage} & \begin{minipage}[b]{0.18\columnwidth}\raggedright\strut
\textbf{DF}~~~~~\strut
\end{minipage} & \begin{minipage}[b]{0.21\columnwidth}\raggedright\strut
\textbf{SS}\strut
\end{minipage} & \begin{minipage}[b]{0.18\columnwidth}\raggedright\strut
\textbf{MS}\strut
\end{minipage} & \begin{minipage}[b]{0.12\columnwidth}\raggedright\strut
\textbf{F-statistics}\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.17\columnwidth}\raggedright\strut
A\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\raggedright\strut
\(J-1\)\strut
\end{minipage} & \begin{minipage}[t]{0.21\columnwidth}\raggedright\strut
\(\text{SS}_A\)\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\raggedright\strut
\(\text{MS}_A=\text{SS}_A/\text{df}_A\)\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedright\strut
\(\text{MS}_A/\text{MS}_E\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.17\columnwidth}\raggedright\strut
B\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\raggedright\strut
\(K-1\)\strut
\end{minipage} & \begin{minipage}[t]{0.21\columnwidth}\raggedright\strut
\(\text{SS}_B\)\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\raggedright\strut
\(\text{MS}_B=\text{SS}_B/\text{df}_B\)\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedright\strut
\(\text{MS}_B/\text{MS}_E\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.17\columnwidth}\raggedright\strut
Error\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\raggedright\strut
\(N-J-K+1\)\strut
\end{minipage} & \begin{minipage}[t]{0.21\columnwidth}\raggedright\strut
\(\text{SS}_E\)\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\raggedright\strut
\(\text{MS}_E=\text{SS}_E/\text{df}_E\)\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedright\strut
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.17\columnwidth}\raggedright\strut
\textcolor{red}{\textbf{Total}}\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\raggedright\strut
\(\color{red}{\mathbf{N-1}}\)\strut
\end{minipage} & \begin{minipage}[t]{0.21\columnwidth}\raggedright\strut
\(\color{red}{\textbf{SS}_{\textbf{Total}}}\)\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\raggedright\strut
\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedright\strut
\strut
>>>>>>> origin/chapter0to1_edits
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

<<<<<<< HEAD
When working with data, we should always start with summarizing the
sample size. We will use \textbf{\emph{n}} for the number of subjects in
the sample and denote the population size (if available) with
\textbf{\emph{N}}. Here, the sample size is \textbf{\emph{n=114}}. In
this situation, we do not have a random sample from a population (these
were volunteers from the population of prisoners at the particular
prison) so we cannot make inferences to a larger group. But we can
assess whether there is a \textbf{\emph{causal effect}}\footnote{As
  noted previously, we reserve the term ``effect'' for situations where
  random assignment allows us to consider causality as the reason for
  the differences in the response variable among levels of the
  explanatory variable, but this is only the case if we find evidence
  against the null hypothesis of no difference in the groups.}: if
sufficient evidence is found to conclude that there is some difference
in the responses across the treated groups, we can attribute those
differences to the treatments applied, since the groups should be same
otherwise due to the pictures being randomly assigned to the ``jurors''.
The story of the data set -- that it was collected on prisoners --
becomes pretty important in thinking about the ramifications of any
results. Are male prisoners different from the population of college
males or all residents of a state such as Montana? If so, then we should
not assume that the detected differences, if detected, would also exist
in some other group of male subjects. The lack of a random sample makes
it impossible to assume that this set of prisoners might be like other
prisoners. So there are definite limitations to the inferences in the
following results. But it is still interesting to see if the pictures
caused a difference in the suggested mean sentences, even though the
inferences are limited to this group of prisoners. If this had been an
observational study (suppose that the prisoners could select one of the
three pictures), then we would have to avoid any of the ``causal''
language that we can consider here because the pictures were not
randomly assigned to the subjects. Without random assignment, the
explanatory variable of picture choice could be
\textbf{\emph{confounded}} with another characteristic of prisoners that
was related to which picture they selected and the rating they provided.
Confounding is not the only reason to avoid causal statements with
non-random assignment but the inability to separate the effect of other
variables (measured or unmeasured) from the differences we are observing
means that our inferences in these situations need to be carefully
stated.

Instead of loading this data set into R using the ``Import Dataset''
functionality, we can load an R package that contains the data, making
for easy access to this data set. The package called \texttt{heplots}
\citep{R-heplots} contains a data set called MockJury that contains the
results of the study. We also rely the R package called \texttt{mosaic}
\citep{R-mosaic} that was introduced previously. First (but only once),
you need to install both packages, which can be done either using the
Packages tab in the lower right panel of R-studio or using the
\texttt{install.packages} function with quotes around the package name:

\begin{Shaded}
\begin{Highlighting}[]
\OperatorTok{>}\StringTok{ }\NormalTok{install. }\KeywordTok{packages}\NormalTok{(}\StringTok{"heplots"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

After making sure that both packages are installed, we use the
\texttt{require} function around the package name (no quotes now!) to
load the package, something that you need to do any time you want to use
features of a package.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{require}\NormalTok{(heplots)}
\KeywordTok{require}\NormalTok{(mosaic)}
\end{Highlighting}
\end{Shaded}

There will be some results of the loading process that may discuss
loading other required packages. If the output says that it needs a
package that is unavailable, then follow the same process noted above to
install that package as well.

To load the data set that is available in an active package, we use the
\texttt{data} function.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{(MockJury)}
\end{Highlighting}
\end{Shaded}

Now there will be a data.frame called \texttt{MockJury} available for us
to analyze and some information about it in the Environment tab. Again,
we can find out more about the data set in a couple of ways. First, we
can use the \texttt{View} function to provide a spreadsheet type of
display in the upper left panel. Second, we can use the \texttt{head}
and \texttt{tail} functions to print out the beginning and end of the
data set. Because there are so many variables, it may wrap around to
show all the columns.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{View}\NormalTok{(MockJury)}
\KeywordTok{head}\NormalTok{(MockJury)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        Attr    Crime Years Serious exciting calm independent sincere warm
## 1 Beautiful Burglary    10       8        6    9           9       8    5
## 2 Beautiful Burglary     3       8        9    5           9       3    5
## 3 Beautiful Burglary     5       5        3    4           6       3    6
## 4 Beautiful Burglary     1       3        3    6           9       8    8
## 5 Beautiful Burglary     7       9        1    1           5       1    8
## 6 Beautiful Burglary     7       9        1    5           7       5    8
##   phyattr sociable kind intelligent strong sophisticated happy ownPA
## 1       9        9    9           6      9             9     5     9
## 2       9        9    4           9      5             5     5     7
## 3       7        4    2           4      5             4     5     5
## 4       9        9    9           9      9             9     9     9
## 5       8        9    4           7      9             9     8     7
## 6       8        9    5           8      9             9     9     9
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tail}\NormalTok{(MockJury)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        Attr   Crime Years Serious exciting calm independent sincere warm
## 109 Average Swindle     3       2        7    6           9       9    6
## 110 Average Swindle     2       1        8    8           8       8    8
## 111 Average Swindle     7       4        1    6           9       1    1
## 112 Average Swindle     6       3        5    3           5       2    4
## 113 Average Swindle    12       9        1    9           9       1    1
## 114 Average Swindle     8       8        1    9           1       5    1
##     phyattr sociable kind intelligent strong sophisticated happy ownPA
## 109       4        7    6           8      6             5     7     2
## 110       8        9    9           9      9             9     9     6
## 111       1        9    4           1      1             1     1     9
## 112       1        4    9           3      3             9     5     3
## 113       1        9    1           9      9             1     9     1
## 114       1        9    1           1      9             5     1     1
\end{verbatim}

When data sets are loaded from packages, there is often extra
documentation available about the data set which can be accessed using
the help function. In this case, it will bring up a screen with
information about the study and each variable that was measured.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{help}\NormalTok{(MockJury)}
\end{Highlighting}
\end{Shaded}

The help function is also useful with functions in R to help you
understand options and, at the bottom of the help, see examples of using
the function.

With many variables in a data set, it is often useful to get some quick
information about all of them; the \texttt{summary} function provides
useful information whether the variables are categorical or quantitative
and notes if any values were missing.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(MockJury)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##            Attr         Crime        Years           Serious     
##  Beautiful   :39   Burglary:59   Min.   : 1.000   Min.   :1.000  
##  Average     :38   Swindle :55   1st Qu.: 2.000   1st Qu.:3.000  
##  Unattractive:37                 Median : 3.000   Median :5.000  
##                                  Mean   : 4.693   Mean   :5.018  
##                                  3rd Qu.: 7.000   3rd Qu.:6.750  
##                                  Max.   :15.000   Max.   :9.000  
##     exciting          calm        independent       sincere     
##  Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  
##  1st Qu.:3.000   1st Qu.:4.250   1st Qu.:5.000   1st Qu.:3.000  
##  Median :5.000   Median :6.500   Median :6.500   Median :5.000  
##  Mean   :4.658   Mean   :5.982   Mean   :6.132   Mean   :4.789  
##  3rd Qu.:6.000   3rd Qu.:8.000   3rd Qu.:8.000   3rd Qu.:7.000  
##  Max.   :9.000   Max.   :9.000   Max.   :9.000   Max.   :9.000  
##       warm         phyattr        sociable          kind      
##  Min.   :1.00   Min.   :1.00   Min.   :1.000   Min.   :1.000  
##  1st Qu.:2.00   1st Qu.:2.00   1st Qu.:5.000   1st Qu.:3.000  
##  Median :5.00   Median :5.00   Median :7.000   Median :5.000  
##  Mean   :4.57   Mean   :4.93   Mean   :6.132   Mean   :4.728  
##  3rd Qu.:7.00   3rd Qu.:8.00   3rd Qu.:8.000   3rd Qu.:7.000  
##  Max.   :9.00   Max.   :9.00   Max.   :9.000   Max.   :9.000  
##   intelligent        strong      sophisticated       happy      
##  Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  
##  1st Qu.:4.000   1st Qu.:4.000   1st Qu.:3.250   1st Qu.:3.000  
##  Median :7.000   Median :6.000   Median :5.000   Median :5.000  
##  Mean   :6.096   Mean   :5.649   Mean   :5.061   Mean   :5.061  
##  3rd Qu.:8.750   3rd Qu.:7.000   3rd Qu.:7.000   3rd Qu.:7.000  
##  Max.   :9.000   Max.   :9.000   Max.   :9.000   Max.   :9.000  
##      ownPA      
##  Min.   :1.000  
##  1st Qu.:5.000  
##  Median :6.000  
##  Mean   :6.377  
##  3rd Qu.:9.000  
##  Max.   :9.000
\end{verbatim}

If we take a few moments to explore the output we can discover some
useful aspects of the data set. The output is organized by variable,
providing summary information based on the type of variable, either
counts by category for categorical variables \texttt{Attr} and
\texttt{Crime} mean for quantitative variables. If present, you would
also get a count of missing values that are called ``NAs'' in R. For the
first variable, called \texttt{Attr} in the data.frame and that we might
we find counts of the number of subjects shown each picture: 37/114
viewed the ``Unattractive'' picture, 38 viewed ``Average'', and 39
viewed ``Beautiful''. We can also see that suggested prison sentences
(data.frame variable \texttt{Years} ) ranged from 1 year to 15 years
with a median of 3 years. It seems that all the other variables except
for \emph{Crime} (type of crime that they were told the pictured woman
committed) contained responses between 1 and 9 based on rating scales
from 1 = low to 9 = high.

To accompany the numerical summaries, histograms, and boxplots can
provide some initial information on the shape of the distribution of the
responses for the Figure \ref{fig:Figure2-1} contains the histogram and
boxplot of Years, ignoring any information on which picture the
``jurors'' were shown. The calls to the two plotting functions are
enhanced slightly to add better labels.



\begin{figure}
\centering
\includegraphics{02-reintroductionToStatistics_files/figure-latex/Figure2-1-1.pdf}
\caption{\label{fig:Figure2-1}Histogram and boxplot of suggested sentences in years.}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\KeywordTok{hist}\NormalTok{(MockJury}\OperatorTok{$}\NormalTok{Years, }\DataTypeTok{xlab=}\StringTok{"Years"}\NormalTok{, }\DataTypeTok{labels=}\NormalTok{T, }\DataTypeTok{main=}\StringTok{"Histogram of Years"}\NormalTok{)}
\KeywordTok{boxplot}\NormalTok{(MockJury}\OperatorTok{$}\NormalTok{Years, }\DataTypeTok{ylab=}\StringTok{"Years"}\NormalTok{, }\DataTypeTok{main=}\StringTok{"Boxplot of Years"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The distribution appears to have a strong right skew with three
observations at 15 years flagged as potential outliers. You can only
tell that there are three observations and that they are at 15 by
looking at both plots -- the bar around 15 years in the histogram has a
count of three and the boxplot only shows a single point at 15 which is
actually three tied points at exactly 15 years plotted on top of each
other (we call this ``overplotting''). These three observations really
seem to be the upper edge of the overall pattern of a strongly right
skewed distribution, so even though they are flagged in the boxplot, we
likely would not want to remove them from our data set. In real data
sets, outliers are commonly encountered and the first step is to verify
that they were not errors in recording. The next step is to study their
impact on the statistical analyses performed, potentially considering
reporting results with and without the influential observation(s) in the
results. If the analysis is unaffected by the ``unusual'' observations,
then it matters little whether they are dropped or not. If they do
affect the results, then reporting both versions of results allows the
reader to judge the impacts for themselves. It is important to remember
that sometimes the outliers are the most interesting part of the data
set.

Often when statisticians think of distributions, we think of the smooth
underlying shape that led to the data set that is being displayed in the
histogram. Instead of binning up observations and making bars in the
histogram, we can estimate what is called a \textbf{\emph{density curve
}} as a smooth curve that represents the observed distribution of the
responses. Density curves can sometimes help us see features of the data
sets more clearly.

To understand the density curve, it is useful to initially see the
histogram and density curve together. The density curve is scaled so
that the total area\footnote{If you've taken calculus, you will know
  that the curve is being constructed so that the integral from
  \(-\infty\) to \(\infty\) is 1. If you don't know calculus, think of a
  rectangle with area of 1 based on its height and width. These cover
  the same area but the top of the region wiggles.} under the curve is
1. To make a comparable histogram, the y-axis needs to be scaled so that
the histogram is also on the ``density'' scale which makes the bar
heights required so that the proportion of the total data set in each
bar is represented by the area in each bar (remember that area is height
times width). So the height depends on the width of the bars and the
total area across all the bars has to be 1. In the \texttt{hist}
function, the \texttt{freq=F} to get density-scaled histogram bars. The
density curve is added to the histogram using the R code of
\texttt{lines(density())}, producing the result in Figure
\ref{fig:Figure2-2} with added modifications of options for \texttt{lwd}
(line width) and \texttt{col} (color) to make the plot more interesting.
You can see how the density curve somewhat matches the histogram bars
but deals with the bumps up and down and edges a little differently. We
can pick out the strong right skew using either display and will rarely
make both together.



\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{hist}\NormalTok{(MockJury}\OperatorTok{$}\NormalTok{Years,}\DataTypeTok{freq=}\NormalTok{F,}\DataTypeTok{xlab=}\StringTok{"Years"}\NormalTok{,}\DataTypeTok{main=}\StringTok{"Histogram of Years"}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(}\KeywordTok{density}\NormalTok{(MockJury}\OperatorTok{$}\NormalTok{Years),}\DataTypeTok{lwd=}\DecValTok{3}\NormalTok{,}\DataTypeTok{col=}\StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{02-reintroductionToStatistics_files/figure-latex/Figure2-2-1.pdf}
\caption{\label{fig:Figure2-2}Histogram and density curve of Years data.}
\end{figure}

Histograms can be sensitive to the choice of the number of bars and even
the cut-offs used to define the bins for a given number of bars. Small
changes in the definition of cut-offs for the bins can have noticeable
impacts on the shapes observed but this does not impact density curves.
We are not going to tinker with the default choices for bars in
histogram as they are reasonably selected, but we can add information on
the original observations being included in each bar to better
understand the choices that \texttt{hist} is making. In the previous
display, we can add what is called a \textbf{\emph{rug}} to the plot,
were a tick mark is made on the x-axis for each observation. Because the
responses were provided as whole years (1, 2, 3, \ldots{}, 15), we need
to use a graphical technique called \textbf{\emph{jittering}} to add a
little noise\footnote{Jittering typically involves adding random
  variability to each observation that is uniformly distributed in a
  range determined based on the spacing of the function, the results
  will change. For more details, type \texttt{help(jitter)} in R.} to
each observation so all the observations at each year value do not plot
as a single line. In Figure \ref{fig:Figure2-3}, the added tick marks on
the x-axis show the approximate locations of the original observations.
We can see how there are 3 observations at 15 (all were 15 and the noise
added makes it possible to see them all). The limitations of the
histogram arise around the 10 year sentence area where there are many
responses at 10 years and just one at both 9 and 11 years, but the
histogram bars sort of miss this that aspect of the data set. The
density curve did show a small bump at 10 years. Density curves are,
however, not perfect and this one shows area for sentences less than 0
years which is not possible here.




\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{hist}\NormalTok{(MockJury}\OperatorTok{$}\NormalTok{Years, }\DataTypeTok{freq=}\NormalTok{F, }\DataTypeTok{xlab=}\StringTok{"Years"}\NormalTok{,}
     \DataTypeTok{main=}\StringTok{"Histogram of Years with density curve and rug"}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(}\KeywordTok{density}\NormalTok{(MockJury}\OperatorTok{$}\NormalTok{Years),}\DataTypeTok{lwd=}\DecValTok{3}\NormalTok{,}\DataTypeTok{col=}\StringTok{"red"}\NormalTok{)}
\KeywordTok{rug}\NormalTok{(}\KeywordTok{jitter}\NormalTok{(MockJury}\OperatorTok{$}\NormalTok{Years),}\DataTypeTok{col=}\StringTok{"blue"}\NormalTok{,}\DataTypeTok{lwd=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{02-reintroductionToStatistics_files/figure-latex/Figure2-3-1.pdf}
\caption{\label{fig:Figure2-3}Histogram with density curve and rug plot of the jittered
responses.}
\end{figure}

The graphical tools we've just discussed are going to help us move to
comparing the distribution of responses across more than one group. We
will have two displays that will help us make these comparisons. The
simplest is \emph{the \textbf{side-by-side boxplot}}, where a boxplot is
displayed for each group of interest using the same y-axis scaling. In
R, we can use its \textbf{\emph{formula}} notation to see if the
response (\texttt{Years}) differs based on the group (\texttt{Attr}) by
using something like \texttt{Y\textasciitilde{}X} or, here,
\texttt{Years\textasciitilde{}Attr}. We also need to tell R where to
find the variables -- use the last option in the command,
\texttt{data=DATASETNAME} , to inform R of the data.frame to look in to
find the variables. In this example, \texttt{data=MockJury}. We will use
the formula and \texttt{data=...} options in almost every function we
use from here forward. Figure \ref{fig:Figure2-4} contains the
side-by-side boxplots showing right skew for all the groups, slightly
higher median and more variability for the \emph{Unattractive} group
along with some potential outliers indicated in two of the three groups.



\begin{figure}
\centering
\includegraphics{02-reintroductionToStatistics_files/figure-latex/Figure2-4-1.pdf}
\caption{\label{fig:Figure2-4}Side-by-side boxplot of Years based on picture groups.}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{boxplot}\NormalTok{(Years}\OperatorTok{~}\NormalTok{Attr,}\DataTypeTok{data=}\NormalTok{MockJury)}
\end{Highlighting}
\end{Shaded}

The ``\textasciitilde{}'' (which is read as the \emph{tilde} symbol,
which you can find in the upper left corner of your keyboard) notation
will be used in two ways this semester. The formula use in R employed
previously declares that the response variable here is \emph{Years} and
the explanatory variable is \emph{Attr}. The other use for
``\textasciitilde{}'' is as shorthand for ``is distributed as'' and is
used in the context of \texttt{Y\textasciitilde{}N(0,1)}, which
translates (in statistics) to defining the random variable \emph{Y} as
following a Normal distribution\footnote{Remember the bell-shaped curve
  you encountered in introductory statistics? If not, you can see some
  at \url{https://en.wikipedia.org/wiki/Normal_distribution}} with mean
0 and standard deviation of 1. In the current situation, we could ask
whether the \texttt{Years} variable seems like it may follow a normal
distribution, in other words, is
\emph{Years}\texttt{\textasciitilde{}N(0,1)}? Since the responses are
right skewed with some groups having outliers, it is not reasonable to
assume that the \emph{Years} variable for any of the three groups may
follow a Normal distribution (more later on the issues this creates!).
Remember that \(\mu\) and \(\sigma\) are parameters where \(\mu\)
(``mu'') is our standard symbol for the \textbf{\emph{population mean}}
and that \(\sigma\) (``sigma'') is the symbol of the
\textbf{\emph{population standard deviation}}.

\section{Beanplots}\label{section2-2}

The other graphical display for comparing multiple groups we will use is
a newer display called a \textbf{\emph{beanplot}} \citep{Kampstra2008}.
Figure \ref{fig:Figure2-5} shows an example of a beanplot that provides
a side-by-side display that contains the density curves, the original
observations that generated the density curve in a (jittered) rug-plot,
the mean of each group, and the overall mean of the entire data set. For
each group, the density curves are mirrored to aid in visual assessment
of the shape of the distribution, which makes a ``bean'' in some cases.
This mirroring also creates a shape that resembles a violin with skewed
distributions so this display has also been called a ``violin plot''.
The innovation in the beanplot is to add bold horizontal lines at the
mean for each group. It also adds a lighter dashed line for the overall
mean. All together this plot shows us information on the center (mean),
spread, and shape of the distributions of the responses. Our inferences
typically focus on the means of the groups and this plot allows us to
compare those across the groups while gaining information on the shapes
of the distributions of responses in each group.

To use the \texttt{beanplot} function we need to install and load the
\texttt{beanplot} package \citep{R-beanplot}. The function works like
the boxplot used previously except that options for \texttt{log},
\texttt{col}, and \texttt{method} need to be specified. Use
these\footnote{Well, you can use other colors (try ``lightblue'' for
  example), but I think bisque looks nice in these plots.} options for
any beanplots you make: \texttt{log="",\ col="bisque",\ method="jitter"}




\begin{figure}
\centering
\includegraphics{02-reintroductionToStatistics_files/figure-latex/Figure2-5-1.pdf}
\caption{\label{fig:Figure2-5}Beanplot of Years by picture group. Long, bold lines
correspond to mean of each group.}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{require}\NormalTok{(beanplot)}
\KeywordTok{beanplot}\NormalTok{(Years}\OperatorTok{~}\NormalTok{Attr,}\DataTypeTok{data=}\NormalTok{MockJury,}\DataTypeTok{log=}\StringTok{""}\NormalTok{,}\DataTypeTok{col=}\StringTok{"bisque"}\NormalTok{,}\DataTypeTok{method=}\StringTok{"jitter"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Figure \ref{fig:Figure2-5} reinforces the strong right skews that were
also detected in the boxplots previously. The three large sentences of
15 years can now be clearly identified, with one in the \emph{Beautiful}
group and two in the \emph{Unattractive} group. The \emph{Unattractive}
group seems to have more high observations than the other groups even
though the \emph{Beautiful} group had the largest number of observations
around 10years. The mean sentence was highest for the
\emph{Unattractive} group and the difference in the means between
\emph{Beautiful} and \emph{Average} was small.

In this example, it appears that the mean for \emph{Unattractive} is
larger than the other two groups. But is this difference real? We will
never know the answer to that question, but we can assess how likely we
are to have seen a result as extreme or more extreme than our result,
assuming that there is no difference in the means of the groups. And if
the observed result is (extremely) unlikely to occur, then we can reject
the hypothesis that the groups have the same mean and conclude that
there is evidence of a real difference. To start exploring whether there
are differences in the means, we need to have numerical values to
compare. We can get means and standard deviations by groups easily using
the same formula notation with the \texttt{mean} and \texttt{sd}
functions if the \texttt{mosaic} package is loaded.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(Years }\OperatorTok{~}\StringTok{ }\NormalTok{Attr, }\DataTypeTok{data =}\NormalTok{ MockJury)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Beautiful      Average Unattractive 
##     4.333333     3.973684     5.810811
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sd}\NormalTok{(Years }\OperatorTok{~}\StringTok{ }\NormalTok{Attr, }\DataTypeTok{data =}\NormalTok{ MockJury)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Beautiful      Average Unattractive 
##     3.405362     2.823519     4.364235
\end{verbatim}

We can also use the \texttt{favstats} function to get those summaries
and others.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{favstats}\NormalTok{(Years }\OperatorTok{~}\StringTok{ }\NormalTok{Attr, }\DataTypeTok{data =}\NormalTok{ MockJury)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           Attr min Q1 median   Q3 max     mean       sd  n missing
## 1    Beautiful   1  2      3  6.5  15 4.333333 3.405362 39       0
## 2      Average   1  2      3  5.0  12 3.973684 2.823519 38       0
## 3 Unattractive   1  2      5 10.0  15 5.810811 4.364235 37       0
\end{verbatim}

Based on these results, we can see that there is an estimated difference
of almost 2 years in the mean sentence between \emph{Average} and
\emph{Unattractive} groups. Because there are three groups being
compared in this study, we will have to wait until Chapter 3 and the
One-Way ANOVA test to fully assess evidence related to some difference
among the three groups. For now, we are going to focus on comparing the
mean \emph{Years} between \emph{Average} and \emph{Unattractive} groups
-- which is a \textbf{\emph{2 independent sample mean}} situation and
something you should have seen before. Remember that the ``independent''
sample part of this refers to observations that are independently
observed for the two groups as opposed to the paired sample situation
that you may have explored where one observation from the first group is
related to an observation in the second group (repeated measures on the
same person or the famous ``twin'' studies with one twin assigned to
each group).

Here we are going to use the ``simple'' two independent group scenario
to review some basic statistical concepts and connect two different
frameworks for conducting statistical inference: randomization and
parametric inference techniques. \textbf{\emph{Parametric}} statistical
methods involve making assumptions about the distribution of the
responses and obtaining confidence intervals and/or p-values using a
\emph{named} distribution (like the z or \(t\)-distributions). Typically
these results are generated using formulas and looking up areas under
curves or cutoffs using a table or a computer.
\textbf{\emph{Randomization}}-based statistical methods use a computer
to shuffle, sample, or simulate observations in ways that allow you to
obtain distributions of possible results to find areas and cutoffs
without resorting to using tables and named distributions. Randomization
methods are what are called \textbf{\emph{nonparametric}} methods that
often make fewer assumptions (they are \textbf{\emph{not free of
assumptions}}!) and so can handle a larger set of problems more easily
than parametric methods. When the assumptions involved in the parametric
procedures are met by a data set, the randomization methods often
provide very similar results to those provided by the parametric
techniques. To be a more sophisticated statistical consumer, it is
useful to have some knowledge of both of these approaches to statistical
inference and the fact that they can provide similar results might
deepen your understanding of both approaches.

We will start with comparing the \emph{Average} and \emph{Unattractive}
groups to compare these two ways of doing inference. We could remove the
\emph{Beautiful} group observations in a spreadsheet program and read
that new data set back into R, but it is actually pretty easy to use R
to do data management once the data set is loaded. To remove the
observations that came from the \emph{Beautiful} group, we are going to
generate a new variable that we will call \texttt{NotBeautiful} that is
true when observations came from another group (\emph{Average} or
\emph{Unattractive}) and false for observations from the
\emph{Beautiful} group. To do this, we will apply the \textbf{\emph{not
equal}} logical function (\texttt{!=} ) to the variable \texttt{Attr},
inquiring whether it was different from the \texttt{"Beautiful"} level.
You can see the content of the new variable in the output:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MockJury}\OperatorTok{$}\NormalTok{NotBeautiful <-}\StringTok{ }\NormalTok{MockJury}\OperatorTok{$}\NormalTok{Attr }\OperatorTok{!=}\StringTok{ "Beautiful"}
\NormalTok{MockJury}\OperatorTok{$}\NormalTok{NotBeautiful}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
##  [12] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE
##  [23]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
##  [34]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
##  [45]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
##  [56]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
##  [67]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE
##  [78] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
##  [89] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE
## [100]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
## [111]  TRUE  TRUE  TRUE  TRUE
\end{verbatim}

This new variable is only FALSE for the \emph{Beautiful} responses as we
can see if we compare some of the results from the original and new
variable:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(}\KeywordTok{data.frame}\NormalTok{(MockJury}\OperatorTok{$}\NormalTok{Attr, MockJury}\OperatorTok{$}\NormalTok{NotBeautiful))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   MockJury.Attr MockJury.NotBeautiful
## 1     Beautiful                 FALSE
## 2     Beautiful                 FALSE
## 3     Beautiful                 FALSE
## 4     Beautiful                 FALSE
## 5     Beautiful                 FALSE
## 6     Beautiful                 FALSE
\end{verbatim}

\newpage

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tail}\NormalTok{(}\KeywordTok{data.frame}\NormalTok{(MockJury}\OperatorTok{$}\NormalTok{Attr, MockJury}\OperatorTok{$}\NormalTok{NotBeautiful))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     MockJury.Attr MockJury.NotBeautiful
## 109       Average                  TRUE
## 110       Average                  TRUE
## 111       Average                  TRUE
## 112       Average                  TRUE
## 113       Average                  TRUE
## 114       Average                  TRUE
\end{verbatim}

To get rid of one of the groups, we need to learn a little bit about
data management in R. \textbf{\emph{Brackets}} \texttt{({[},\ {]})} are
used to modify the rows or columns in a data.frame with entries before
the comma operating on rows and entries after the comma on the columns.
For example, if you want to see the results for the 5\(^{th}\) subject,
you can reference the 5\(^{th}\) row of the data.frame using
\texttt{{[}5,\ {]}} after the data.frame name:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MockJury[}\DecValTok{5}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        Attr    Crime Years Serious exciting calm independent sincere warm
## 5 Beautiful Burglary     7       9        1    1           5       1    8
##   phyattr sociable kind intelligent strong sophisticated happy ownPA
## 5       8        9    4           7      9             9     8     7
##   NotBeautiful
## 5        FALSE
\end{verbatim}

We could just extract the \emph{Years} response for the 5\(^{th}\)
subject by incorporating information on the row and column of interest
(\texttt{Years} is the 3\(^{rd}\) column):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MockJury[}\DecValTok{5}\NormalTok{,}\DecValTok{3}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 7
\end{verbatim}

In R, we can use logical vectors to keep any rows of the data.frame
where the variable is true and drop any rows where it is false by
placing the logical variable in the first element of the brackets. The
reduced version of the data set should be saved with a different name
such as \texttt{MockJury2} that is used here to reduce the chances of
confusing it with the previous full data set:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MockJury2 <-}\StringTok{ }\NormalTok{MockJury[MockJury}\OperatorTok{$}\NormalTok{NotBeautiful,]}
\end{Highlighting}
\end{Shaded}

You will always want to check that the correct observations were dropped
either using \texttt{View(MockJury2)} or by doing a quick summary of the
\texttt{Attr} variable in the new data.frame.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(MockJury2}\OperatorTok{$}\NormalTok{Attr)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Beautiful      Average Unattractive 
##            0           38           37
\end{verbatim}

It ends up that R remembers the \emph{Beautiful} category even though
there are 0 observations in it now and that can cause us some problems.
When we remove a group of observations, we sometimes need to clean up
categorical variables to just reflect the categories that are present.
The \texttt{factor} function creates categorical variables based on the
levels of the variables that are observed and is useful to run here to
clean up \texttt{Attr}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MockJury2}\OperatorTok{$}\NormalTok{Attr <-}\StringTok{ }\KeywordTok{factor}\NormalTok{(MockJury2}\OperatorTok{$}\NormalTok{Attr) }
\KeywordTok{summary}\NormalTok{(MockJury2}\OperatorTok{$}\NormalTok{Attr)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      Average Unattractive 
##           38           37
\end{verbatim}

Now if we remake the boxplots and beanplots, they only contain results
for the two groups of interest here as seen in Figure
\ref{fig:Figure2-6}.




\begin{figure}
\centering
\includegraphics{02-reintroductionToStatistics_files/figure-latex/Figure2-6-1.pdf}
\caption{\label{fig:Figure2-6}Boxplot and beanplot of the Years responses on the reduced
data set.}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\KeywordTok{boxplot}\NormalTok{(Years }\OperatorTok{~}\StringTok{ }\NormalTok{Attr,}\DataTypeTok{data=}\NormalTok{MockJury2) }
\KeywordTok{beanplot}\NormalTok{(Years }\OperatorTok{~}\StringTok{ }\NormalTok{Attr,}\DataTypeTok{data=}\NormalTok{MockJury2,}\DataTypeTok{log=}\StringTok{""}\NormalTok{,}\DataTypeTok{col=}\StringTok{"bisque"}\NormalTok{,}\DataTypeTok{method=}\StringTok{"jitter"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The two-sample mean techniques you learned in your previous course all
start with comparing the means the two groups. We can obtain the two
means using the \texttt{mean} function or directly obtain the difference
in the means using the \texttt{diffmean} function (both require the
\texttt{mosaic} package). The \texttt{diffmean} function provides
\(\bar{x}_{Unattractive} - \bar{x}_{Average}\) where \(\bar{x}\) (read
as ``x-bar'') is the sample mean of observations in the subscripted
group. Note that there are two directions that you could compare the
means and this function chooses to take the mean from the second group
name \emph{alphabetically} and subtract the mean from the first
alphabetical group name. It is always good to check the direction of
this calculation as having a difference of \(-1.84\) years versus
\(1.84\) years could be important.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(Years }\OperatorTok{~}\StringTok{ }\NormalTok{Attr, }\DataTypeTok{data=}\NormalTok{MockJury2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      Average Unattractive 
##     3.973684     5.810811
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{diffmean}\NormalTok{(Years }\OperatorTok{~}\StringTok{ }\NormalTok{Attr, }\DataTypeTok{data=}\NormalTok{MockJury2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## diffmean 
## 1.837127
\end{verbatim}

\section{Models, hypotheses, and permutations for the 2 sample mean
situation}\label{section2-3}

There appears to be some evidence that the \emph{Unattractive} group is
getting higher average lengths of sentences from the prisoner ``jurors''
than the \emph{Average} group, but we want to make sure that the
difference is real -- that there is evidence to reject the assumption
that the means are the same ``in the population''. First, a
\textbf{\emph{null hypothesis}}\footnote{The hypothesis of no difference
  that is typically generated in the hopes of being rejected in favor of
  the alternative hypothesis which contains the sort of difference that
  is of interest in the application.} which defines a \textbf{\emph{null
model}}\footnote{The null model is the statistical model that is implied
  by the chosen null hypothesis. Here, a null hypothesis of no
  difference translates to having a model with the same mean for both
  groups.} needs to be determined in terms of \textbf{\emph{parameters}}
(the true values in the population). The research question should help
you determine the form of the hypotheses for the assumed population. In
the 2 independent sample mean problem, the interest is in testing a null
hypothesis of \(H_0: \mu_1 = \mu_2\) versus the alternative hypothesis
of \(H_A: \mu_1 \ne \mu_2\), where \(\mu_1\) is the parameter for the
true mean of the first group and \(\mu_2\) is the parameter for the true
mean of the second group. The alternative hypothesis involves assuming a
statistical model for the \(i^{th} (i=1,\ldots,n_j)\) response from the
\(j^{th} (j=1,2)\) group, \(\boldsymbol{y}_{ij}\), that involves
modeling it as \(y_{ij} = \mu_j + \varepsilon_{ij}\), where we assume
that \(\varepsilon_{ij} \sim N(0,\sigma^2)\). For the moment, focus on
the models that either assume the means are the same (null) or different
(alternative), which imply:

\begin{itemize}
\item
  Null Model: \(y_{ij} = \mu + \varepsilon_{ij}\) There is \textbf{no}
  difference in \textbf{true} means for the two groups.
\item
  Alternative Model: \(y_{ij} = \mu_j + \varepsilon_{ij}\) There is
  \textbf{a} difference in \textbf{true} means for the two groups.
\end{itemize}

Suppose we are considering the alternative model for the 4th observation
(\(i=4\)) from the second group (\(j=2\)), then the model for this
observation is \(y_{42} = \mu_2 +\varepsilon_{42}\), that defines the
response as coming from the true mean for the second group plus a random
error term for that observation, \(\varepsilon_{42}\). For, say, the 5th
observation from the first group (\(j=1\)), the model is
\(y_{51} = \mu_1 +\varepsilon_{51}\). If we were working with the null
model, the mean is always the same (\(\mu\)) - the group specified does
not change the mean we use for that observation.

It can be helpful to think about the null and alternative models
graphically. By assuming the null hypothesis is true (means are equal)
and that the random errors around the mean follow a normal distribution,
we assume that the truth is as displayed in the left panel of Figure
\ref{fig:Figure2-7} -- two normal distributions with the same mean and
variability. The alternative model allows the two groups to potentially
have different means, such as those displayed in the right panel of
Figure \ref{fig:Figure2-7} where the second group has a larger mean.
Note that in this scenario, we assume that the observations all came
from the same distribution except that they had different means.
Depending on the statistical procedure we are using, we basically are
going to assume that the observations (\(y_{ij}\)) either were generated
as samples from the null or alternative model. You can imagine drawing
observations at random from the pictured distributions. For hypothesis
testing, the null model is assumed to be true and then the unusualness
of the actual result is assessed relative to that assumption. In
hypothesis testing, we have to decide if we have enough evidence to
reject the assumption that the null model (or hypothesis) is true. If we
reject the null hypothesis, then we would conclude that the other model
considered (the alternative model) is more reasonable. The researchers
obviously would have hoped to encounter some sort of noticeable
difference in the sentences provided for the different pictures and been
able to find enough evidence to reject the null model where the groups
``look the same''.





\begin{figure}
\centering
\includegraphics{chapter1_files/image015.png}
\caption{\label{fig:Figure2-7}Illustration of the assumed situations under the null
(left) and a single possibility that could occur if the alternative were
true (right) and the true means were different.}
\end{figure}

In statistical inference, null hypotheses (and their implied models) are
set up as ``straw men'' with every interest in rejecting them even
though we assume they are true to be able to assess the evidence against
them. Consider the original study design here, the pictures were
randomly assigned to the subjects. If the null hypothesis were true,
then we would have no difference in the population means of the groups.
And this would apply if we had done a different random assignment of the
pictures to the subjects. So let's try this: assume that the null
hypothesis is true and randomly re-assign the treatments (pictures) to
the observations that were obtained. In other words, keep the sentences
(\emph{Years}) the same and shuffle the group labels randomly. The
technical term for this is doing a \textbf{\emph{permutation}} (a random
shuffling of the treatments relative to the responses). If the null is
true and the means in the two groups are the same, then we should be
able to re-shuffle the groups to the observed sentences (\emph{Years})
and get results similar to those we actually observed. If the null is
false and the means are really different in the two groups, then what we
observed should differ from what we get under other random permutations.
The differences between the two groups should be more noticeable in the
observed data set than in (most) of the shuffled data sets. It helps to
see an example of a permutation of the labels to understand what this
means here.

In the \texttt{mosaic} package, the \texttt{shuffle} function allows us
to easily perform a permutation\footnote{We'll see the \texttt{shuffle}
  function in a more common usage below; while the code to generate
  \texttt{Perm1} is provided, it isn't something to worry about right
  now.}. Just one time, we can explore what a permutation of the
treatment labels could look like in the \texttt{PermutedAttr} variable
below. Note that the \texttt{Years} are held in the same place the group
labels are shuffled.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Perm1 <-}\StringTok{ }\KeywordTok{with}\NormalTok{(MockJury2,}\KeywordTok{data.frame}\NormalTok{(Years,Attr,}\DataTypeTok{PermutedAttr=}\KeywordTok{shuffle}\NormalTok{(Attr)))}
\NormalTok{Perm1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Years         Attr PermutedAttr
## 1      1 Unattractive Unattractive
## 2      4 Unattractive      Average
## 3      3 Unattractive      Average
## 4      2 Unattractive      Average
## 5      8 Unattractive      Average
## 6      8 Unattractive      Average
## 7      1 Unattractive Unattractive
## 8      1 Unattractive Unattractive
## 9      5 Unattractive      Average
## 10     7 Unattractive Unattractive
## 11     1 Unattractive      Average
## 12     5 Unattractive Unattractive
## 13     2 Unattractive Unattractive
## 14    12 Unattractive      Average
## 15    10 Unattractive      Average
## 16     1 Unattractive      Average
## 17     6 Unattractive Unattractive
## 18     2 Unattractive      Average
## 19     5 Unattractive Unattractive
## 20    12 Unattractive Unattractive
## 21     6 Unattractive      Average
## 22     3 Unattractive      Average
## 23     8 Unattractive      Average
## 24     4 Unattractive Unattractive
## 25    10 Unattractive Unattractive
## 26    10 Unattractive      Average
## 27    15 Unattractive Unattractive
## 28    15 Unattractive      Average
## 29     3 Unattractive      Average
## 30     3 Unattractive      Average
## 31     3 Unattractive Unattractive
## 32    11 Unattractive      Average
## 33    12 Unattractive      Average
## 34     2 Unattractive Unattractive
## 35     1 Unattractive Unattractive
## 36     1 Unattractive Unattractive
## 37    12 Unattractive      Average
## 38     5      Average Unattractive
## 39     5      Average Unattractive
## 40     4      Average Unattractive
## 41     3      Average Unattractive
## 42     6      Average      Average
## 43     4      Average      Average
## 44     9      Average      Average
## 45     8      Average Unattractive
## 46     3      Average      Average
## 47     2      Average Unattractive
## 48    10      Average      Average
## 49     1      Average Unattractive
## 50     1      Average Unattractive
## 51     3      Average Unattractive
## 52     1      Average      Average
## 53     3      Average      Average
## 54     5      Average      Average
## 55     8      Average Unattractive
## 56     3      Average      Average
## 57     1      Average      Average
## 58     1      Average Unattractive
## 59     1      Average      Average
## 60     2      Average      Average
## 61     2      Average Unattractive
## 62     1      Average      Average
## 63     1      Average Unattractive
## 64     2      Average Unattractive
## 65     3      Average Unattractive
## 66     4      Average Unattractive
## 67     5      Average      Average
## 68     3      Average Unattractive
## 69     3      Average      Average
## 70     3      Average      Average
## 71     2      Average Unattractive
## 72     7      Average Unattractive
## 73     6      Average Unattractive
## 74    12      Average Unattractive
## 75     8      Average      Average
\end{verbatim}

If you count up the number of subjects in each group by counting the
number of times each label (Average, Unattractive) occurs, it is the
same in both the \texttt{Attr} and \texttt{PermutedAttr} columns.
Permutations involve randomly re-ordering the values of a variable --
here the \texttt{Attr} group labels -- without changing the content of
the variable. This result can also be generated using what is called
\textbf{\emph{sampling without replacement}}: sequentially select \(n\)
labels from the original variable, removing each used label and making
sure that each original \texttt{Attr} label is selected once and only
once. The new, randomly selected order of selected labels provides the
permuted labels. Stepping through the process helps to understand how it
works: after the initial random sample of one label, there would
\(n - 1\) choices possible; on the \(n^{th}\) selection, there would
only be one label remaining to select. This makes sure that all original
labels are re-used but that the order is random. Sampling without
replacement is like picking names out of a hat, one-at-a-time, and not
putting the names back in after they are selected. It is an exhaustive
process for all the original observations. \textbf{\emph{Sampling with
replacement}} , in contrast, involves sampling from the specified list
with each observation having an equal chance of selection for each
sampled observation -- in other words, observations can be selected more
than once. This is like picking \(n\) names out of a hat that contains
\(n\) names, except that every time a name is selected, it goes back
into the hat -- we'll use this technique in Section \ref{section2-8} to
do what is called \textbf{\emph{bootstrapping}}. Both sampling
mechanisms can be used to generate inferences but each has particular
situations where they are most useful. For hypothesis testing, we will
use permutations (sampling without replacement).

The comparison of the beanplots for the real data set and permuted
version of the labels is what is really interesting (Figure
\ref{fig:Figure2-8}). The original difference in the sample means of the
two groups was 1.84 years (Unattractive minus Average). The sample means
are the \textbf{\emph{statistics}} that estimate the parameters for the
true means of the two groups. In the permuted data set, the difference
in the means is 1.15 years in the opposite direction (Average had a
higher mean than Unattractive in the permuted data).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(Years }\OperatorTok{~}\StringTok{ }\NormalTok{PermutedAttr, }\DataTypeTok{data=}\NormalTok{Perm1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      Average Unattractive 
##     5.447368     4.297297
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{diffmean}\NormalTok{(Years }\OperatorTok{~}\StringTok{ }\NormalTok{PermutedAttr, }\DataTypeTok{data=}\NormalTok{Perm1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  diffmean 
## -1.150071
\end{verbatim}




\begin{figure}
\centering
\includegraphics{02-reintroductionToStatistics_files/figure-latex/Figure2-8-1.pdf}
\caption{\label{fig:Figure2-8}Boxplots of Years responses versus actual treatment groups
and permuted groups.}
\end{figure}

These results suggest that the observed difference was larger than what
we got when we did a single permutation although it was only a little
bit larger than a difference we could observe in permutations if we
ignore the difference in directions. Conceptually, permuting
observations between group labels is consistent with the null hypothesis
-- this is a technique to generate results that we might have gotten if
the null hypothesis were true since the responses are the same in the
two groups if the null is true. We just need to repeat the permutation
process many times and track how unusual our observed result is relative
to this distribution of potential responses if the null were true. If
the observed differences are unusual relative to the results under
permutations, then there is evidence against the null hypothesis, the
null hypothesis should be rejected ( Reject \(H_0\)), and a conclusion
should be made, in the direction of the alternative hypothesis, that
there is evidence that the true means differ. If the observed
differences are similar to (or at least not unusual relative to) what we
get under random shuffling under the null model, we would have a tough
time concluding that there is any real difference between the groups
based on our observed data set.

\section{Permutation testing for the 2 sample mean
situation}\label{section2-4}

In any testing situation, you must define some function of the
observations that gives us a single number that addresses our question
of interest. This quantity is called a \textbf{\emph{test statistic}}.
These often take on complicated forms and have names like \(t\) or \(z\)
statistics that relate to their parametric (named) distributions so we
know where to look up \textbf{\emph{p-values}}\footnote{P-values are the
  probability of obtaining a result as extreme as or more extreme than
  we observed given that the null hypothesis is true.}. In randomization
settings, they can have simpler forms because we use the data set to
find the distribution of the statistic and don't need to rely on a named
distribution. We will label our test statistic \textbf{\emph{T}} (for
\textbf{T} statistic) unless the test statistic has a commonly used
name. Since we are interested in comparing the means of the two groups,
we can define

\[T=\bar{x}_{Unattractive}-\bar{x}_{Average},\]

which coincidentally is what the\texttt{diffmean} function provided us
previously. We label our \textbf{\emph{observed test statistic}} (the
one from the original data set) as

\[T_{obs}=\bar{x}_{Unattractive}-\bar{x}_{Average},\]

which happened to be 1.84 years here. We will compare this result to the
results for the test statistic that we obtain from permuting the group
labels. To denote permuted results, we will add a * to the labels:

\[T^*=\bar{x}_{Unattractive}-\bar{x}_{Average^*}.\]

We then compare the
\(T_{obs}=\bar{x}_{Unattractive}-\bar{x}_{Average} = 1.84\) to the
distribution of results that are possible for the permuted results
(\(T^*\)) which corresponds to assuming the null hypothesis is true.

We need to consider lots of permutations to do a permutation test. In
contrast to your introductory statistics course where, if you did this,
it was just a click away, we are going to learn what was going on under
the hood. Specifically, we need a \textbf{\emph{for loop}} in R to be
able to repeatedly generate the permuted data sets and record \(T^*\)
for each one. Loops are a basic programming task that make randomization
methods possible as well as potentially simplifying any repetitive
computing task. To write a ``for loop'', we need to choose how many
times we want to do the loop (call that \texttt{B}) and decide on a
counter to keep track of where we are at in the loops (call that
\texttt{b}, which goes from 1 up to \texttt{B}). The simplest loop just
involves printing out the index, \texttt{print(b)} at each step. This is
our first use of curly braces, \{ and\}, that are used to group the code
we want to repeatedly run as we proceed through the loop. By typing the
following code in the script window and then highlighting it all and
hitting the run button, R will go through the loop 5 times, printing out
the counter:

\begin{verbatim}
B <- 5
for (b in (1:B)){
  print(b)
}
\end{verbatim}

Note that when you highlight and run the code, it will look about the
same with ``+'' printed after the first line to indicate that all the
code is connected when it appears in the console, looking like this:

\begin{Shaded}
\begin{Highlighting}[]
\OperatorTok{>}\StringTok{ }\ControlFlowTok{for}\NormalTok{(b }\ControlFlowTok{in}\NormalTok{ (}\DecValTok{1}\OperatorTok{:}\NormalTok{B))\{}
\OperatorTok{+}\StringTok{   }\KeywordTok{print}\NormalTok{(b)}
\OperatorTok{+}\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

When you run these three lines of code, the console will show you the
following output:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{[}\DecValTok{1}\NormalTok{] }\DecValTok{1}
\NormalTok{[}\DecValTok{1}\NormalTok{] }\DecValTok{2}
\NormalTok{[}\DecValTok{1}\NormalTok{] }\DecValTok{3}
\NormalTok{[}\DecValTok{1}\NormalTok{] }\DecValTok{4}
\NormalTok{[}\DecValTok{1}\NormalTok{] }\DecValTok{5}
\end{Highlighting}
\end{Shaded}

Instead of printing the counter, we want to use the loop to repeatedly
compute our test statistic across B random permutations of the
observations. The \texttt{shuffle} function performs permutations of the
group labels relative to responses and the \texttt{diffmean} difference
in the two group means in the permuted data set. For a single
permutation, the combination of shuffling \texttt{Attr} and finding the
difference in the means, storing it in a variable called \texttt{Ts} is:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Ts <-}\StringTok{ }\KeywordTok{diffmean}\NormalTok{(Years }\OperatorTok{~}\StringTok{ }\KeywordTok{shuffle}\NormalTok{(Attr), }\DataTypeTok{data=}\NormalTok{MockJury2)}
\NormalTok{Ts}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  diffmean 
## -0.616643
\end{verbatim}

And putting this inside the \texttt{print} function allows us to find
the test statistic under 5 different permutations easily:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{B <-}\StringTok{ }\DecValTok{5}
\ControlFlowTok{for}\NormalTok{ (b }\ControlFlowTok{in}\NormalTok{ (}\DecValTok{1}\OperatorTok{:}\NormalTok{B))\{}
\NormalTok{  Ts <-}\StringTok{ }\KeywordTok{diffmean}\NormalTok{(Years }\OperatorTok{~}\StringTok{ }\KeywordTok{shuffle}\NormalTok{(Attr), }\DataTypeTok{data=}\NormalTok{MockJury2)}
  \KeywordTok{print}\NormalTok{(Ts)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   diffmean 
## -0.8300142 
##   diffmean 
## -0.1365576 
##    diffmean 
## -0.08321479 
##  diffmean 
## 0.5035562 
## diffmean 
## 1.677098
\end{verbatim}

Finally, we would like to store the values of the test statistic instead
of just printing them out on each pass through the loop. To do this, we
need to create a variable to store the results, let's call it
\texttt{Tstar}. We know that we need to store \texttt{B} results so will
create a vector of length B, which contains B elements, full of missing
values (NA) using the \texttt{matrix} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Tstar <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\OtherTok{NA}\NormalTok{, }\DataTypeTok{nrow=}\NormalTok{B)}
\NormalTok{Tstar}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1]
## [1,]   NA
## [2,]   NA
## [3,]   NA
## [4,]   NA
## [5,]   NA
\end{verbatim}

Now we can run our loop B times and store the results in \texttt{Tstar}.

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{ (b }\ControlFlowTok{in}\NormalTok{ (}\DecValTok{1}\OperatorTok{:}\NormalTok{B))\{}
\NormalTok{  Tstar[b] <-}\StringTok{ }\KeywordTok{diffmean}\NormalTok{(Years }\OperatorTok{~}\StringTok{ }\KeywordTok{shuffle}\NormalTok{(Attr), }\DataTypeTok{data=}\NormalTok{MockJury2)}
\NormalTok{\}}
\NormalTok{Tstar}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             [,1]
## [1,] -0.08321479
## [2,]  0.23684211
## [3,] -0.24324324
## [4,] -0.61664296
## [5,]  0.66358464
\end{verbatim}

Five permutations are still not enough to assess whether our \(T_{obs}\)
of 1.84 is unusual and we need to do many permutations to get an
accurate assessment of the possibilities under the null hypothesis. It
is common practice to consider something like 1,000 permutations. The
\texttt{Tstar} vector when we set \emph{B} the permutation distribution
for the selected test statistic under\footnote{We often say ``under'' in
  statistics and we mean ``given that the following is true''.} the null
hypothesis -- what is called the \textbf{\emph{null distribution}} of
the statistic. The null distribution is the distribution of possible
values of a statistic under the null hypothesis. We want to visualize
this distribution and use it to assess how unusual our \(T_{obs}\)
result of 1.84 years was relative to all the possibilities under
permutations (under the null hypothesis). So we repeat the loop, now
with \(B=1000\) and generate a histogram, density curve and summary
statistics of the results:




\begin{figure}
\centering
\includegraphics{02-reintroductionToStatistics_files/figure-latex/Figure2-9-1.pdf}
\caption{\label{fig:Figure2-9}Histogram (left, with counts in bars) and density curve
(right) of values of test statistic for 1,000 permutations.}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\NormalTok{B <-}\StringTok{ }\DecValTok{1000}
\NormalTok{Tstar <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\OtherTok{NA}\NormalTok{, }\DataTypeTok{nrow=}\NormalTok{B)}
\ControlFlowTok{for}\NormalTok{ (b }\ControlFlowTok{in}\NormalTok{ (}\DecValTok{1}\OperatorTok{:}\NormalTok{B))\{}
\NormalTok{  Tstar[b] <-}\StringTok{ }\KeywordTok{diffmean}\NormalTok{(Years }\OperatorTok{~}\StringTok{ }\KeywordTok{shuffle}\NormalTok{(Attr), }\DataTypeTok{data=}\NormalTok{MockJury2)}
\NormalTok{\}}
\KeywordTok{hist}\NormalTok{(Tstar, }\DataTypeTok{label=}\NormalTok{T)}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{density}\NormalTok{(Tstar), }\DataTypeTok{main=}\StringTok{"Density curve of Tstar"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{favstats}\NormalTok{(Tstar)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        min         Q1     median        Q3      max       mean        sd
##  -2.910384 -0.5099573 0.07681366 0.6102418 2.530583 0.04694168 0.8497364
##     n missing
##  1000       0
\end{verbatim}

Figure \ref{fig:Figure2-9} contains visualizations of \(T^*\) and the
\texttt{favstats} summary provides the related numerical summaries. Our
observed \(T_{obs}\) of 1.84 seems fairly unusual relative to these
results with only 20 \(T^*\) values over 2 based on the histogram. We
need to make more specific comparisons of the permuted results versus
our observed result to be able to clearly decide whether our observed
result is really unusual.

To make the comparisons more concrete, first we can enhance the previous
graphs by adding the value of the test statistic from the real data set,
as shown in Figure \ref{fig:Figure2-10}, using the \texttt{abline}
function.





\begin{figure}
\centering
\includegraphics{02-reintroductionToStatistics_files/figure-latex/Figure2-10-1.pdf}
\caption{\label{fig:Figure2-10}Histogram (left) and density curve (right) of values of
test statistic for 1,000 permutations with bold vertical line for value
of observed test statistic.}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\NormalTok{Tobs <-}\StringTok{ }\FloatTok{1.837}
\KeywordTok{hist}\NormalTok{(Tstar, }\DataTypeTok{labels=}\NormalTok{T)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v=}\NormalTok{Tobs, }\DataTypeTok{lwd=}\DecValTok{2}\NormalTok{, }\DataTypeTok{col=}\StringTok{"red"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{density}\NormalTok{(Tstar),}\DataTypeTok{main=}\StringTok{"Density curve of Tstar"}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v=}\NormalTok{Tobs, }\DataTypeTok{lwd=}\DecValTok{2}\NormalTok{, }\DataTypeTok{col=}\StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Second, we can calculate the exact number of permuted results that were
larger than what we observed. To calculate the proportion of the 1,000
values that were larger than what we observed, we will use the
\texttt{pdata} function. To use this function, we need to provide the
distribution of values to compare to the cut-off (\texttt{Tstar}), the
cut-off point (\texttt{Tobs}), and whether we want calculate the
proportion that are below (left of) or above (right of) the cut-off
(\texttt{lower.tail=F} option provides the proportion of values above
the cutoff of interest).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pdata}\NormalTok{(Tstar, Tobs, }\DataTypeTok{lower.tail=}\NormalTok{F)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.02
\end{verbatim}

The proportion of 0.02 tells us that 20 of the 1,000 permuted results
(2\%) were larger than what we observed. This type of work is how we can
generate \textbf{\emph{p-values}} using permutation distributions.
P-values, as you should remember, are the probability of getting a
result as extreme as or more extreme than what we observed, given that
the null is true. Finding only 20 permutations of 1,000 that were larger
than our observed result suggests that it is hard to find a result like
what we observed if there really were no difference, although it is not
impossible.

When testing hypotheses for two groups, there are two types of
alternative hypotheses, one-sided or two-sided. \textbf{\emph{One-sided
tests}} involve only considering differences in one-direction (like
\(\mu_1 > \mu_2\)) and are performed when researchers can decide
\textbf{\emph{a priori}}\footnote{This is a fancy way of saying ``in
  advance'', here in advance of seeing the observations.} which group
should have a larger mean if there is going to be any sort of
difference. In this situation, we did not know enough about the
potential impacts of the pictures to know which group should be larger
than the other so should do a two-sided test. It is important to
remember that you can't look at the responses to decide on the
hypotheses. It is often safer and more
\textbf{\emph{conservative}}\footnote{Statistically, a conservative
  method is one that provides less chance of rejecting the null
  hypothesis in comparison to some other method or less than some
  pre-defined standard.} to start with a \textbf{\emph{two-sided
alternative}} (\(\mathbf{H_A: \mu_1 \ne \mu_2}\)). To do a 2-sided test,
find the area larger than what we observed as above. We also need to add
the area in the other tail (here the left tail) similar to what we
observed in the right tail. Some people suggest doubling the area in one
tail but we will collect information on the number that were more
extreme than the same value in the other tail. In other words, we count
the proportion over 1.84 and below -1.84. So we need to also find how
many of the permuted results were smaller than -1.84years to add to our
previous proportion. Using \texttt{pdata} \texttt{-Tobs} as the cut-off
and \texttt{lower.tail=T} provides this result:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pdata}\NormalTok{(Tstar, }\OperatorTok{-}\NormalTok{Tobs, }\DataTypeTok{lower.tail=}\NormalTok{T)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.014
\end{verbatim}

So the p-value to test our null hypothesis of no difference in the true
means between the groups is 0.02 + 0.014, providing a p-value of 0.034.
Figure \ref{fig:Figure2-11} shows both cut-offs on the histogram and
density curve.





\begin{figure}
\centering
\includegraphics{02-reintroductionToStatistics_files/figure-latex/Figure2-11-1.pdf}
\caption{\label{fig:Figure2-11}Histogram and density curve of values of test statistic
for 1,000 permutations with bold lines for value of observed test
statistic and its opposite value required for performing two-sided test.}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\KeywordTok{hist}\NormalTok{(Tstar, }\DataTypeTok{labels=}\NormalTok{T)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}\OperatorTok{*}\NormalTok{Tobs, }\DataTypeTok{lwd=}\DecValTok{2}\NormalTok{, }\DataTypeTok{col=}\StringTok{"red"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{density}\NormalTok{(Tstar),}\DataTypeTok{main=}\StringTok{"Density curve of Tstar"}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}\OperatorTok{*}\NormalTok{Tobs, }\DataTypeTok{lwd=}\DecValTok{2}\NormalTok{, }\DataTypeTok{col=}\StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In general, the \textbf{\emph{one-sided test p-value}} is the proportion
of the permuted results that are more extreme than observed in the
direction of the \emph{alternative} hypothesis (lower or upper tail,
remembering that this also depends on the direction of the difference
taken). For the 2-sided test, the p-value is the proportion of the
permuted results that are \emph{less than the negative version of the
observed statistic and greater than the positive version of the observed
statistic}. Using absolute values (\textbar{} \textbar{}), we can
simplify this: the \textbf{\emph{two-sided p-value}} is the
\emph{proportion of the \textbar{}permuted statistics\textbar{} that are
larger than \textbar{}observed statistic\textbar{}}. This will always
work and finds areas in both tails regardless of whether the observed
statistic is positive or negative. In R, the \texttt{abs} function
provides the \textbf{\emph{absolute value}} and we can again use
\texttt{pdata} to find our p-value in one line of code:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pdata}\NormalTok{(}\KeywordTok{abs}\NormalTok{(Tstar), }\KeywordTok{abs}\NormalTok{(Tobs), }\DataTypeTok{lower.tail=}\NormalTok{F)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.034
\end{verbatim}

We will discuss the choice of \textbf{\emph{significance level}} below,
but for the moment, assume that \(\alpha\) is chosen to be our standard
value of 0.05. Since the p-value is smaller than \(\alpha\), this
suggests that we can \textbf{\emph{reject the null hypothesis}} and
conclude that there is evidence of some difference in the true mean
sentences given between the two types of pictures.

Before we move on, let's note some interesting features of the
permutation distribution of the difference in the sample means shown in
Figure \ref{fig:Figure2-11}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  It is basically centered at 0. Since we are performing permutations
  assuming the null model is true, we are assuming that
  \(\mu_1 = \mu_2\) which implies that \(\mu_1 - \mu_2 = 0\). This also
  suggests that 0 should be the center of the permutation distribution
  and it was.
\item
  It is approximately normally distributed. This is due to the
  \textbf{\emph{Central Limit Theorem}}\footnote{We'll leave the
    discussion of the CLT to your previous stat coursework or an
    internet search. Remember that it has something to do with
    distributions looking more normal as the sample size increases.},
  where the \textbf{\emph{sampling distribution}} (distribution of all
  possible results for samples of this size) of the difference in sample
  means (\(\bar{x}_1 - \bar{x}_2\)) becomes more and normally
  distributed as the sample sizes increase. With 38 and 37 observations
  in the groups, we are likely to have a relatively normal looking
  distribution of the difference in the sample means. This result will
  allow us to use a parametric method to approximate this sampling
  distribution under the null model if some assumptions are met, as
  we'll discuss below.
\item
  Our observed difference in the sample means (1.84 years) is a fairly
  unusual result relative to the rest of these results but there are
  some permuted data sets that produce more extreme differences in the
  sample means. When the observed differences are really large, we may
  not see any permuted results that are as extreme as what we observed.
  When \texttt{pdata} gives you 0, the p-value should be reported to be
  smaller than 0.0001 (\textbf{not 0!}) since it happened in less than 1
  in 1000 tries but does occur once -- in the actual data set.
\item
  Since our null model is not specific about the direction of the
  difference, considering a result like ours but in the other direction
  (-1.84 years) needs to be included. The observed result seems to put
  about the same area in both tails of the distribution but it is not
  exactly the same. The small difference in the tails is a useful aspect
  of this approach compared to the parametric method discussed below as
  it accounts for slight asymmetry in the sampling distribution.
\end{enumerate}

Earlier, we decided that the p-value was small enough to reject the null
hypothesis since it was smaller than our chosen level of significance.
In this course, you will often be allowed to use your own judgment about
an appropriate significance level in a particular situation (in other
words, if we forget to tell you an \(\alpha\) -level, you can still make
a decision using a reasonably selected significance level). Remembering
that the p-value is the probability you would observe a result like you
did (or more extreme), assuming the null hypothesis is true, this tells
you that the smaller the p-value is, the more evidence you have against
the null. The next section provides a more formal review of the
hypothesis testing infrastructure, terminology, and some of things that
can happen when testing hypotheses.

\section{Hypothesis testing (general)}\label{section2-5}

In hypothesis testing, it is formulated to answer a specific question
about a population or true parameter(s) using a statistic based on a
data set. In your previous statistics course, you (hopefully) considered
one-sample hypotheses about population means and proportions and the two
sample mean situation we are focused on here. Our hypotheses relate to
trying to answer the question about whether the population mean
sentences between the two groups are different, with an initial
assumption of no difference.

Hypothesis testing is much like a criminal trial where you are in the
role of a jury member or judge, if no jury is present. Initially, the
defendant is assumed innocent. In our situation, the true means are
assumed to be equal between the groups. Then evidence is presented and,
as a juror, you analyze it. In statistical hypothesis testing, data are
collected and analyzed. Then you have to decide if we had ``enough''
evidence to reject the initial assumption (``innocence'' that is
initially assumed). To make this decision, you want to have previously
decided on the standard of evidence required to reject the initial
assumption. In criminal cases, ``beyond a reasonable doubt'' is used.
Wikipedia's definition
(\url{https://en.wikipedia.org/wiki/Reasonable_doubt}) suggests that
this standard is that ``there can still be a doubt, but only to the
extent that it would not affect a reasonable person's belief regarding
whether or not the defendant is guilty''. In civil trials, a lower
standard called a ``preponderance of evidence'' is used. Based on that
defined and pre-decided (\emph{a priori}) measure, you decide that the
defendant is guilty or not guilty. In statistics, we compare our p-value
to a significance level, \(\alpha\), which is most of the time selected
to be 5\%. If our p-value is less than \(\alpha\), we reject the null
hypothesis. The choice of the significance level is like the variation
in standards of evidence between criminal and civil trials -- and in all
situations everyone should know the standards required for rejecting the
initial assumption before any information is ``analyzed''. Once someone
is found guilty, then there is the matter of sentencing which is related
to the impacts (``size'') of the crime. In statistics, this is similar
to the estimated size of differences and the related judgments about
whether the differences are practically important or not. If the crime
is proven beyond a reasonable doubt but it is a minor crime, then the
sentence will be small. With the same level of evidence and a more
serious crime, the sentence will be more dramatic.

There are some important aspects of the testing process to note that
inform how we interpret statistical hypothesis test results. When
someone is found ``not guilty'', it does not mean ``innocent'', it just
means that there was not enough evidence to find the person guilty
``beyond a reasonable doubt''. Not finding enough evidence to reject the
null hypothesis does not imply that the true means are equal, just that
there was not enough evidence to conclude that they were different.
There are many potential reasons why we might fail to reject the null,
but the most common one is that our sample size was too small (which is
related to having too little evidence).

Throughout this material, we will continue to re-iterate the
distinctions between parameters and statistics and want you to be clear
about the distinctions between estimates based on the sample and
inferences for the population or true values of the parameters of
interest. Remember that statistics are summaries of the sample
information and parameters are characteristics of populations (which we
rarely know). In the two-sample mean situation, the sample means are
always at least a little different -- that is not an interesting
conclusion. What is interesting is whether we have enough evidence to
prove that the population or true means differ ``beyond a reasonable
doubt''.

The scope of any inferences is constrained based on whether there is a
\textbf{\emph{random sample}} (RS) and/or \textbf{\emph{random
assignment}} (RA). Table \ref{tab:Table2-2} contains the four possible
combinations of these two characteristics of a given study. Random
assignment allows for causal inferences for differences that are
observed -- the difference in treatment levels causes differences in the
mean responses. Random sampling (or at least some sort of representative
sample) allows inferences to be made to the population of interest. If
we do not have RA, then causal inferences cannot be made. If we do not
have a representative sample, then our inferences are limited to the
sampled subjects.

\footnotesize



\begin{longtable}[]{@{}lll@{}}
\caption{\label{tab:Table2-2} Scope of inference summary.}\tabularnewline
\toprule
\begin{minipage}[b]{0.27\columnwidth}\raggedright\strut
\textbf{Random\\
Sampling/Random\\
Assignment}\strut
\end{minipage} & \begin{minipage}[b]{0.31\columnwidth}\raggedright\strut
\textbf{Random Assignment (RA)\\
-- Yes (controlled experiment)}\strut
\end{minipage} & \begin{minipage}[b]{0.33\columnwidth}\raggedright\strut
\textbf{Random Assignment (RA)\\
-- No (observational study)}\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.27\columnwidth}\raggedright\strut
\textbf{Random\\
Sampling/Random\\
Assignment}\strut
\end{minipage} & \begin{minipage}[b]{0.31\columnwidth}\raggedright\strut
\textbf{Random Assignment (RA)\\
-- Yes (controlled experiment)}\strut
\end{minipage} & \begin{minipage}[b]{0.33\columnwidth}\raggedright\strut
\textbf{Random Assignment (RA)\\
-- No (observational study)}\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.27\columnwidth}\raggedright\strut
\textbf{Random Sampling (RS)\\
-- Yes (or some method\\
that results in a\\
representative sample of\\
population of\\
interest)}\strut
\end{minipage} & \begin{minipage}[t]{0.31\columnwidth}\raggedright\strut
Because we have RS, we can\\
generalize inferences to the\\
population the RS was taken\\
from. Because we have\\
RA we can assume the groups\\
were equivalent on all aspects\\
except for the treatment\\
and can establish causal inference.\\
\strut
\end{minipage} & \begin{minipage}[t]{0.33\columnwidth}\raggedright\strut
Can generalize inference to\\
population the RS was taken\\
from but cannot establish\\
causal inference (no RA\\
-- cannot isolate treatment\\
variable as only difference\\
among groups, could be\\
confounding variables).\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.27\columnwidth}\raggedright\strut
\textbf{Random Sampling (RS)\\
-- No (usually a\\
convenience sample)}\strut
\end{minipage} & \begin{minipage}[t]{0.31\columnwidth}\raggedright\strut
Cannot generalize inference to\\
the population of interest\\
because the sample was\\
not random and could be\\
biased -- may not be\\
``representative'' of the\\
population of interest.\\
Can establish causal\\
inference due to RA \(\rightarrow\)\\
the inference from this type of\\
study applies only to the sample.\strut
\end{minipage} & \begin{minipage}[t]{0.33\columnwidth}\raggedright\strut
Cannot generalize inference to\\
the population of interest\\
because the sample was\\
not random and could be\\
biased -- may not be\\
``representative'' of the\\
population of interest.\\
Cannot establish causal\\
inference due to lack of RA of\\
the treatment.\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\normalsize

A simple example helps to clarify how the scope of inference can change.
Suppose we are interested in studying the GPA of students. If we had
taken a random sample from, say, the STAT 217 students in a given
semester, our scope of inference would be the population of 217 students
in that semester. If we had taken a random sample from the entire MSU
population, then the inferences would be to the entire MSU population in
that semester. These are similar types of problems but the two
populations are very different and the group you are trying to make
conclusions about should be noted carefully in your results -- it does
matter! If we did not have a representative sample, say the students
could choose to provide this information or not, then we can only make
inferences to volunteers. These volunteers might differ in systematic
ways from the entire population of STAT 217 students so we cannot safely
extend our inferences beyond the group that volunteered.

To consider the impacts of RA versus observational studies, we need to
be comparing groups. Suppose that we are interested in differences in
the mean GPAs for different sections of STAT 217 and that we take a
random sample of students from each section and compare the results and
find evidence of some difference. In this scenario, we can conclude that
there is some difference in the population of STAT 217 students but we
can't say that being in different sections caused the differences in the
mean GPAs. Now suppose that we randomly assigned every 217 student to
get extra training in one of three different study techniques and found
evidence of differences among the training methods. We could conclude
that the training methods caused the differences in these students.
These conclusions would only apply to STAT 217 students and could not be
generalized to a larger population of students. If we took a random
sample of STAT 217 students (say only 10 from each section) and then
randomly assigned them to one of three training programs and found
evidence of differences, then we can say that the training programs
caused the differences. We can also say that we have evidence that those
differences pertain to the population of STAT 217 students. This seems
similar to the scenario where all 217 students participated in the
training programs except that by using random sampling, only a fraction
of the population needs to actually be studied to make inferences to the
entire population of interest -- saving time and money.

A quick summary of the terminology of hypothesis testing is useful at
this point. The \textbf{\emph{null hypothesis}} (\(H_0\)) states that
there is no difference or no relationship in the population. This is the
statement of no effect or no difference and the claim that we are trying
to find evidence against. In this chapter, \(H_0\): \(\mu_1=\mu_2\).
When doing two-group problems, you always need to specify which group is
1 and which one is 2 because the order does matter. The
\textbf{\emph{alternative hypothesis}} (\(H_1\) or \(H_A\)) states a
specific difference between parameters. This is the research hypothesis
and the claim about the population that we hope to demonstrate is more
reasonable to conclude than the null hypothesis. In the two-group
situation, we can have \textbf{\emph{one-sided alternatives}}
\(H_A\):\(\mu_1 > \mu_2\) (greater than) or \(H_A\):\(\mu_1 < \mu_2\)
(less than) or, the more common, \textbf{\emph{two-sided alternative}}
\(H_A\):\(\mu_1 \ne \mu_2\) (not equal to). We usually default to using
two-sided tests because we often do not know enough to know the
direction of a difference in advance, especially in more complicated
situations. The \textbf{\emph{sampling distribution under the null}} is
the distribution of all possible values of a statistic under \(H_0\) is
true. It is used to calculate the \textbf{\emph{p-value}} , the
probability of obtaining a result as extreme or more extreme than what
we observed given that the null hypothesis is true. We will find
sampling distributions using \textbf{\emph{nonparametric}} approaches
(like the permutation approach used above) and
\textbf{\emph{parametric}} methods (using ``named'' distributions like
the \(t\), F, and \(\chi^2\)).

Small p-values are evidence against the null hypothesis because the
observed result is unlikely due to chance if \(H_0\) is true. Large
p-values provide no evidence against \(H_0\) but do not allow us to
conclude that there is no difference. The \textbf{\emph{level of
significance}} is an \emph{a priori} definition of how small the p-value
needs to be to provide ``enough'' (sufficient) evidence against \(H_0\).
This is most useful to prevent sliding the standards after the results
are found. We compare the p-value to the level of significance to decide
if the p-value is small enough to constitute sufficient evidence to
reject the null hypothesis. We use \(\alpha\) to denote the level of
significance and most typically use 0.05 which we refer to as the 5\%
significance level. We compare the p-value to this level and make a
decision. The two options for \emph{decisions} are to either
\emph{reject the null hypothesis} if the p-value \(\le \alpha\) or
\emph{fail to reject the null hypothesis} if the p-value \(> \alpha\).
When interpreting hypothesis testing results, remember that the p-value
is a measure of how unlikely the observed outcome was, assuming that the
null hypothesis is true. It is \textbf{NOT} the probability of the data
or the probability of either hypothesis being true. The p-value, simply,
is a measure of evidence against the null hypothesis.

The specific definition of \(\alpha\) is that it is the probability of
rejecting \(H_0\) when \(H_0\) is true, the probability of what is
called a \textbf{\emph{Type I error}}. Type I errors are also called
\textbf{\emph{false rejections}}. In the two-group mean situation, a
Type I error would be concluding that there is a difference in the true
means between the groups when none really exists in the population. In
the courtroom setting, this is like falsely finding someone guilty. We
don't want to do this very often, so we use small values of the
significance level, allowing us to control the rate of Type I errors at
\(\alpha\). We also have to worry about \textbf{Type II errors}, which
are failing to reject the null hypothesis when it's false. In a
courtroom, this is the same as failing to convict a truly guilty person.
This most often occurs due to a lack of evidence. You can use the Table
\ref{tab:Table2-3} to help you remember all the possibilities.




\begin{longtable}[]{@{}lll@{}}
\caption{\label{tab:Table2-3} Table of decisions and truth scenarios in a hypothesis
testing situation. But we never know the truth in a real situation.}\tabularnewline
\toprule
\begin{minipage}[b]{0.30\columnwidth}\raggedright\strut
~\strut
\end{minipage} & \begin{minipage}[b]{0.30\columnwidth}\raggedright\strut
\(\mathbf{H_0}\) \textbf{True}\strut
\end{minipage} & \begin{minipage}[b]{0.30\columnwidth}\raggedright\strut
\(\mathbf{H_0}\) \textbf{False}\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.30\columnwidth}\raggedright\strut
~\strut
\end{minipage} & \begin{minipage}[b]{0.30\columnwidth}\raggedright\strut
\(\mathbf{H_0}\) \textbf{True}\strut
\end{minipage} & \begin{minipage}[b]{0.30\columnwidth}\raggedright\strut
\(\mathbf{H_0}\) \textbf{False}\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.30\columnwidth}\raggedright\strut
\textbf{FTR} \(\mathbf{H_0}\)\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright\strut
Correct decision\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright\strut
Type II error\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.30\columnwidth}\raggedright\strut
\textbf{Reject} \(\mathbf{H_0}\)\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright\strut
Type I error\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright\strut
Correct decision\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

In comparing different procedures, there is an interest in studying the
rate or probability of Type I and II errors. The probability of a Type I
error was defined previously as \(\alpha\), the significance level. The
\textbf{\emph{power}} of a procedure is the probability of rejecting the
null hypothesis when it is false. Power is defined as

\[\text{Power} = 1 - \text{Probability(Type II error) } = 
\text{Probability(Reject) } H_0 | H_0 \text{ is false),}\]

or, in words, the probability of detecting a difference when it actually
exists. We want to use a statistical procedure that controls the Type I
error rate at the pre-specified level and has high power to detect false
null alternatives. Increasing the sample size is one of the most
commonly used methods for increasing the power in a given situation.
Sometimes we can choose among different procedures and use the power of
the procedures to help us make that selection. Note that there are many
ways \(H_0\) false and the power changes based on how false the null
hypothesis actually is. To make this concrete, suppose that the true
mean sentences differed by either 1 or 20 years in previous example. The
chances of rejecting the null hypothesis are much larger when the groups
actually differ by 20 years than if they differ by just 1 year.

After making a decision (was there enough evidence to reject the null or
not), we want to make the conclusions specific to the problem of
interest. If we reject \(H_0\), then we can conclude that there was
sufficient evidence at the \(\alpha\)-level that the null hypothesis is
wrong (and the results point in the direction of the alternative). If we
fail to reject \(H_0\) (FTR \(H_0\)), then we can conclude that there
was insufficient evidence at the \(\alpha\)-level to say that the null
hypothesis is wrong. We are \textbf{NOT} saying that the null is correct
and we \textbf{NEVER} accept the null hypothesis. We just failed to find
enough evidence to say it's wrong. If we find sufficient evidence to
reject the null, then we need to revisit the method of data collection
and design of the study to discuss scope of inference. Can we discuss
causality (due to RA) and/or make inferences to a larger group than
those in the sample (due to RS)?

To perform a hypothesis test, there are some steps to remember to
complete to make sure you have thought through all the aspects of the
results.

\begin{longtable}[]{@{}l@{}}
\toprule
\begin{minipage}[t]{0.97\columnwidth}\raggedright\strut
\textbf{Outline of 6+ steps to perform a Hypothesis Test}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.97\columnwidth}\raggedright\strut
Isolate the claim to be proved, method to use (define a test statistic
T), and significance level.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.97\columnwidth}\raggedright\strut
1. Write the null and alternative hypotheses,\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.97\columnwidth}\raggedright\strut
2. Assess the ``Validity Conditions'' for the procedure being used
(discussed below),\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.97\columnwidth}\raggedright\strut
3. Find the value of the appropriate test statistic,\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.97\columnwidth}\raggedright\strut
4. Find the p-value,\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.97\columnwidth}\raggedright\strut
5. Make a decision, and\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.97\columnwidth}\raggedright\strut
6. Write a conclusion specific to the problem, including scope of
inference discussion.\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\section{Connecting randomization (nonparametric) and parametric
tests}\label{section2-6}

In developing statistical inference techniques, we need to define the
test quantity of interest. To compare the means of two groups, a
statistic is needed that measures their differences. In general, for
comparing two groups, the choices are simple -- a difference in the
means often works well and is a natural choice.

There are other options such as tracking the ratio of means or possibly
the difference in medians. Instead of just using the difference in the
means, we also could ``standardize'' the difference in the means by
dividing by an appropriate quantity that reflects the variation in the
difference in the means. All of these are valid and can sometimes
provide similar results - it ends up that there are many possibilities
for testing using the randomization (nonparametric) techniques
introduced previously. Parametric statistical methods focus on means
because the statistical theory surrounding means is quite a bit easier
(not easy, just easier) than other options but there are just a couple
of test statistics that you can use and end up with named distributions
to use for generating inferences. Randomization techniques allow
inference for other quantities but our focus here will be on using
randomization for inferences on means to see the similarities with the
more traditional parametric procedures.

In two-sample mean situations, instead of working just with the
difference in the means, we often calculate a test statistic that is
called the \textbf{\emph{equal variance two-independent samples
t-statistic}}. The test statistic is

\[t = \frac{\bar{x}_1 - \bar{x}_2}{s_p\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}},\]

where \(s_1^2\) and \(s_2^2\) are the sample variances for the two
groups, \(n_1\) and \(n_2\) are the sample sizes for the two groups, and
the \textbf{\emph{pooled sample standard deviation}},

\[s_p = \sqrt{\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2}}.\]

The \(t\)-statistic keeps the important comparison between the means in
the numerator that we used before and standardizes (re-scales) that
difference so that \(t\) will follow a \(t\)-distribution (a parametric
``named''distribution) if certain assumptions are met. But first we
should see if standardizing the difference in the means had an impact on
our permutation test results. Instead of using the \texttt{diffmean}
function, we will use the \texttt{t.test} function (see its full use
below) and have it calculate the formula for \(t\) for us. The R code
``\texttt{\$statistic}'' is basically a way of extracting just the
number we want to use for \(T\) from a larger set of output the
\texttt{t.test} function wants to provide you. We will see below that
\texttt{t.test} switches the order of the difference (now it is
\emph{Average} - \emph{Unattractive}) -- always carefully check for the
direction of the difference in the results. Since we are doing a
two-sided test, the code resembles the permutation test code in Section
\ref{section2-4} with the new \(t\)-statistic replacing the difference
in the sample means that we used before.

The permutation distribution in Figure \ref{fig:Figure2-12} looks
similar to the previous results with slightly different \(x\)-axis
scaling. The the proportion of permuted results that were more extreme
than the observed result was 0.031. This difference is due to a
different set of random permutations being selected. If you run
permutation code, you will often get slightly different results each
time you run it. If you are uncomfortable with the variation in the
results, you can run more than \(B=\) 1,000 permutations (say 10,000)
and the variability in the resulting p-values will be reduced further.
Usually this uncertainty will not cause any substantive problems -- but
do not be surprised if your results vary from a colleagues if you are
both analyzing the same data set or if you re-run your permutation code.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\NormalTok{Tobs <-}\StringTok{ }\KeywordTok{t.test}\NormalTok{(Years }\OperatorTok{~}\StringTok{ }\NormalTok{Attr, }\DataTypeTok{data=}\NormalTok{MockJury2, }\DataTypeTok{var.equal=}\NormalTok{T)}\OperatorTok{$}\NormalTok{statistic}
\NormalTok{Tobs}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        t 
## -2.17023
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Tstar <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\OtherTok{NA}\NormalTok{, }\DataTypeTok{nrow=}\NormalTok{B)}
\ControlFlowTok{for}\NormalTok{ (b }\ControlFlowTok{in}\NormalTok{ (}\DecValTok{1}\OperatorTok{:}\NormalTok{B))\{}
\NormalTok{  Tstar[b] <-}\StringTok{ }\KeywordTok{t.test}\NormalTok{(Years }\OperatorTok{~}\StringTok{ }\KeywordTok{shuffle}\NormalTok{(Attr), }\DataTypeTok{data=}\NormalTok{MockJury2, }\DataTypeTok{var.equal=}\NormalTok{T)}\OperatorTok{$}\NormalTok{statistic}
\NormalTok{\}}
\KeywordTok{pdata}\NormalTok{(}\KeywordTok{abs}\NormalTok{(Tstar),}\KeywordTok{abs}\NormalTok{(Tobs),}\DataTypeTok{lower.tail=}\NormalTok{F)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     t 
## 0.031
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{hist}\NormalTok{(Tstar, }\DataTypeTok{labels=}\NormalTok{T)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}\OperatorTok{*}\NormalTok{Tobs, }\DataTypeTok{lwd=}\DecValTok{2}\NormalTok{, }\DataTypeTok{col=}\StringTok{"red"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{density}\NormalTok{(Tstar), }\DataTypeTok{main=}\StringTok{"Density curve of Tstar"}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}\OperatorTok{*}\NormalTok{Tobs, }\DataTypeTok{lwd=}\DecValTok{2}\NormalTok{, }\DataTypeTok{col=}\StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}



\begin{figure}
\centering
\includegraphics{02-reintroductionToStatistics_files/figure-latex/Figure2-12-1.pdf}
\caption{\label{fig:Figure2-12}Permutation distribution of the \(t\)-statistic}
\end{figure}

The parametric version of these results is based on using what is called
the \textbf{\emph{two-independent sample t-test}}. There are actually
two versions of this test, one that assumes that variances are equal in
the groups and one that does not. There is a rule of thumb that if the
\textbf{ratio of the larger standard deviation over the smaller standard
deviation is less than 2, the equal variance procedure is OK}. It ends
up that this assumption is less important if the sample sizes in the
groups are approximately equal and more important if the groups contain
different numbers of observations. In comparing the two potential test
statistics, the procedure that assumes equal variances has a complicated
denominator (see the formula above for \(t\) involving \(s_p\)) but a
simple formula for \textbf{\emph{degrees of freedom}}
(\textbf{\emph{df}}) for the \(t\)-distribution (\(df=n_1+n_2-2\)) that
approximates the distribution of the test statistic, \(t\), under the
null hypothesis. The procedure that assumes unequal variances has a
simpler test statistic and a very complicated degrees of freedom
formula. The equal variance procedure is most similar to the ANOVA
methods we will consider in Chapters 2 and 3 so that will be our focus
for the two group problem. Fortunately, both of these methods are
readily available in the \texttt{t.test} function in R if needed.

If the assumptions for the equal variance \(t\)-test are met and the
null hypothesis is true, then the sampling distribution of the test
statistic should follow a \(t\)-distribution with \(n_1+n_2-2\) degrees
of freedom. The \textbf{\emph{t-distribution}} is a bell-shaped curve
that is more spread out for smaller values of degrees of freedom as
shown in Figure \ref{fig:Figure2-13}. The \(t\)-distribution looks more
and more like a \textbf{\emph{standard normal distribution}}
(\(N(0,1)\)) as the degrees of freedom increase.
=======
The mean squares are formed by taking the sums of squares (we'll let R
find those for us) and dividing by the \(df\) in the row. The
\(F\)-ratios are found by taking the mean squares from the row and
dividing by the mean squared error (\(\text{MS}_E\)). They follow
\(F\)-distributions with numerator degrees of freedom from the row and
denominator degrees of freedom from the Error row (in R output this the
\texttt{Residuals} row). It is possible to develop permutation tests for
these methods but some technical issues arise in doing permutation tests
for interaction model components so we will not use them here. This
means we will have to place even more emphasis on meeting the
assumptions since we only have the parametric method available.

With some basic expectations about the ANOVA tables and \(F\)-statistic
construction in mind, we can get to actually estimating the models and
exploring the results. The first example involves the fake paper towel
data displayed in Figure \ref{fig:Figure4-1} and \ref{fig:Figure4-2}. It
appeared that Scenario 5 was the correct story since the lines were not
parallel, but we need to know whether there is evidence to suggest that
the interaction is ``real'' and we get that through the interaction
hypothesis test. To fit the interaction model using \texttt{lm}, the
general formulation is
\texttt{lm(y\ \textasciitilde{}\ x1*x2,\ data=...)}. The order of the
variables doesn't matter and the most important part of the model, to
start with, relates to the interaction of the variables.

The ANOVA table output shows the results for the interaction model
obtained by running the \texttt{anova} function on the model called
\texttt{m1}. Specifically, the test that
\(H_0: \text{ All } \omega_{jk}\text{'s} = 0\) has a test statistic of
\(F(2,24)=1.92\) (in bold in the output from the row with brands:drops)
and a p-value of 0.17. So there is insufficient evidence to reject the
null hypothesis of no interaction, with a 17\% chance we would observe a
difference in the \(\omega_{jk}\text{'s}\) like we did or more extreme
if the \(\omega_{jk}\text{'s}\) really were all 0. For the interaction
model components, R presents them with a colon, \texttt{:}, between the
variable names.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m1<-}\KeywordTok{lm}\NormalTok{(responses }\OperatorTok{~}\StringTok{ }\NormalTok{brand}\OperatorTok{*}\NormalTok{drops, }\DataTypeTok{data=}\NormalTok{pt)}
\KeywordTok{anova}\NormalTok{(m1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Analysis of Variance Table
## 
## Response: responses
##             Df Sum Sq Mean Sq F value   Pr(>F)
## brand        1 4.3322  4.3322 10.5192 0.003458
## drops        2 4.8581  2.4290  5.8981 0.008251
## brand:drops  2 1.5801  0.7901  1.9184 0.168695
## Residuals   24 9.8840  0.4118
\end{verbatim}

It is useful to display the estimates from this model and we can utilize
\texttt{plot(allEffects(modelname))} to visualize the results for the
terms in our models. If we turn on the options for \texttt{grid=T},
\texttt{multiline=T}, and \texttt{ci.\ style="bars"} we will get a more
useful version of the basic ``effect plot'' for Two-Way ANOVA models
with interaction. The results of the estimated interaction model are
displayed in Figure \ref{fig:Figure4-6}, which looks very similar to our
previous interaction plot. The only difference is that this comes from
model that assumes equal variance and these plots show 95\% confidence
intervals for the means instead of the 1 standard error used above.
>>>>>>> origin/chapter0to1_edits



\begin{figure}
\centering
<<<<<<< HEAD
\includegraphics{chapter1_files/image045small.png}
\caption{\label{fig:Figure2-13}Plots of \(t\) and normal distributions}
\end{figure}

To get the p-value for the parametric \(t\)-test, we need to calculate
the test statistic and \(df\), then look up the areas in the tails of
the \(t\)-distribution relative to the observed \(t\)-statistic. We'll
learn how to use R to do this below, but for now we will allow the
\texttt{t.test} function to take care of this for us. The
\texttt{t.test} function uses our formula notation
(\texttt{Years\ \textasciitilde{}\ Attr}) and then \texttt{data=...} as
we saw before for making plots. To get the equal-variance test result,
the \texttt{var.equal=T} option needs to be turned on. Then
\texttt{t.test} provides us with lots of useful output. The three
results we've been discussing are highlighted in the output below -- the
test statistic value (-2.17), \(df=73\), and the p-value, from the
\(t\)-distribution with 73 degrees of freedom, of 0.033.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{t.test}\NormalTok{(Years }\OperatorTok{~}\StringTok{ }\NormalTok{Attr, }\DataTypeTok{data=}\NormalTok{MockJury2, }\DataTypeTok{var.equal=}\NormalTok{T)}
=======
\includegraphics{04-twoWayAnova_files/figure-latex/Figure4-6-1.pdf}
\caption{\label{fig:Figure4-6}Plot of estimated results of interaction model.}
\end{figure}

In the absence of evidence to include the interaction, the model should
be simplified to the additive model and the interpretation focused on
each main effect, conditional on having the other variable in the model.
To fit an additive model and not include an interaction, the model
formula involves a ``+'' instead of a ``*" between the explanatory
variables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m2<-}\KeywordTok{lm}\NormalTok{(responses }\OperatorTok{~}\StringTok{ }\NormalTok{brand }\OperatorTok{+}\StringTok{ }\NormalTok{drops, }\DataTypeTok{data=}\NormalTok{pt)}
\KeywordTok{anova}\NormalTok{(m2)}
>>>>>>> origin/chapter0to1_edits
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
<<<<<<< HEAD
## 
##  Two Sample t-test
## 
## data:  Years by Attr
## t = -2.1702, df = 73, p-value = 0.03324
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -3.5242237 -0.1500295
## sample estimates:
##      mean in group Average mean in group Unattractive 
##                   3.973684                   5.810811
\end{verbatim}

So the parametric \(t\)-test gives a p-value of 0.033 from a test
statistic of -2.1702. The negative sign on the test statistic occurred
because the function took \emph{Average} - \emph{Unattractive} which is
the opposite direction as \texttt{diffmean}. The p-value is very similar
to the two permutation results found before. The reason for this
similarity is that the permutation distribution with 73 degrees of
freedom. Figure \ref{fig:Figure2-14} shows how similar the two
distributions happened to be here.



\begin{figure}
\centering
\includegraphics{chapter1_files/image047small.png}
\caption{\label{fig:Figure2-14}Plot of permutation and \(t\) distribution with \(df=73\).}
\end{figure}

In your previous statistics course, you might have used an applet or a
table to find p-values such as what was provided in the previous R
output. When not directly provided in the output of a function, R can be
used to look up p-values\footnote{On exams, you will be asked to
  describe the area of interest, sketch a picture of the area of
  interest, and/or note the distribution you would use.} from named
distributions such as the \(t\)-distribution. In this case, the
distribution of the test statistic under the null hypothesis is a
\(t(73)\) or a \(t\) with 73 degrees of freedom. The \texttt{pt}
function is used to get p-values from the \(t\)-distribution in the same
manner that \texttt{pdata} could help us to find p-values from the
permutation distribution. We need to provide the \texttt{df=...} and
specify the tail of the distribution of interest using the
\texttt{lower.tail} option along with the cutoff of interest. If we want
the area to the left of -2.17:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pt}\NormalTok{(}\OperatorTok{-}\FloatTok{2.1702}\NormalTok{, }\DataTypeTok{df=}\DecValTok{73}\NormalTok{, }\DataTypeTok{lower.tail=}\NormalTok{T)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.01662286
\end{verbatim}

And we can double it to get the p-value that \texttt{t.test} provided
earlier, because the \(t\)-distribution is symmetric:

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{2}\OperatorTok{*}\KeywordTok{pt}\NormalTok{(}\OperatorTok{-}\FloatTok{2.1702}\NormalTok{, }\DataTypeTok{df=}\DecValTok{73}\NormalTok{, }\DataTypeTok{lower.tail=}\NormalTok{T)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.03324571
\end{verbatim}

More generally, we could always make the test statistic positive using
the absolute value, find the area to the right of it, and then double
that for a two-sided test p-value:

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{2}\OperatorTok{*}\KeywordTok{pt}\NormalTok{(}\KeywordTok{abs}\NormalTok{(}\OperatorTok{-}\FloatTok{2.1702}\NormalTok{), }\DataTypeTok{df=}\DecValTok{73}\NormalTok{, }\DataTypeTok{lower.tail=}\NormalTok{T)}
=======
## Analysis of Variance Table
## 
## Response: responses
##           Df  Sum Sq Mean Sq F value   Pr(>F)
## brand      1  4.3322  4.3322  9.8251 0.004236
## drops      2  4.8581  2.4290  5.5089 0.010123
## Residuals 26 11.4641  0.4409
\end{verbatim}

The p-values for the main effects of \texttt{brand} and \texttt{drops}
change slightly from the results in the interaction model due to changes
in the \(\text{MS}_E\) from 0.4118 to 0.4409 (more variability is left
over in the simpler model) and the \(\text{DF}_{\text{error}}\) that
increases from 24 to 26. In both models, the
\(\text{SS}_{\text{Total}}\) is the same (20.6544). In the interaction
model,

\[\begin{array}{rl}
\text{SS}_{\text{Total}} & = \text{SS}_{\text{brand}} + \text{SS}_{\text{drops}}
+ \text{SS}_{\text{brand:drops}} + \text{SS}_{\text{E}}\\
& = 4.3322 + 4.8581 + 1.5801 + 9.8840\\
& = 20.6544\\
\end{array}\]

In the additive model, the variability that was attributed to the
interaction term in the interaction model
(\(\text{SS}_{\text{brand:drops}} = 1.5801\)) is pushed into the
\(\text{SS}_{\text{E}}\), which increases from 9.884 to 11.4641. The
sums of squares decomposition in the additive model is

\[\begin{array}{rl}
\text{SS}_{\text{Total}} & = \text{SS}_{\text{brand}} + \text{SS}_{\text{drops}}
 + \text{SS}_{\text{E}} \\
& = 4.3322 + 4.8581 + 11.4641 \\
& = 20.6544 \\
\end{array}\]

This shows that the sums of squares decomposition applies in these more
complicated models as it did in the One-Way ANOVA. It also shows that if
the interaction is removed from the model, that variability is lumped in
with the other unexplained variability that goes in the
\(\text{SS}_{\text{E}}\) in any model.

The fact that the sums of squares decomposition can be applied here is
useful, except that there is a small issue with the main effect tests in
the ANOVA table results that follow this decomposition when the design
is not balanced. It ends up that the tests in a typical ANOVA table are
only conditional on the tests higher up in the table. For example, in
the additive model ANOVA table, the \texttt{Brand} test is not
conditional on the \texttt{Drops} effect, but the \texttt{Drops} effect
is conditional on the \texttt{Brand} effect. To fix this issue, we have
to use another type of sums of squares, called \textbf{\emph{Type II
sums of squares}}. They will no longer always follow the rules of the
sums of squares decomposition but they will test the desired hypotheses.
Specifically, they provide each test conditional on any other terms at
the same level of the model and match the hypotheses written out earlier
in this section. To get the ``correct'' ANOVA results, the \texttt{car}
(\citet{R-car}, \citet{Fox2011}) package is required. We use the
\texttt{Anova} function on our linear models from here forward to get
the ``right'' tests in our ANOVA tables. Note how the case-sensitive
nature of R code shows up in the use of the capital-A \texttt{Anova}
function instead of the \texttt{anova} function used previously. In this
case, because the design was balanced, the results are the same using
either function. Observational studies rarely generate balanced designs
(some designed studies can result in unbalanced designs) so we will
generally just use the Type II version of the sums of squares. The
\texttt{Anova} results using the Type II sums of squares are slightly
more conservative than the results from \texttt{anova}, which are called
Type I sums of squares. The sums of squares decomposition no longer can
be applied, but it is a small sacrifice to get each test after adjusting
for all other variables\footnote{Actually, the tests are only
  conditional on other main effects if Type II Sums of Squares are used
  for an interaction model.}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{require}\NormalTok{(car)}
\KeywordTok{Anova}\NormalTok{(m2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Anova Table (Type II tests)
## 
## Response: responses
##            Sum Sq Df F value   Pr(>F)
## brand      4.3322  1  9.8251 0.004236
## drops      4.8581  2  5.5089 0.010123
## Residuals 11.4641 26
\end{verbatim}

The new output switches the columns around and doesn't show you the mean
squares, but gives the most critical parts of the output. Here, there is
no change in results because it is balanced design with equal counts of
responses in each combination of the two explanatory variables.

The additive model, when appropriate, provides simpler interpretations
for each explanatory variable compared to models with interactions
because the effect of one variable is the same regardless of the levels
of the other variable and vice versa. There are two tools to aid in
understanding the impacts of the two variables in the additive model.
First, the model summary provides estimated coefficients with
interpretations like those seen in Chapter \ref{chapter3} (deviation of
group \(j\) or \(k\) from the baseline group's mean), except with the
additional wording of ``controlling for'' the other variable added to
any of the discussion. Second, the term-plots now show each main effect
and how the groups differ with one panel for each of the two explanatory
variables in the model. These term-plots are created by holding the
other variable constant at one of its levels.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(m2)}
>>>>>>> origin/chapter0to1_edits
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
<<<<<<< HEAD
## [1] 1.966754
\end{verbatim}

Permutation distributions do not need to match the named parametric
distribution to work correctly, although this happened in the previous
example. The parametric certain conditions to be met for the sampling
distribution of the statistic to follow the named distribution and
provide accurate p-values. The conditions for the equal variance t-test
are:

\newpage

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Independent observations}: Each observation obtained is
  unrelated to all other observations. To assess this, consider whether
  anything in the data collection might lead to clustered or related
  observations that are un-related to the differences in the groups. For
  example, was the same person measured more than once?\footnote{In some
    studies, the same subject might be measured in both conditions and
    this violates the assumptions of this procedure.}
\item
  \textbf{Equal variances} in the groups (because we used a procedure
  that assumes equal variances! -- there is another procedure that
  allows you to relax this assumption if needed\ldots{}). To assess
  this, compare the standard deviations and variability in the beanplots
  and see if they look noticeably different. Be particularly critical of
  this assessment if the sample sizes differ greatly between groups.
\item
  \textbf{Normal distributions} of the observations in each group. We'll
  learn more diagnostics later, but the boxplots and beanplots are a
  good place to start to help you look for skews or outliers, which were
  both present here. If you find skew and/or outliers, that would
  suggest a problem with the assumption of normality as normal
  distributions are symmetric and extreme observations occur very
  rarely.
\end{enumerate}

For the permutation test, we relax the third condition and replace it
with:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{\emph{Similar distributions between the groups:}} The
  permutation approach allows valid inferences as long as the two groups
  have similar shapes and only possibly differ in their centers. In
  other words, the distributions need not look normal for the procedure
  to work well, but they do need to look similar.
\end{enumerate}

In the prisoner ``juror'' study, we can assume that the independent
observation condition is met because there is no information suggesting
that the same subjects were measured more than once or that some other
type of grouping in the responses was present (like the subjects were
divided in groups and placed in rooms to discuss their responses prior
to submitting them). The equal variance condition might be violated. The
variances need not be equal as the procedure can still provide
reasonable results with some violation of this assumption. The standard
deviations are 2.8 vs 4.4, so this difference is not ``large'' according
to the rule of thumb noted above. It is, however, close to being
considered problematic. It would be difficult to reasonably assume that
the normality condition is met here (Figure \ref{fig:Figure2-6} with
clear right skews in both groups and potential outliers which causes
concerns for (3) for the parametric procedure. The shapes look similar
for the two groups so there is less reason to be concerned with using
the permutation approach based on its version of (3) above.

The permutation approach is resistant to impacts of violations of the
normality assumption. It is not resistant to impact of violations of any
of the other assumptions. In fact, it can be quite sensitive to unequal
variances as it will detect differences in the variances of the groups
instead of differences in the means. Its scope of inference is the same
as the parametric approach and can lead to similarly inaccurate
conclusions in the presence of non-independent observations as for the
parametric approach. In this example, we discover that parametric and
permutation approaches provide very similar inferences.

\section{Second example of permutation tests}\label{section2-7}

In every chapter, we will follow the first example used to motivate and
explain the methods with a ``worked'' example where we focus just on the
results. In a previous semester, some of the STAT 217 students (
\textbf{\emph{n}}=79) provided information on their \emph{Sex},
\emph{Age}, and current cumulative \emph{GPA}. We might be interested in
whether Males and Females had different average GPAs. First, we can take
a look at the difference in the responses by groups based on the output
and as displayed in Figure \ref{fig:Figure2-15}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{s217 <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"http://www.math.montana.edu/courses/s217/documents/s217.csv"}\NormalTok{)}
\KeywordTok{require}\NormalTok{(mosaic)}
\KeywordTok{require}\NormalTok{(beanplot)}
\KeywordTok{mean}\NormalTok{(GPA}\OperatorTok{~}\NormalTok{Sex, }\DataTypeTok{data=}\NormalTok{s217)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        F        M 
## 3.338378 3.088571
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{favstats}\NormalTok{(GPA}\OperatorTok{~}\NormalTok{Sex, }\DataTypeTok{data=}\NormalTok{s217)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Sex  min  Q1 median   Q3 max     mean        sd  n missing
## 1   F 2.50 3.1  3.400 3.70   4 3.338378 0.4074549 37       0
## 2   M 1.96 2.8  3.175 3.46   4 3.088571 0.4151789 42       0
\end{verbatim}

=======
## 
## Call:
## lm(formula = responses ~ brand + drops, data = pt)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.4561 -0.4587  0.1297  0.4434  0.9695 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)
## (Intercept)   1.8454     0.2425   7.611 4.45e-08
## brandB2       0.7600     0.2425   3.134  0.00424
## drops20      -0.4680     0.2970  -1.576  0.12715
## drops30      -0.9853     0.2970  -3.318  0.00269
## 
## Residual standard error: 0.664 on 26 degrees of freedom
## Multiple R-squared:  0.445,  Adjusted R-squared:  0.3809 
## F-statistic: 6.948 on 3 and 26 DF,  p-value: 0.001381
\end{verbatim}

In the model summary, the baseline combination estimated in the
\texttt{(Intercept)} row is for \texttt{Brand} \emph{B1} and
\texttt{Drops} 10 and estimates the mean failure time as 1.85 seconds
for this combination. As before, the group labels that do not show up
are the baseline but there are two variables' baselines to identify. Now
the ``simple'' aspects of the additive model show up. The interpretation
of the \texttt{Brands} \emph{B2} coefficient is as a deviation from the
baseline but it applies regardless of the level of \texttt{Drops}. Any
difference between \emph{B1} and \emph{B2} involves a shift up of 0.76
seconds in the estimated mean failure time. Similarly, going from 10
(baseline) to 20 drops results in a drop in the estimated failure mean
of 0.47 seconds and going from 10 to 30 drops results in a drop of
almost 1 second in the average time to failure, both estimated changes
are the same regardless of the brand of paper towel being considered.
Sometimes, especially in observational studies, we use the terminology
``controlled for'' to remind the reader that the other variable was
present in the model\footnote{In Multiple Linear Regression models in
  Chapter \ref{chapter8}, the reasons for this wording will (hopefully)
  become clearer.} and also explained some of the variability in the
responses. The term-plots for the additive model (Figure
\ref{fig:Figure4-7}) help us visualize the impacts of changes brand and
changing water levels, holding the other variable constant. The
differences in heights in each panel correspond to the coefficients just
discussed.





\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{require}\NormalTok{(effects)}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{allEffects}\NormalTok{(m2))}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{04-twoWayAnova_files/figure-latex/Figure4-7-1.pdf}
\caption{\label{fig:Figure4-7}Term-plots of additive model for paper towel data. Left
panel displays results for two brands and right panel for number of
drops of water, each after controlling for the other.}
\end{figure}

\newpage

\section{Guinea pig tooth growth analysis with Two-Way
ANOVA}\label{section4-4}

The effects of dosage and delivery method of ascorbic acid on Guinea Pig
odontoblast growth was analyzed as a One-Way ANOVA in Section
\ref{section3-4} by assessing evidence of any difference in the means of
any combinations of dosage method (Vit C capsule vs Orange Juice) and
three dosage amounts (0.5, 1, and 2 mg/day). Now we will consider the
dosage and delivery methods as two separate variables and explore their
potential interaction. A beanplot and interaction plot are provided in
Figure \ref{fig:Figure4-8}.
>>>>>>> origin/chapter0to1_edits



\begin{figure}
\centering
<<<<<<< HEAD
\includegraphics{02-reintroductionToStatistics_files/figure-latex/Figure2-15-1.pdf}
\caption{\label{fig:Figure2-15}Side-by-side boxplot and beanplot of GPAs of STAT 217
students by sex.}
=======
\includegraphics{04-twoWayAnova_files/figure-latex/Figure4-8-1.pdf}
\caption{\label{fig:Figure4-8}Beanplot and interaction plot of the tooth growth data set.}
>>>>>>> origin/chapter0to1_edits
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
<<<<<<< HEAD
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\KeywordTok{boxplot}\NormalTok{(GPA}\OperatorTok{~}\NormalTok{Sex, }\DataTypeTok{data=}\NormalTok{s217)}
\KeywordTok{beanplot}\NormalTok{(GPA}\OperatorTok{~}\NormalTok{Sex, }\DataTypeTok{data=}\NormalTok{s217, }\DataTypeTok{log=}\StringTok{""}\NormalTok{, }\DataTypeTok{col=}\StringTok{"lightblue"}\NormalTok{, }\DataTypeTok{method=}\StringTok{"jitter"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In these data, the distributions of the GPAs look to be left skewed but
maybe not as dramatically as the responses were right-skewed in the
previous example. The Female GPAs look to be slightly higher than for
Males (0.25 GPA difference in the means) but is that a ``real''
difference? We need our inference tools to more fully assess these
differences.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{diffmean}\NormalTok{(GPA}\OperatorTok{~}\NormalTok{Sex, }\DataTypeTok{data=}\NormalTok{s217)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   diffmean 
## -0.2498069
\end{verbatim}

First, we can try the parametric approach:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{t.test}\NormalTok{(GPA}\OperatorTok{~}\NormalTok{Sex, }\DataTypeTok{data=}\NormalTok{s217, }\DataTypeTok{var.equal=}\NormalTok{T)}
=======
\KeywordTok{data}\NormalTok{(ToothGrowth)}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\KeywordTok{beanplot}\NormalTok{(len }\OperatorTok{~}\StringTok{ }\NormalTok{supp}\OperatorTok{*}\NormalTok{dose, }\DataTypeTok{data=}\NormalTok{ToothGrowth, }\DataTypeTok{side=}\StringTok{"b"}\NormalTok{, }\DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{5}\NormalTok{,}\DecValTok{40}\NormalTok{),}
         \DataTypeTok{main=}\StringTok{"Beanplot"}\NormalTok{, }\DataTypeTok{col=}\KeywordTok{list}\NormalTok{(}\StringTok{"white"}\NormalTok{,}\StringTok{"orange"}\NormalTok{), }\DataTypeTok{xlab=}\StringTok{"Dosage"}\NormalTok{,}
         \DataTypeTok{ylab=}\StringTok{"Tooth Growth"}\NormalTok{)}
\KeywordTok{legend}\NormalTok{(}\StringTok{"bottomright"}\NormalTok{, }\DataTypeTok{bty=}\StringTok{"n"}\NormalTok{, }\KeywordTok{c}\NormalTok{(}\StringTok{"VC"}\NormalTok{,}\StringTok{"OJ"}\NormalTok{), }\DataTypeTok{fill=}\KeywordTok{c}\NormalTok{(}\StringTok{"white"}\NormalTok{,}\StringTok{"orange"}\NormalTok{))}
\KeywordTok{intplot}\NormalTok{(len }\OperatorTok{~}\StringTok{ }\NormalTok{supp}\OperatorTok{*}\NormalTok{dose, }\DataTypeTok{data=}\NormalTok{ToothGrowth, }\DataTypeTok{col=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{), }
        \DataTypeTok{main=}\StringTok{"Interaction Plot"}\NormalTok{, }\DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{5}\NormalTok{,}\DecValTok{40}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

It appears that the effect of method changes based on the dosage as the
interaction plot seems to show some evidence of non-parallel lines.
Actually, it appears that the effect of delivery method is parallel for
doses 0.5 and 1.0 mg/day but that the effect of delivery method changes
for 2 mg/day.

We can use the ANOVA \(F\)-test for an interaction to assess whether the
interaction is ``real'' relative to the variability in the responses.
That is, is it larger than we would expect due to natural variation in
the data? If yes, then it is a real effect and we should account for it.
The following results fit the interaction model and provide an ANOVA
table.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{TG1 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(len}\OperatorTok{~}\NormalTok{supp}\OperatorTok{*}\NormalTok{dose,}\DataTypeTok{data=}\NormalTok{ToothGrowth)}
\KeywordTok{Anova}\NormalTok{(TG1)}
>>>>>>> origin/chapter0to1_edits
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
<<<<<<< HEAD
## 
##  Two Sample t-test
## 
## data:  GPA by Sex
## t = 2.6919, df = 77, p-value = 0.008713
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  0.06501838 0.43459552
## sample estimates:
## mean in group F mean in group M 
##        3.338378        3.088571
\end{verbatim}

So the test statistic was observed to be \(t=2.69\) and it hopefully
follows a \(t(77)\) distribution under the null hypothesis. This
provides a p-value of 0.008713 that we can trust if all the conditions
to use this procedure are met. Compare these results to the permutation
approach, which relaxes that normality assumption, with the results that
follow. In the permutation test, \(T=2.692\) and the p-value is 0.005
which is a little smaller than the result provided by the parametric
approach. The agreement of the two approaches, again, provides some
re-assurance about the use of either approach.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Tobs <-}\StringTok{ }\KeywordTok{t.test}\NormalTok{(GPA}\OperatorTok{~}\NormalTok{Sex, }\DataTypeTok{data=}\NormalTok{s217, }\DataTypeTok{var.equal=}\NormalTok{T)}\OperatorTok{$}\NormalTok{statistic}
\NormalTok{Tstar <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\OtherTok{NA}\NormalTok{, }\DataTypeTok{nrow=}\NormalTok{B)}
\ControlFlowTok{for}\NormalTok{ (b }\ControlFlowTok{in}\NormalTok{ (}\DecValTok{1}\OperatorTok{:}\NormalTok{B))\{}
\NormalTok{  Tstar[b] <-}\StringTok{ }\KeywordTok{t.test}\NormalTok{(GPA}\OperatorTok{~}\KeywordTok{shuffle}\NormalTok{(Sex), }\DataTypeTok{data=}\NormalTok{s217, }\DataTypeTok{var.equal=}\NormalTok{T)}\OperatorTok{$}\NormalTok{statistic}
\NormalTok{\}}
\KeywordTok{pdata}\NormalTok{(}\KeywordTok{abs}\NormalTok{(Tstar),}\KeywordTok{abs}\NormalTok{(Tobs),}\DataTypeTok{lower.tail=}\NormalTok{F)}
\end{Highlighting}
\end{Shaded}




\begin{figure}
\centering
\includegraphics{02-reintroductionToStatistics_files/figure-latex/Figure2-16-1.pdf}
\caption{\label{fig:Figure2-16}Histogram and density curve of permutation distribution of
test statistic for STAT 217 GPAs.}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\KeywordTok{hist}\NormalTok{(Tstar,}\DataTypeTok{labels=}\NormalTok{T)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}\OperatorTok{*}\NormalTok{Tobs, }\DataTypeTok{lwd=}\DecValTok{2}\NormalTok{, }\DataTypeTok{col=}\StringTok{"red"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{density}\NormalTok{(Tstar), }\DataTypeTok{main=}\StringTok{"Density curve of Tstar"}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}\OperatorTok{*}\NormalTok{Tobs, }\DataTypeTok{lwd=}\DecValTok{2}\NormalTok{, }\DataTypeTok{col=}\StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Here is a full write-up of the results using all 6+ hypothesis testing
steps, using the permutation results:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{-1}
\item
  \emph{Isolate the claim to be proved and method to use (define a test
  statistic T)} We want to test for a difference in the means between
  males and females and will use the equal-variance two-sample t-test
  statistic to compare them, making a decision at the 5\% significance
  level.
\item
  Write the null and alternative hypotheses

  \begin{itemize}
  \item
    \(H_0: \mu_{male} = \mu_{female}\)

    \begin{itemize}
    \tightlist
    \item
      where \(\mu_{male}\) is the true mean GPA for males and
      \(\mu_{female}\) is true mean GPA for females.
    \end{itemize}
  \item
    \(H_A: \mu_{male} \ne \mu_{female}\)
  \end{itemize}
\item
  Check conditions for the procedure being used

  \begin{itemize}
  \item
    \textbf{Independent observations condition}: It appears that this
    assumption is met because there is no reason to assume any
    clustering or grouping of responses that might create dependence in
    the observations. The only possible consideration is that the
    observations were taken from different sections and there could be
    some differences between the sections. However, for overall GPA this
    not likely to be a big issue. The only way this could create a
    violation here is if certain sections tended to attract students
    with different GPA levels (such as the 9 am section had the
    best/worst GPA students\ldots{}).
  \item
    \textbf{Equal variance condition} : There is a small difference in
    the range of the observations in the two groups but the standard
    deviations are very similar so there is no evidence that this
    condition is violated.
  \item
    \textbf{Similar distribution condition}: Based on the side-by-side
    boxplots and beanplots, it appears that both groups have slightly
    left-skewed distributions, which could be problematic for the
    parametric approach, but the permutation approach condition is not
    violated since the distributions look to have fairly similar shapes.
  \end{itemize}
\item
  Find the value of the appropriate test statistic
=======
## Anova Table (Type II tests)
## 
## Response: len
##            Sum Sq Df  F value    Pr(>F)
## supp       205.35  1  12.3170 0.0008936
## dose      2224.30  1 133.4151 < 2.2e-16
## supp:dose   88.92  1   5.3335 0.0246314
## Residuals  933.63 56
\end{verbatim}

The R output is reporting an interaction test result of \(F(1,56)=5.3\)
with a p-value of 0.025. But this should raise a red flag since the
numerator degrees of freedom are not what we should expect of
\((K-1)*(J-1) = (2-1)*(3-1)=2\). This brings up an issue in R when
working with categorical variables. If the levels of a categorical
variable are entered numerically, R will treat them as quantitative
variables and not split out the different levels of the categorical
variable. To make sure that R treats categorical variables the correct
way, we should use the \texttt{factor} function on any variables that
are categorical but are coded numerically in the data set. The following
code creates a new variable called \texttt{dosef} using the function
that will help us obtain correct results from the linear model. The
re-run of the ANOVA table provides the correct analysis and the expected
\(df\) for the two rows of output involving \texttt{dosef}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ToothGrowth}\OperatorTok{$}\NormalTok{dosef<-}\KeywordTok{factor}\NormalTok{(ToothGrowth}\OperatorTok{$}\NormalTok{dose)}
\NormalTok{TG2 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(len }\OperatorTok{~}\StringTok{ }\NormalTok{supp}\OperatorTok{*}\NormalTok{dosef, }\DataTypeTok{data=}\NormalTok{ToothGrowth)}
\KeywordTok{Anova}\NormalTok{(TG2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Anova Table (Type II tests)
## 
## Response: len
##             Sum Sq Df F value    Pr(>F)
## supp        205.35  1  15.572 0.0002312
## dosef      2426.43  2  92.000 < 2.2e-16
## supp:dosef  108.32  2   4.107 0.0218603
## Residuals   712.11 54
\end{verbatim}

The ANOVA \(F\)-test for an interaction between supplement type and
dosage level is \(F(2,54)= 4.107\) with a p-value of 0.022. So there
appears to be enough evidence to reject the null hypothesis of no
interaction between \emph{Dosage} and \emph{Delivery method}, supporting
a changing effect on tooth growth of dosage based on the delivery method
in the Guinea Pigs that were assigned.

Any similarities between this correct result and the previous WRONG
result are coincidence. I (Greenwood) once attended a Master's defense
where the results from a similar model were not as expected (small
p-values in places they didn't expect and large p-values in places where
they thought differences existed). During the presentation, the student
showed some ANOVA tables and the four level categorical variable had 1
numerator \(df\) in the ANOVA table. The student passed with major
revisions but had to re-run \textbf{all} the results and re-write
\textbf{all} of the conclusions\ldots{} So be careful to check the ANOVA
results (\(df\) and for the right number of expected model coefficients)
to make sure they match your expectations. This is one reason why you
will be learning to fill in ANOVA tables based on information about the
study so that you can be prepared to detect when your code has let you
down\footnote{Just so you don't think that perfect R code should occur
  on the first try, we have all made similarly serious coding mistakes
  even after accumulating more than decade of experience with R. It is
  finding those mistakes that matters.}. It is also a great reason to
explore term-plots and coefficient interpretations as that can also help
diagnose errors in model construction.

Getting back to the previous results, we now have enough background
information to more formally write up a focused interpretation of these
results. The 6+ hypothesis testing steps in this situation would be
focused on first identifying that the best analysis here is as a Two-Way
ANOVA situation (these data were analyzed in Chapter \ref{chapter3} as a
One-Way ANOVA but this version is better because it can explore whether
there is an interaction between delivery method and dosage). We will use
a 5\% significance level and start with assessing the evidence for an
interaction. If the interaction had not been dropped, we would have
reported the test for the interaction, re-fit the additive model and
used it to explore the main effect tests and estimates for \emph{Dose}
and \emph{Delivery method}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Hypotheses:}

  \begin{itemize}
  \tightlist
  \item
    \(H_0\): No interaction between \emph{Delivery method} and
    \emph{Dose} on odontoblast growth in population of guinea pigs
  \end{itemize}

  \(\Leftrightarrow\) All \(\omega_{jk}\text{'s}=0\).
>>>>>>> origin/chapter0to1_edits

  \begin{itemize}
  \tightlist
  \item
<<<<<<< HEAD
    \(T=2.69\) from the previous R output.
  \end{itemize}
\item
  Find the p-value

  \begin{itemize}
  \item
    p-value=0.005 from the permutation distribution results.
  \item
    This means that there is about a 0.5\% chance we would observe a
    difference in mean GPA (female-male or male-female) of 0.25 points
    or more if there in fact no difference in true mean GPA between
    females and males in STAT 217 in a particular semester.
  \end{itemize}
\end{enumerate}

\newpage

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\item
  Decision

  \begin{itemize}
  \tightlist
  \item
    Since the p-value is ``small'' (\emph{a priori} 5\% significance
    level selected), we can reject the null hypothesis.
  \end{itemize}
\item
  Conclusion and scope of inference, specific to the problem

  \begin{itemize}
  \item
    There is strong evidence against the null hypothesis of no
    difference in the true mean GPA between males and females for the
    STAT 217 students in this semester and so we conclude that there is
    evidence of a difference in the mean GPAs between males and females
    in STAT 217 students.
  \item
    Because this was not a randomized experiment, we can't say that the
    difference in sex causes the difference in mean GPA and because it
    was not a random sample from a larger population, our inferences
    only pertain the STAT 217 students that responded to the survey in
    that semester.
  \end{itemize}
\end{enumerate}

\section{Confidence intervals and bootstrapping}\label{section2-8}

Randomly shuffling the treatments between the observations is like
randomly sampling the treatments without replacement. In other words, we
randomly sample one observation at a observations. This provides us with
a technique for testing hypotheses because it provides new splits of the
observations into groups that are as interesting as what we observed if
the null hypothesis is assumed true. In most situations, we also want to
estimate parameters of interest and provide \textbf{\emph{confidence
intervals}} for those parameters (an interval where we are \_\_\%
\textbf{\emph{confident}} that the true parameter lies). As before,
there are two options we will consider -- a parametric and a
nonparametric approach. The nonparametric approach will be using what is
called \textbf{\emph{bootstrapping}} and draws its name from ``pull
yourself up by your bootstraps'' where you improve your situation based
on your own efforts. In statistics, we make our situation or inferences
better by re-using the observations we have by assuming that the sample
represents the population. Since each observation represents other
similar observations in the population that we didn't get to measure, if
we \textbf{\emph{sample with replacement}} to generate a new data set of
size \emph{n} from our data set (also of size \emph{n}) it mimics the
process of taking our population of interest. This process also ends up
giving us useful sampling distributions of statistics even when our
standard normality assumption is violated, similar to what we
encountered in the permutation tests. Bootstrapping is especially useful
in situations where we are interested in statistics other than the mean
(say we want a confidence interval for a median or a standard deviation)
or when we consider functions of more than one parameter and don't want
to derive the distribution of the statistic (say the difference in two
medians). In this text, bootstrapping is used to provide more
trustworthy inferences when some of our assumptions (especially
normality) might be violated for our parametric procedure.

To perform bootstrapping, we will use the \texttt{resample} function
from the \texttt{mosaic} package. We can apply this function to a data
set and get a new version of the data set by sampling new observations
\emph{with replacement} from the original one. The new, bootstrapped
version of the data set (called \texttt{MockJury\_BTS} below) contains a
new variable called \texttt{orig.\ id} which is the number of the
subject from the original data set. By summarizing how often each of
these id's occurred in a bootstrapped data set, we can see how the
re-sampling works. The \texttt{table} function will count up how many
times each observation was used in the bootstrap sample, providing a row
with the id followed by a row with the count\footnote{The
  \texttt{as.numeric} function is also used here. It really isn't
  important but makes sure the output of \texttt{table} is sorted by
  observation number by first converting the \emph{orig.id} variable
  into a numeric vector.}. In the first bootstrap sample shown, the 2nd,
7th, and 9th observations were sampled one time each, the 4th
observation was sampled three times, and the 1st, 3rd, 5th, and many
others were not sampled at all. Bootstrap sampling thus picks some
observations multiple times and to do that it has to ignore some
observations.

\newpage

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MockJury_BTS <-}\StringTok{ }\KeywordTok{resample}\NormalTok{(MockJury2)}
\KeywordTok{table}\NormalTok{(}\KeywordTok{as.numeric}\NormalTok{(MockJury_BTS}\OperatorTok{$}\NormalTok{orig.id))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  1  2  3  4  5  6 10 11 12 14 15 17 18 19 20 22 24 26 29 30 32 35 36 37 39 
##  1  2  2  1  3  2  1  2  1  1  3  1  2  1  2  1  2  1  2  2  2  2  1  2  2 
## 40 42 43 44 45 46 47 48 49 55 58 59 60 61 69 70 71 72 74 75 
##  2  1  1  4  2  2  1  2  1  2  1  1  2  2  2  2  2  1  1  1
\end{verbatim}

Like in permutations, one randomization isn't enough. A second bootstrap
sample is also provided to help you get a sense of what it is doing to
generate a data set. It did not select subject 7 but did select 2, 4, 6,
and 8 two times. You can see other variations in the resulting
re-sampling of subjects with the most sampled subject being the chance
of selecting any observation for any slot in the new data set is
\(1/75\) and the expected or mean number of appearances we expect to see
for an observation is the number of tries times the probably of
selection on each so \(75*1/75=1\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MockJury_BTS2 <-}\StringTok{ }\KeywordTok{resample}\NormalTok{(MockJury2)}
\KeywordTok{table}\NormalTok{(}\KeywordTok{as.numeric}\NormalTok{(MockJury_BTS2}\OperatorTok{$}\NormalTok{orig.id))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  1  2  3  5  6  8 11 12 13 14 15 18 19 20 21 23 24 26 27 28 29 31 32 34 36 
##  1  1  1  1  4  1  1  1  1  3  1  1  1  1  3  2  2  1  1  1  2  1  2  1  2 
## 37 38 40 42 46 48 50 51 52 56 58 59 61 62 63 66 67 68 69 72 73 74 75 
##  1  2  1  1  1  2  4  1  1  1  3  2  1  1  1  1  1  1  2  3  1  4  2
\end{verbatim}

We can use the two results to get an idea of distribution of results in
terms of number of times observations might be re-sampled when sampling
with replacement and the variation in those results, as shown in Figure
\ref{fig:Figure2-17}. We could also derive the expected counts for each
number of times of re-sampling when we start with all observations
having an equal chance and sampling with replacement but this isn't
important for using bootstrapping methods.




\begin{figure}
\centering
\includegraphics{02-reintroductionToStatistics_files/figure-latex/Figure2-17-1.pdf}
\caption{\label{fig:Figure2-17}Counts of number of times of observation (or not observed
for times re-sampled of 0) for two bootstrap samples.}
\end{figure}

The main point of this exploration was to see that each run of the
\texttt{resample} function provides a new version of the data set.
Repeating this \(B\) times using another \texttt{for} loop, we will
track our quantity of interest, say \(T\), in all these new ``data
sets'' and call those results \(T^*\). The distribution of the
bootstrapped \(T^*\) statistics will tell us about the range of results
to expect for the statistic and the middle \_\_\% of the \(T^*\)'s
provides a \textbf{\emph{bootstrap confidence interval}}\footnote{There
  are actually many ways to use this information to make a confidence
  interval. We are using the simplest method that is called the
  ``percentile'' method.} for the true parameter -- here the
\emph{difference in the two population means}.

To make this concrete, we can revisit our previous examples, starting
with the \texttt{MockJury2} data created before and our interest in
comparing the mean sentences for the \emph{Average}and
\emph{Unattractive} picture groups. The bootstrapping code is very
similar to the permutation code except that we apply the
\texttt{resample} function to the entire data set as opposed to the
\texttt{shuffle} function being applied to the explanatory variable.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\NormalTok{Tobs <-}\StringTok{ }\KeywordTok{diffmean}\NormalTok{(Years }\OperatorTok{~}\StringTok{ }\NormalTok{Attr, }\DataTypeTok{data=}\NormalTok{MockJury2); Tobs}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## diffmean 
## 1.837127
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{B <-}\StringTok{ }\DecValTok{1000}
\NormalTok{Tstar <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\OtherTok{NA}\NormalTok{,}\DataTypeTok{nrow=}\NormalTok{B)}
\ControlFlowTok{for}\NormalTok{ (b }\ControlFlowTok{in}\NormalTok{ (}\DecValTok{1}\OperatorTok{:}\NormalTok{B))\{}
\NormalTok{  Tstar[b] <-}\StringTok{ }\KeywordTok{diffmean}\NormalTok{(Years }\OperatorTok{~}\StringTok{ }\NormalTok{Attr, }\DataTypeTok{data=}\KeywordTok{resample}\NormalTok{(MockJury2))}
\NormalTok{  \}}
\KeywordTok{favstats}\NormalTok{(Tstar)}
=======
    \(H_A\): Interaction between \emph{Delivery method} and \emph{Dose}
    on odontoblast growth in population of guinea pigs
  \end{itemize}

  \(\Leftrightarrow\) At least one \(\omega_{jk}\ne 0\).
\item
  \textbf{Validity conditions:}

  \begin{itemize}
  \item
    Independence:

    \begin{itemize}
    \tightlist
    \item
      This assumption is presumed to be met because we don't know of a
      reason why the independence of the measurements of tooth growth of
      the guinea pigs as studied might be violated.
    \end{itemize}
  \item
    Constant variance:

    \begin{itemize}
    \item
      To assess this assumption, we can use the diagnostic plots in
      Figure \ref{fig:Figure4-9}.
    \item
      In the Residuals vs Fitted and the Scale-Location plots, the
      differences in variability among the groups (see the different
      x-axis positions for each group's fitted values) is minor, so
      there is not strong evidence of a problem with the equal variance
      assumption.
    \end{itemize}

    \begin{figure}
    \centering
    \includegraphics{04-twoWayAnova_files/figure-latex/Figure4-9-1.pdf}
    \caption{\label{fig:Figure4-9}Diagnostic plots for the interaction model
    for Tooth Growth.}
    \end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(TG2, }\DataTypeTok{pch=}\DecValTok{16}\NormalTok{) }
\end{Highlighting}
\end{Shaded}
  \item
    Normality of residuals:

    \begin{itemize}
    \tightlist
    \item
      The QQ-Plot in Figure \ref{fig:Figure4-9} does not suggest a
      problem with this assumption.
    \end{itemize}
  \end{itemize}
\end{enumerate}

\newpage

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\item
  \textbf{Calculate the test statistic for the Interaction test.}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{TG2 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(len }\OperatorTok{~}\StringTok{ }\NormalTok{supp}\OperatorTok{*}\NormalTok{dosef, }\DataTypeTok{data=}\NormalTok{ToothGrowth)}
\KeywordTok{Anova}\NormalTok{(TG2) }
>>>>>>> origin/chapter0to1_edits
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
<<<<<<< HEAD
##         min       Q1   median       Q3      max     mean        sd    n
##  -0.3627312 1.305773 1.833091 2.385281 4.988756 1.854428 0.8438987 1000
##  missing
##        0
\end{verbatim}





\begin{figure}
\centering
\includegraphics{02-reintroductionToStatistics_files/figure-latex/Figure2-18-1.pdf}
\caption{\label{fig:Figure2-18}Histogram and density curve of bootstrap distributions of
difference in sample mean \texttt{Years} with vertical line for the
observed difference in the means of 1.84 years.}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{hist}\NormalTok{(Tstar, }\DataTypeTok{labels=}\NormalTok{T)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v=}\NormalTok{Tobs, }\DataTypeTok{col=}\StringTok{"red"}\NormalTok{, }\DataTypeTok{lwd=}\DecValTok{2}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{density}\NormalTok{(Tstar), }\DataTypeTok{main=}\StringTok{"Density curve of Tstar"}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v=}\NormalTok{Tobs, }\DataTypeTok{col=}\StringTok{"red"}\NormalTok{, }\DataTypeTok{lwd=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In this situation, the observed difference in the mean sentences is 1.84
years (Unattractive-Average), which is the vertical line in Figure
\ref{fig:Figure2-18}. The bootstrap distribution shows the results for
the difference in the sample means when fake data sets are
re-constructed by sampling from the data set with replacement. The
bootstrap distribution is approximately centered at the observed value
(difference in the sample means) and is relatively symmetric.

The permutation distribution in the same situation (Figure
\ref{fig:Figure2-12}) had a similar shape but was centered at 0.
Permutations create sampling distributions based on assuming the null
hypothesis is true, which is useful for hypothesis testing.
Bootstrapping creates distributions centered at the observed result,
which is the sampling distribution ``under the alternative'' or when no
null hypothesis is assumed; bootstrap distributions are useful for
generating confidence intervals for the true parameter values.

To create a 95\% bootstrap confidence interval for the difference in the
true mean sentences (\(\mu_{Unattr}-\mu_{Avg}\)), select the middle 95\%
of results from the bootstrap distribution. Specifically, find the 2.5th
percentile and the 97.5th percentile (values that put 2.5 and 97.5\% of
the results to the left) in the bootstrap distribution, which leaves
95\% in the middle for the confidence interval. To find percentiles in a
distribution in R, functions are of the form
\texttt{q{[}Name\ of\ distribution{]}}, with the function \texttt{qt}
extracting percentiles from a \(t\)-distribution (examples below). From
the bootstrap results, use the \texttt{qdata} function on the
\texttt{Tstar} results that contain the bootstrap distribution of the
statistic of interest.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{qdata}\NormalTok{(Tstar, }\FloatTok{0.025}\NormalTok{)}
=======
## Anova Table (Type II tests)
## 
## Response: len
##             Sum Sq Df F value    Pr(>F)
## supp        205.35  1  15.572 0.0002312
## dosef      2426.43  2  92.000 < 2.2e-16
## supp:dosef  108.32  2   4.107 0.0218603
## Residuals   712.11 54
\end{verbatim}

  \begin{itemize}
  \tightlist
  \item
    The test statistic is \(F(2,54)=4.107\).
  \end{itemize}
\item
  \textbf{Find the p-value:}

  \begin{itemize}
  \item
    The ANOVA \(F\)-test p-value of 0.0219 for the interaction.
  \item
    To find this p-value directly in R, we can use the \texttt{pf}
    function.
  \end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pf}\NormalTok{(}\FloatTok{4.107}\NormalTok{, }\DataTypeTok{df1=}\DecValTok{2}\NormalTok{, }\DataTypeTok{df2=}\DecValTok{54}\NormalTok{, }\DataTypeTok{lower.tail=}\NormalTok{F)}
>>>>>>> origin/chapter0to1_edits
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
<<<<<<< HEAD
##         p  quantile 
## 0.0250000 0.2414232
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{qdata}\NormalTok{(Tstar, }\FloatTok{0.975}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        p quantile 
## 0.975000 3.521528
\end{verbatim}

These results tell us that the 2.5th percentile of the bootstrap
distribution is at 0.26 years and the 97.5th percentile is at 3.50
years. We can combine these results to provide a 95\% confidence for
\(\mu_{Unattr}-\mu_{Avg}\) that is between 0.26 and 3.50. We can
interpret this as with any confidence interval, that we are 95\%
confident that the difference in the true mean suggested sentences
(Unattractive minus Average group) is between 0.26 and 3.50 years. We
can also obtain both percentiles in one line of code using:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{quantiles <-}\StringTok{ }\KeywordTok{qdata}\NormalTok{(Tstar, }\KeywordTok{c}\NormalTok{(}\FloatTok{0.025}\NormalTok{,}\FloatTok{0.975}\NormalTok{))}
\NormalTok{quantiles}
=======
## [1] 0.0218601
\end{verbatim}
\item
  \textbf{Make a decision:}

  \begin{itemize}
  \tightlist
  \item
    Reject \(H_0\) since the p-value (0.0219) is less than 0.05. With a
    p-value of 0.0219, there is about a 2.19\% chance we would observe
    interaction like we did (or more extreme) if none were truly
    present. This provides strong evidence against the null hypothesis
    of no interaction between delivery method and dosage on odontoblast
    growth so we reject the null hypothesis of no interaction.
  \end{itemize}
\item
  \textbf{Write a conclusion:}

  \begin{itemize}
  \tightlist
  \item
    Therefore, the effects of dosage level (0.5, 1, or 2 mg/day) on
    population average tooth odontoblast growth rates of Guinea pigs are
    changed by the delivery (OJ, Vitamin C) method (and vice versa) and
    we should keep the interaction in the model. With the random
    assignment of levels but not random selection of subjects here, we
    could also write this as: Different dosage levels cause different
    changes in the odontoblast growth based on the delivery method for
    these guinea pigs.
  \end{itemize}
\end{enumerate}

In a Two-Way ANOVA, we need to go a little further to get to the final
interpretations since the models are more complicated. When there is an
interaction present, we should focus on the interaction plot or
term-plot of the interaction model for an interpretation of the form and
pattern of the interaction. If the interaction were unimportant, then
the hypotheses and results should focus on the additive model results,
especially the estimated model coefficients. To see why we don't spend
much time with the estimated model coefficients in an interaction model,
the model summary for this model is provided:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(TG2)}
>>>>>>> origin/chapter0to1_edits
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
<<<<<<< HEAD
##        quantile     p
## 2.5%  0.2414232 0.025
## 97.5% 3.5215278 0.975
\end{verbatim}

Figure \ref{fig:Figure2-19} displays those same percentiles on the
bootstrap distribution residing in \texttt{Tstar}.




\begin{figure}
\centering
\includegraphics{02-reintroductionToStatistics_files/figure-latex/Figure2-19-1.pdf}
\caption{\label{fig:Figure2-19}Histogram and density curve of bootstrap distribution with
95\% bootstrap confidence intervals displayed (vertical lines).}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\KeywordTok{hist}\NormalTok{(Tstar, }\DataTypeTok{labels=}\NormalTok{T)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v=}\NormalTok{quantiles}\OperatorTok{$}\NormalTok{quantile, }\DataTypeTok{col=}\StringTok{"blue"}\NormalTok{, }\DataTypeTok{lwd=}\DecValTok{3}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{density}\NormalTok{(Tstar), }\DataTypeTok{main=}\StringTok{"Density curve of Tstar"}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v=}\NormalTok{quantiles}\OperatorTok{$}\NormalTok{quantile, }\DataTypeTok{col=}\StringTok{"blue"}\NormalTok{, }\DataTypeTok{lwd=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Although confidence intervals can exist without referencing hypotheses,
we can revisit our previous \(H_0: \mu_{Unattr} = \mu_{Avg}\). This null
hypothesis is equivalent to testing
\(H_0: \mu_{Unattr} - \mu_{Avg} = 0\), that the difference in the true
means is equal to 0 years. And the difference in the means was the scale
for our confidence interval, which did not contain 0 years. We will call
0 an interesting \textbf{\emph{reference value}} for the confidence
interval, because here it is the value where the true means are equal to
each other (have a difference of 0 years). In general, if our confidence
interval does not contain 0, then it is saying that 0 is not one of our
likely values for the difference in the true means. This implies that we
should reject a claim that they are equal. This provides the same
inferences for the hypotheses that we considered previously using both a
parametric and permutation approach.

The general summary is that we can use confidence intervals to test
hypotheses by assessing whether the reference value under the null
hypothesis is in the confidence interval (FTR \(H_0\)) or outside the
confidence interval (Reject \(H_0\)). P-values are more informative
about hypotheses but confidence intervals are more informative about the
size of differences, so both offer useful information and, as shown
here, can provide consistent conclusions about hypotheses.

As in the previous situation, we also want to consider the parametric
approach for comparison purposes and to have that method available,
especially to help us understand some methods where we will only
consider parametric inferences in later chapters. The parametric
confidence interval is called the \textbf{\emph{equal variance,
two-sample t confidence interval}} and assumes that the populations
being sampled from are normally distributed and leads to using a
\(t\)-distribution to form the interval. The output from the
\texttt{t.test} function provides the parametric 95\% confidence
interval calculated for you:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{t.test}\NormalTok{(Years }\OperatorTok{~}\StringTok{ }\NormalTok{Attr, }\DataTypeTok{data=}\NormalTok{MockJury2, }\DataTypeTok{var.equal=}\NormalTok{T)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Two Sample t-test
## 
## data:  Years by Attr
## t = -2.1702, df = 73, p-value = 0.03324
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -3.5242237 -0.1500295
## sample estimates:
##      mean in group Average mean in group Unattractive 
##                   3.973684                   5.810811
\end{verbatim}

The \texttt{t.test} function again switched the order of the groups and
provides slightly different end-points than our bootstrap confidence
interval (both are made at the 95\% confidence level though), which was
slightly narrower. Both intervals have the same interpretation, only the
methods for calculating the intervals and the assumptions differ.
Specifically, the bootstrap interval can tolerate different distribution
shapes other than normal and still provide intervals that work
well\footnote{When hypothesis tests ``work well'' they have high power
  to detect differences while having Type I error rates that are close
  to what we choose a priori. When confidence intervals ``work well'',
  they contain the true parameter value in repeated random samples at
  around the selected confidence level}. The other assumptions are all
the same as for the hypothesis test, where we continue to assume that we
have independent observations with equal variances for the two groups.

The formula that \texttt{t.test} is using to calculate the parametric
\textbf{\emph{equal-variance two-sample t confidence interval}} is:

\[\bar{x}_1 - \bar{x}_2 \mp t^*_{df}s_p\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}\]

In this situation, the \emph{df} is again \(n_1+n_2-2\) and
\(s_p = \sqrt{\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2}}\). The
\(t^*_{df}\) is a multiplier that comes from finding the percentile from
the \(t\)-distribution that puts \(C\)\% in the middle of the
distribution with \(C\) being the confidence level. It is important to
note that this \(t^*\) has nothing to do with the previous test
statistic \(t\). It is confusing and many of you will, at some point,
happily take the result from a test statistic calculation and use it for
a multiplier in a \(t\)-based confidence interval. Figure
\ref{fig:Figure2-20} shows the \(t\)-distribution with 73 degrees of
freedom and the cut-offs that put 95\% of the area in the middle.




\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\NormalTok{x<-}\KeywordTok{seq}\NormalTok{(}\DataTypeTok{from=}\OperatorTok{-}\DecValTok{4}\NormalTok{,}\DataTypeTok{to=}\DecValTok{4}\NormalTok{,}\DataTypeTok{length.out=}\DecValTok{200}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(x,}\KeywordTok{dt}\NormalTok{(x,}\DataTypeTok{df=}\DecValTok{73}\NormalTok{),}\DataTypeTok{col=}\StringTok{"red"}\NormalTok{,}\DataTypeTok{lty=}\DecValTok{2}\NormalTok{,}\DataTypeTok{lwd=}\DecValTok{3}\NormalTok{,}\DataTypeTok{type=}\StringTok{"l"}\NormalTok{,}\DataTypeTok{xlab=}\StringTok{"t-values"}\NormalTok{,}\DataTypeTok{ylab=}\StringTok{"Density"}\NormalTok{,}
     \DataTypeTok{main=}\StringTok{"Plot of t(73) distribution"}\NormalTok{ )}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v=}\OperatorTok{-}\FloatTok{2.1702}\NormalTok{,}\DataTypeTok{lwd=}\DecValTok{3}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v=}\FloatTok{2.1702}\NormalTok{,}\DataTypeTok{lwd=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{02-reintroductionToStatistics_files/figure-latex/Figure2-20-1.pdf}
\caption{\label{fig:Figure2-20}Plot of \(t(73)\) with cut-offs for putting 95\% of
distributions in the middle.}
\end{figure}

For 95\% confidence intervals, the multiplier is going to be close to 2
and anything else is a sign of a mistake. We can use R to get the
multipliers for confidence intervals using the \texttt{qt} function in a
similar fashion to how \texttt{qdata} was used in the bootstrap results,
except that this new value must be used in the previous confidence
interval formula. This function produces values for requested
percentiles, so if we want to put 95\% in the middle, we place 2.5\% in
each tail of the distribution and need to request the 97.5th percentile.
Because the \(t\)-distribution is always symmetric around 0, we merely
need to look up the value for the 97.5th percentile and know that the
multiplier for the 2.5th percentile is just \(-t^*\). The \(t^*\)
multiplier to form the confidence interval is 1.993 for a 95\%
confidence interval when the \(df=73\) based on the results from
\texttt{qt}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{qt}\NormalTok{(}\FloatTok{0.975}\NormalTok{, }\DataTypeTok{df=}\DecValTok{73}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1.992997
\end{verbatim}

Note that the 2.5th percentile is just the negative of this value due to
symmetry and the real source of the minus in the minus/plus in the
formula for the confidence interval.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{qt}\NormalTok{(}\FloatTok{0.025}\NormalTok{, }\DataTypeTok{df=}\DecValTok{73}\NormalTok{)}
=======
## 
## Call:
## lm(formula = len ~ supp * dosef, data = ToothGrowth)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
##  -8.20  -2.72  -0.27   2.65   8.27 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(>|t|)
## (Intercept)     13.230      1.148  11.521 3.60e-16
## suppVC          -5.250      1.624  -3.233  0.00209
## dosef1           9.470      1.624   5.831 3.18e-07
## dosef2          12.830      1.624   7.900 1.43e-10
## suppVC:dosef1   -0.680      2.297  -0.296  0.76831
## suppVC:dosef2    5.330      2.297   2.321  0.02411
## 
## Residual standard error: 3.631 on 54 degrees of freedom
## Multiple R-squared:  0.7937, Adjusted R-squared:  0.7746 
## F-statistic: 41.56 on 5 and 54 DF,  p-value: < 2.2e-16
\end{verbatim}

There are two \(\omega_{jk}\text{'s}\) in the results, related to
modifying the estimates for doses of 1 (-0.68) and 2 (5.33) for the
Vitamin C group. If you want to re-construct the fitted values from the
model that are displayed in the Figure \ref{fig:Figure4-10}, you have to
look for any coefficients that are ``turned on'' for a combination of
levels of interest. For example, for the OJ group (solid line in Figure
\ref{fig:Figure4-10}), the dosage of 0.5 mg/day has an estimate of an
average growth of approximately 13 mm. This is the baseline group, so
the model estimate for an observation in the OJ and 0.5 mg/day dosage is
simply \(\hat{y}_{i,\text{OJ},0.5mg}=\hat{\alpha}=13.23\) microns. For
the OJ and 2 mg dosage estimate that has a value over 25 microns in the
plot, the model incorporates the deviation for the 2 mg dosage:
\(\hat{y}_{i,\text{OJ},2mg}=\hat{\alpha} + \hat{\tau}_{2mg}=13.23 + 12.83 = 26.06\)
microns. For the Vitamin C group, another coefficient becomes involved
from its ``main effect''. For the VC and 0.5 mg dosage level, the
estimate is approximately 8 microns. The pertinent model components are
\(\hat{y}_{i,\text{VC},0.5mg}=\hat{\alpha} + \hat{\gamma}_{\text{VC}}=13.23 + (-5.25) = 7.98\)
microns. Finally, when we consider non-baseline results for both groups,
three coefficients are required to reconstruct the results in the plot.
For example, the estimate for the VC, 1 mg dosage is
\(\hat{y}_{i,\text{VC},1mg}=\hat{\alpha} + \hat{\tau}_{1mg} + \hat{\gamma}_{\text{VC}} =13.23 + 9.47 + (-5.25) = 17.45\)
microns. We usually will by-pass all this fun(!) with the coefficients
in an interaction model and go from the ANOVA interaction test to
focusing on the pattern of the responses in the interaction plot, but it
is good to know that there are still model coefficients driving our
results.




\begin{figure}
\centering
\includegraphics{04-twoWayAnova_files/figure-latex/Figure4-10-1.pdf}
\caption{\label{fig:Figure4-10}Term-plot for the estimated interaction for the Tooth
Growth data.}
\end{figure}



\begin{figure}
\centering
\includegraphics{04-twoWayAnova_files/figure-latex/Figure4-11-1.pdf}
\caption{\label{fig:Figure4-11}Interaction plot with added CLD from Tukey's HSD.}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(}\KeywordTok{allEffects}\NormalTok{(TG2), }\DataTypeTok{grid=}\NormalTok{T, }\DataTypeTok{multiline=}\NormalTok{T, }\DataTypeTok{ci.style=}\StringTok{"bars"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Given the presence of an important interaction, then the final step in
the interpretation here is to interpret the results in the interaction
plot or term-plot of the interaction model, supported by the p-value
suggesting evidence of a different effect of supplement type based on
the dosage level. To supplement this even more, knowing which
combinations of levels differ can enhance our discussion. Tukey's HSD
results (specifically the CLD) can be added to the original interaction
plot by turning on the \texttt{cld=T} option in the \texttt{intplot}
function as seen in Figure \ref{fig:Figure4-11}. Sometimes it is hard to
see the letters and so there is also a \texttt{cldshift=...} option to
move the letters up or down, here a value of 1 seemed to work.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{intplot}\NormalTok{(len }\OperatorTok{~}\StringTok{ }\NormalTok{supp}\OperatorTok{*}\NormalTok{dose, }\DataTypeTok{data=}\NormalTok{ToothGrowth, }\DataTypeTok{col=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{), }\DataTypeTok{cldshift=}\DecValTok{1}\NormalTok{,}
        \DataTypeTok{cld=}\NormalTok{T, }\DataTypeTok{main=}\StringTok{"Interaction Plot with CLD"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The interpretation of the previous hypothesis test result can be
concluded with the following discussion. Generally increasing the dosage
increases the amount of mean growth except for the 2 mg/day dosage level
where the increase levels off in the OJ group (OJ 1 and 2 mg/day are not
detectably different) and the differences between the two delivery
methods disappear at the highest dosage level. But for 0.5 and 1 mg/day
dosages, OJ is clearly better than VC by about 10 microns of growth on
average.

\section{Observational study example: The Psychology of
Debt}\label{section4-5}

In this section, the analysis of a survey of \(N=464\) randomly sampled
adults will be analyzed from a survey conducted by \citet{Lea1995} and
available in the \texttt{debt} data set from the \texttt{faraway}
package \citep{R-faraway}. The subjects responded to a variety of
questions including whether they buy cigarettes (\texttt{cigbuy}: 0 if
no, 1 if yes), their housing situation (\texttt{house}: 1 = rent, 2 =
mortgage, and 3 = owned outright), their income group
(\texttt{incomegp}: 1 = lowest, 5 = highest), and their score on a
continuous scale of attitudes about debt (\texttt{prodebt}: 1 = least
favorable, 5 = most favorable). \texttt{prodebt} was derived as the
average of a series of questions about debt with each question measured
on an \textbf{\emph{ordinal}} 1 to 5 scale, with higher values
corresponding to more positive responses about \textbf{going into debt}
of various kinds. The ordered scale on surveys that try to elicit your
opinions on topics with scales from 1 to 5 or 1 to 7 or even, sometimes,
1 to 10 is called a \textbf{\emph{Likert scale}} \citep{Likert1932}. It
is not a quantitative scale and really should be handled more carefully
than taking an average of a set responses. That said, it is extremely
common practice in social science research to treat ordinal responses as
if they are quantitative and take the average of many of them to create
a more continuous response variable like the one we are using here. If
you continue your statistics explorations, you will see some better
techniques for analyzing responses obtained in this fashion. That said,
the scale of the response is relatively easy to understand as an amount
of willingness to go into debt on a scale from 1 to 5 with higher values
corresponding to more willingness to be in debt.

This data set is typical of survey data where respondents were not
required to answer all questions and there are some missing responses.
We will clean out any individuals that failed to respond to all
questions using the \texttt{na.omit} function, which will return only
subjects that responded to every question in the data set. But is this
dangerous? Suppose that people did not want to provide their income
levels if they were in the lowest or, maybe, highest income groups. Then
we would be missing responses systematically and conclusions could be
biased because of ignoring these types of subjects. This is another
topic for more advanced statistical methods to try to handle but
something every researcher should worry about when selected subjects do
not respond at all or fail to answer some questions. Is there bias
because of responses that were not observed that could invalidate all my
hard won statistical conclusions? This ties back into our discussion of
who was sampled. We need to think carefully about who was part of the
sample but refused to participate and how that might impact our
inferences.

Ignoring this potential for bias in the results for the moment, we are
first interested in whether buying cigarettes/not and income groups
interact in their explanation of the respondent's mean opinions on being
in debt. The interaction plot (Figure \ref{fig:Figure4-12}) may suggest
an interaction between \texttt{cigbuy} and \texttt{incomegp} from income
levels 1 to 3 where the lines cross but it is not as clear as the
previous examples. The interaction \(F\)-test helps us objectively
assess evidence for that interaction. Based on the plot, there do not
appear to be differences based on cigarette purchasing but there might
be some differences between the income groups. If there is no
interaction present, then this suggests that we might be in Scenario 2
or 3 where a single main effect of interest is present.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{require}\NormalTok{(faraway)}
\KeywordTok{data}\NormalTok{(debt)}
\NormalTok{debt}\OperatorTok{$}\NormalTok{incomegp <-}\StringTok{ }\KeywordTok{factor}\NormalTok{(debt}\OperatorTok{$}\NormalTok{incomegp)}
\NormalTok{debt}\OperatorTok{$}\NormalTok{cigbuy <-}\StringTok{ }\KeywordTok{factor}\NormalTok{(debt}\OperatorTok{$}\NormalTok{cigbuy)}
\NormalTok{debtc <-}\StringTok{ }\KeywordTok{na.omit}\NormalTok{(debt)}
\end{Highlighting}
\end{Shaded}




\begin{figure}
\centering
\includegraphics{04-twoWayAnova_files/figure-latex/Figure4-12-1.pdf}
\caption{\label{fig:Figure4-12}Interaction plot of \texttt{prodebt} by income group and
buy cigarettes (0=no, 1=yes).}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{intplot}\NormalTok{(prodebt }\OperatorTok{~}\StringTok{ }\NormalTok{cigbuy}\OperatorTok{*}\NormalTok{incomegp, }\DataTypeTok{data=}\NormalTok{debtc, }\DataTypeTok{col=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{), }\DataTypeTok{lwd=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

As in other situations, and especially with observational studies where
a single large sample is analyzed, it is important to check for balance
- whether all the combinations of the two predictor variables are
similarly represented. Even more critically, we need to check whether
all the combinations of levels of factors are measured. If a combination
is not measured, then we lose the ability to estimate the mean for that
combination and the ability to test for an interaction. A solution to
that problem would be to collapse the categories of one of the
variables, changing the definitions of the levels but if you fail to
obtain information for all combinations, you can't work with the
interaction model. In this situation, we barely have enough information
to proceed (the smallest \(n_{jk}\) is 8 for income group 4 that buys
cigarettes). We have a very unbalanced design with counts between 8 and
51 in the different combinations.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tally}\NormalTok{(cigbuy }\OperatorTok{~}\StringTok{ }\NormalTok{incomegp, }\DataTypeTok{data=}\NormalTok{debtc)}
>>>>>>> origin/chapter0to1_edits
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
<<<<<<< HEAD
## [1] -1.992997
\end{verbatim}

We can also re-write the confidence interval formula into a slightly
more general form as

\[\bar{x}_1 - \bar{x}_2 \mp t^*_{df}SE_{\bar{x}_1 - \bar{x}_2}\ \text{ OR }\ 
\bar{x}_1 - \bar{x}_2 \mp ME\]

where
\(SE_{\bar{x}_1 - \bar{x}_2} = s_p\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}\)
and \(ME = t^*_{df}SE_{\bar{x}_1 - \bar{x}_2}\). In some situations,
researchers will report the \textbf{\emph{standard error}} (SE) or
\textbf{\emph{margin of error}} (ME) as a method of quantifying the
uncertainty in a statistic. The SE is an estimate of the standard
deviation of the statistic (here \(\bar{x}_1 - \bar{x}_2\)) and the ME
is an estimate of the precision of a statistic that can be used to
directly form a confidence interval. The ME depends on the choice of
confidence level although 95\% is almost always selected.

To finish this example, R can be used to help you do calculations much
like a calculator except with much more power ``under the hood''. You
have to make sure you are careful with using \texttt{(\ )} to group
items and remember that the asterisk (*) is used for multiplication in
R. We need the pertinent information which is available from the
\texttt{favstats} output repeated below to calculate the confidence
interval ``by hand'' using R.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{favstats}\NormalTok{(Years}\OperatorTok{~}\NormalTok{Attr, }\DataTypeTok{data=}\NormalTok{MockJury2)}
=======
##       incomegp
## cigbuy  1  2  3  4  5
##      0 24 40 45 47 51
##      1 23 29 18  8 19
\end{verbatim}

The test for the interaction is always how we start our modeling in
Two-Way ANOVA situations. The ANOVA table suggests that there is little
evidence of interaction between the income level and buying cigarettes
on the opinions of the respondents towards debt (\(F(4,294)=1.0003\),
p-value=0.408). This suggests that the initial assessment that the
interaction wasn't too prominent was correct. We should move to the
additive model here but first need to check the assumptions to make sure
we can trust this initial test.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{require}\NormalTok{(car)}
\NormalTok{debt1 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(prodebt }\OperatorTok{~}\StringTok{ }\NormalTok{incomegp}\OperatorTok{*}\NormalTok{cigbuy, }\DataTypeTok{data=}\NormalTok{debtc)}
\KeywordTok{Anova}\NormalTok{(debt1)}
>>>>>>> origin/chapter0to1_edits
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
<<<<<<< HEAD
##           Attr min Q1 median Q3 max     mean       sd  n missing
## 1      Average   1  2      3  5  12 3.973684 2.823519 38       0
## 2 Unattractive   1  2      5 10  15 5.810811 4.364235 37       0
\end{verbatim}

Start with typing the following command to calculate \(s_p\) and store
it in a variable named \texttt{sp}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sp <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(((}\DecValTok{38}\OperatorTok{-}\DecValTok{1}\NormalTok{)}\OperatorTok{*}\NormalTok{(}\FloatTok{2.8235}\OperatorTok{^}\DecValTok{2}\NormalTok{)}\OperatorTok{+}\NormalTok{(}\DecValTok{37}\OperatorTok{-}\DecValTok{1}\NormalTok{)}\OperatorTok{*}\NormalTok{(}\FloatTok{4.364}\OperatorTok{^}\DecValTok{2}\NormalTok{))}\OperatorTok{/}\NormalTok{(}\DecValTok{38}\OperatorTok{+}\DecValTok{37}\OperatorTok{-}\DecValTok{2}\NormalTok{))}
\NormalTok{sp}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3.665036
\end{verbatim}

Then calculate the confidence interval that \texttt{t.test} provided
using:

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{3.974}\OperatorTok{-}\FloatTok{5.811}\OperatorTok{+}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}\OperatorTok{*}\KeywordTok{qt}\NormalTok{(.}\DecValTok{975}\NormalTok{,}\DataTypeTok{df=}\DecValTok{73}\NormalTok{)}\OperatorTok{*}\NormalTok{sp}\OperatorTok{*}\KeywordTok{sqrt}\NormalTok{(}\DecValTok{1}\OperatorTok{/}\DecValTok{38}\OperatorTok{+}\DecValTok{1}\OperatorTok{/}\DecValTok{37}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -3.5240302 -0.1499698
\end{verbatim}

The previous code uses \texttt{c(-1,\ 1)} times the margin of error to
subtract and add the ME to the difference in the sample means
(\(3.974-5.811\)), which generates the lower and then upper bounds of
the confidence interval. If desired, we can also use just the last
portion of the previous calculation to find the margin of error, which
is 1.69 here.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{qt}\NormalTok{(.}\DecValTok{975}\NormalTok{,}\DataTypeTok{df=}\DecValTok{73}\NormalTok{)}\OperatorTok{*}\NormalTok{sp}\OperatorTok{*}\KeywordTok{sqrt}\NormalTok{(}\DecValTok{1}\OperatorTok{/}\DecValTok{38}\OperatorTok{+}\DecValTok{1}\OperatorTok{/}\DecValTok{37}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1.68703
\end{verbatim}

\section{Bootstrap confidence intervals for difference in
GPAs}\label{section2-9}

We can now apply the new confidence interval methods on the STAT 217
grade data. This time we start with the parametric 95\% confidence
interval ``by hand'' in R and then use \texttt{t.test} to verify our
result. The \texttt{favstats} output provides us with the required
information to calculate the confidence interval:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{favstats}\NormalTok{(GPA}\OperatorTok{~}\NormalTok{Sex,}\DataTypeTok{data=}\NormalTok{s217)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Sex  min  Q1 median   Q3 max     mean        sd  n missing
## 1   F 2.50 3.1  3.400 3.70   4 3.338378 0.4074549 37       0
## 2   M 1.96 2.8  3.175 3.46   4 3.088571 0.4151789 42       0
\end{verbatim}

The \(df\) are \(37+42-2 = 77\). Using the SDs from the two groups and
their sample sizes, we can calculate \(s_p\):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sp <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(((}\DecValTok{37}\OperatorTok{-}\DecValTok{1}\NormalTok{)}\OperatorTok{*}\NormalTok{(}\FloatTok{0.4075}\OperatorTok{^}\DecValTok{2}\NormalTok{)}\OperatorTok{+}\NormalTok{(}\DecValTok{42}\OperatorTok{-}\DecValTok{1}\NormalTok{)}\OperatorTok{*}\NormalTok{(}\FloatTok{0.41518}\OperatorTok{^}\DecValTok{2}\NormalTok{))}\OperatorTok{/}\NormalTok{(}\DecValTok{37}\OperatorTok{+}\DecValTok{42}\OperatorTok{-}\DecValTok{2}\NormalTok{))}
\NormalTok{sp}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.4116072
\end{verbatim}

The margin of error is:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{qt}\NormalTok{(.}\DecValTok{975}\NormalTok{,}\DataTypeTok{df=}\DecValTok{77}\NormalTok{)}\OperatorTok{*}\NormalTok{sp}\OperatorTok{*}\KeywordTok{sqrt}\NormalTok{(}\DecValTok{1}\OperatorTok{/}\DecValTok{37}\OperatorTok{+}\DecValTok{1}\OperatorTok{/}\DecValTok{42}\NormalTok{)}
=======
## Anova Table (Type II tests)
## 
## Response: prodebt
##                  Sum Sq  Df F value   Pr(>F)
## incomegp          9.018   4  4.5766 0.001339
## cigbuy            0.703   1  1.4270 0.233222
## incomegp:cigbuy   1.971   4  1.0003 0.407656
## Residuals       144.835 294
\end{verbatim}

The diagnostic plots (Figure \ref{fig:Figure4-13}) seem to be pretty
well-behaved with no apparent violations of the normality assumption and
no clear evidence of a violation of the constant variance assumption.
The observations would seem to be independent because there is no
indication of structure to the measurements of the survey respondents
that might create dependencies. In observational studies, violations of
the independence assumption might come from repeated measures of the
same person or multiple measurements within the same family/household or
samples that are clustered geographically. The random sampling from a
population should allow inferences to a larger population except for
that issue of removing partially missing responses. We also don't have
much information on the population sampled, so will just leave this
vague here but know that there is a population these conclusions apply
to since it was random sample. All of this suggests proceeding to
fitting and exploring the additive model is reasonable here. No causal
inferences are possible because this is an observational study.




\begin{figure}
\centering
\includegraphics{04-twoWayAnova_files/figure-latex/Figure4-13-1.pdf}
\caption{\label{fig:Figure4-13}Diagnostic plot for \texttt{prodebt} by income group and
buy cigarettes/not interaction model.}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(debt1)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Hypotheses (Two sets apply when the additive model is the
  focus!):}

  \begin{itemize}
  \tightlist
  \item
    \(H_0\): No difference in means for \texttt{prodebt} for income
    groups in population, given cigarette buying in model
  \end{itemize}

  \(\Leftrightarrow\) All \(\tau_j\text{'s} = 0\) in additive model.

  \begin{itemize}
  \tightlist
  \item
    \(H_A\): Some difference in means for \texttt{prodebt} for income
    group in population, given cigarette buying in model
  \end{itemize}

  \(\Leftrightarrow\) Not all \(\tau_j\text{'s} = 0\) in additive model.

  \begin{itemize}
  \tightlist
  \item
    \(H_0\): No difference in means for \texttt{prodebt} for cigarette
    buying/not in population, given income group in model 
  \end{itemize}

  \(\Leftrightarrow\) All \(\gamma_k\text{'s} = 0\) in additive model.

  \begin{itemize}
  \tightlist
  \item
    \(H_A\): Some difference in means for \texttt{prodebt} for cigarette
    buying/not in population, given income group in model
  \end{itemize}

  \(\Leftrightarrow\) Not all \(\gamma_k\text{'s} = 0\) in additive
  model.
\item
  \textbf{Validity conditions -- discussed above but with new plots for
  the additive:}

  \begin{figure}
  \centering
  \includegraphics{04-twoWayAnova_files/figure-latex/Figure4-14-1.pdf}
  \caption{\label{fig:Figure4-14}Diagnostic plot for \texttt{prodebt} by
  income group and buy cigarettes/not}
  \end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{debt1r<-}\KeywordTok{lm}\NormalTok{(prodebt}\OperatorTok{~}\NormalTok{incomegp}\OperatorTok{+}\NormalTok{cigbuy,}\DataTypeTok{data=}\NormalTok{debtc)}
\KeywordTok{plot}\NormalTok{(debt1r)}
\end{Highlighting}
\end{Shaded}

  \begin{itemize}
  \item
    Constant Variance:

    \begin{itemize}
    \tightlist
    \item
      In the Residuals vs Fitted and the Scale-Location plots in Figure
      \ref{fig:Figure4-14}, the differences in variability among groups
      is minor and nothing suggests a violation. \textbf{If you change
      models, you should always revisit the diagnostic plots to make
      sure you didn't create problems that were not present in more
      complicated models.}
    \end{itemize}
  \item
    Normality of residuals:

    \begin{itemize}
    \tightlist
    \item
      The QQ-Plot in Figure \ref{fig:Figure4-14} does not suggest a
      problem with this assumption.
    \end{itemize}
  \end{itemize}
\item
  \textbf{Calculate the test statistic for the two main effect tests.}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{Anova}\NormalTok{(debt1r)}
>>>>>>> origin/chapter0to1_edits
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
<<<<<<< HEAD
## [1] 0.1847982
\end{verbatim}

All together, the 95\% confidence interval is:

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{3.338}\OperatorTok{-}\FloatTok{3.0886}\OperatorTok{+}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}\OperatorTok{*}\KeywordTok{qt}\NormalTok{(.}\DecValTok{975}\NormalTok{,}\DataTypeTok{df=}\DecValTok{77}\NormalTok{)}\OperatorTok{*}\NormalTok{sp}\OperatorTok{*}\KeywordTok{sqrt}\NormalTok{(}\DecValTok{1}\OperatorTok{/}\DecValTok{37}\OperatorTok{+}\DecValTok{1}\OperatorTok{/}\DecValTok{42}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.0646018 0.4341982
\end{verbatim}

So we are 95\% confident that the difference in the true mean GPAs
between females and males (females minus males) is between 0. 065 and 0.
434 GPA points. We get a similar\footnote{We rounded the means a little
  and that caused the small difference in results.} result from the
\texttt{t.test} output:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{t.test}\NormalTok{(GPA}\OperatorTok{~}\NormalTok{Sex,}\DataTypeTok{data=}\NormalTok{s217,}\DataTypeTok{var.equal=}\NormalTok{T)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Two Sample t-test
## 
## data:  GPA by Sex
## t = 2.6919, df = 77, p-value = 0.008713
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  0.06501838 0.43459552
## sample estimates:
## mean in group F mean in group M 
##        3.338378        3.088571
\end{verbatim}

Note that we can easily switch to 90\% or 99\% confidence intervals by
simply changing the percentile in \texttt{qt} or changing
\texttt{conf.\ level} in the \texttt{t.test} function. In the following
two lines of code, we added octothorpes\footnote{You can correctly call
  octothorpes \emph{number} symbols or, in the twitter verse,
  \emph{hashtags}. For more on this symbol, see
  ``\url{http://blog.dictionary.com/octothorpe/}''. I usually call them
  number symbols too.}(\#) and then some text after function calls to
explain what is being calculated. In computer code, octothorpes provide
a way of adding comments that tell the software (here R) to ignore any
text after a ``\#'' on a given line. In the color version of the text,
comments are also clearly distinguished.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{qt}\NormalTok{(.}\DecValTok{95}\NormalTok{,}\DataTypeTok{df=}\DecValTok{77}\NormalTok{) }\CommentTok{# For 90% confidence and 77 df}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1.664885
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{qt}\NormalTok{(.}\DecValTok{995}\NormalTok{,}\DataTypeTok{df=}\DecValTok{77}\NormalTok{) }\CommentTok{#For 99% confidence and 77 df}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2.641198
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{t.test}\NormalTok{(GPA}\OperatorTok{~}\NormalTok{Sex,}\DataTypeTok{data=}\NormalTok{s217,}\DataTypeTok{var.equal=}\NormalTok{T,}\DataTypeTok{conf.level=}\FloatTok{0.90}\NormalTok{)}
=======
## Anova Table (Type II tests)
## 
## Response: prodebt
##            Sum Sq  Df F value   Pr(>F)
## incomegp    9.018   4  4.5766 0.001335
## cigbuy      0.703   1  1.4270 0.233210
## Residuals 146.806 298
\end{verbatim}

  \begin{itemize}
  \tightlist
  \item
    The test statistics are \(F(4,298)=4.577\) and \(F(1,298)=1.427\).
  \end{itemize}
\item
  \textbf{Find the p-value:}

  \begin{itemize}
  \tightlist
  \item
    The ANOVA \(F\)-test p-values are 0.001335 for the income group
    variable (conditional on cigarette buy) and 0.2232 for the cigarette
    buy variable (conditional on income group).
  \end{itemize}
\item
  \textbf{Make decisions:}

  \begin{itemize}
  \tightlist
  \item
    Reject \(H_0\) of no income group differences (p-value=0.0013) and
    fail to reject \(H_0\) of no cigarette buying differences
    (p-value=0.2232), each after controlling for the other variable.
  \end{itemize}
\item
  \textbf{Write a conclusion:}

  \begin{itemize}
  \tightlist
  \item
    There was initially no evidence to support retaining the interaction
    of income group and cigarette buying on pro-debt feelings
    (\(F(4,294)=1.00, \text{p-value} =0.408\)) so the interaction was
    dropped from the model. There is strong evidence of some difference
    in the mean pro-debt feelings in the population across the income
    groups, after adjusting for cigarette buying. There is little to no
    evidence of a difference in the mean pro-debt feelings in the
    population based on cigarette buying/not, after adjusting for income
    group.
  \end{itemize}
\end{enumerate}

So we learned that the additive model was more appropriate for these
responses and that the results resemble Scenario 2 or 3 with only one
main effect being important. In the additive model, the coefficients can
be interpreted as shifts from the baseline after controlling for the
other variable in the model. Figure \ref{fig:Figure4-15} shows the
increasing average comfort with being in debt as the income groups go
up. Being a cigarette buyer was related to a lower comfort level with
debt. But compare the y-axis scales in the two plots -- the differences
in the means across income groups are almost 0.5 points on a 5 point
scale whereas the difference across \texttt{cigbuy}'s two levels is less
than 0.15 units. The error bars for the 95\% confidence intervals are of
similar width but the differences in means show up clearly in the income
group term-plot. This is all indirectly related to the size of the
p-values for each term in the additive model but hopefully helps to
build some intuition on the reason for differences.





\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(}\KeywordTok{allEffects}\NormalTok{(debt1r))}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{04-twoWayAnova_files/figure-latex/Figure4-15-1.pdf}
\caption{\label{fig:Figure4-15}Term-plots for the \texttt{prodebt} additive model with
left panel for income group and the right panel for buying cigarettes or
not (1 for yes).}
\end{figure}

The estimated coefficients can also be interesting to interpret for the
additive model. Here are the model summary coefficients:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(debt1r)}\OperatorTok{$}\NormalTok{coefficients}
>>>>>>> origin/chapter0to1_edits
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
<<<<<<< HEAD
## 
##  Two Sample t-test
## 
## data:  GPA by Sex
## t = 2.6919, df = 77, p-value = 0.008713
## alternative hypothesis: true difference in means is not equal to 0
## 90 percent confidence interval:
##  0.09530553 0.40430837
## sample estimates:
## mean in group F mean in group M 
##        3.338378        3.088571
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{t.test}\NormalTok{(GPA}\OperatorTok{~}\NormalTok{Sex,}\DataTypeTok{data=}\NormalTok{s217,}\DataTypeTok{var.equal=}\NormalTok{T,}\DataTypeTok{conf.level=}\FloatTok{0.99}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Two Sample t-test
## 
## data:  GPA by Sex
## t = 2.6919, df = 77, p-value = 0.008713
## alternative hypothesis: true difference in means is not equal to 0
## 99 percent confidence interval:
##  0.004703598 0.494910301
## sample estimates:
## mean in group F mean in group M 
##        3.338378        3.088571
\end{verbatim}

As a review of some basic ideas with confidence intervals make sure you
can answer the following questions:
=======
##                Estimate Std. Error    t value     Pr(>|t|)
## (Intercept)  3.05483517 0.11127284 27.4535561 1.357428e-83
## incomegp2    0.01640636 0.13288796  0.1234601 9.018260e-01
## incomegp3    0.17477182 0.13649309  1.2804444 2.013846e-01
## incomegp4    0.16900515 0.14274836  1.1839376 2.373813e-01
## incomegp5    0.46833117 0.13377661  3.5008449 5.347171e-04
## cigbuy1     -0.10640228 0.08907259 -1.1945569 2.332101e-01
\end{verbatim}

In the model, the baseline group is for non-cigarette buyers
(\texttt{cigbuy=0}) and income group 1 with \(\hat{\alpha}= 3.055\)
points. Regardless of the \texttt{cigbuy} level, the difference between
income groups 2 and 1 is estimated to be \(\hat{\tau}_2=0.016\), an
increase in the mean score of 0.016 points. Similarly, the difference
between income groups 3 and 1 is \(\hat{\tau}_3=0.175\) points,
regardless of cigarette smoking status. The estimated difference between
cigarette buyers and non-buyers was estimated as
\(\hat{\gamma}_2=-0.106\) points for any income group, remember that
this variable had a moderately large p-value in this model. The additive
model-based estimates for all six combinations can be found in Table
\ref{tab:Table4-3}.




\small

\begin{longtable}[]{@{}llllll@{}}
\caption{\label{tab:Table4-3} Calculations to construct the estimates for all
combinations of variables for the \texttt{prodebt} additive model.}\tabularnewline
\toprule
\begin{minipage}[b]{0.12\columnwidth}\raggedright\strut
\(\color{red}{\text{Cig}}\)\\
\(\color{red}{\text{Buy}}\)\strut
\end{minipage} & \begin{minipage}[b]{0.12\columnwidth}\raggedright\strut
\(\color{blue}{\textbf{Income}}\)\\
\(\color{blue}{\textbf{Group 1}}\)\strut
\end{minipage} & \begin{minipage}[b]{0.15\columnwidth}\raggedright\strut
\(\color{blue}{\textbf{Income}}\)\\
\(\color{blue}{\textbf{Group 2}}\)\strut
\end{minipage} & \begin{minipage}[b]{0.15\columnwidth}\raggedright\strut
\(\color{blue}{\textbf{Income}}\)\\
\(\color{blue}{\textbf{Group 3}}\)\strut
\end{minipage} & \begin{minipage}[b]{0.15\columnwidth}\raggedright\strut
\(\color{blue}{\textbf{Income}}\)\\
\(\color{blue}{\textbf{Group 4}}\)\strut
\end{minipage} & \begin{minipage}[b]{0.15\columnwidth}\raggedright\strut
\(\color{blue}{\textbf{Income}}\)\\
\(\color{blue}{\textbf{Group 5}}\)\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.12\columnwidth}\raggedright\strut
\(\color{red}{\text{Cig}}\)\\
\(\color{red}{\text{Buy}}\)\strut
\end{minipage} & \begin{minipage}[b]{0.12\columnwidth}\raggedright\strut
\(\color{blue}{\textbf{Income}}\)\\
\(\color{blue}{\textbf{Group 1}}\)\strut
\end{minipage} & \begin{minipage}[b]{0.15\columnwidth}\raggedright\strut
\(\color{blue}{\textbf{Income}}\)\\
\(\color{blue}{\textbf{Group 2}}\)\strut
\end{minipage} & \begin{minipage}[b]{0.15\columnwidth}\raggedright\strut
\(\color{blue}{\textbf{Income}}\)\\
\(\color{blue}{\textbf{Group 3}}\)\strut
\end{minipage} & \begin{minipage}[b]{0.15\columnwidth}\raggedright\strut
\(\color{blue}{\textbf{Income}}\)\\
\(\color{blue}{\textbf{Group 4}}\)\strut
\end{minipage} & \begin{minipage}[b]{0.15\columnwidth}\raggedright\strut
\(\color{blue}{\textbf{Income}}\)\\
\(\color{blue}{\textbf{Group 5}}\)\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.12\columnwidth}\raggedright\strut
\(\color{red}{\text{0:No}}\)\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedright\strut
\(\hat{\alpha} ={\small 3.055}\)\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright\strut
\(\hat{\alpha} + \hat{\tau}_2\)\\
\(=3.055 + 0.016\)\\
\(= 3.071\)\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright\strut
\(\hat{\alpha} + \hat{\tau}_3\)\\
\(=3.055 + 0.175\)\\
\(=3.230\)\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright\strut
\(\hat{\alpha} + \hat{\tau}_4\)\\
\(=3.055 + 0.169\)\\
\(=3.224\)\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright\strut
\(\hat{\alpha} + \hat{\tau}_5\)\\
\(=3.055 + 0.468\)\\
\(=3.523\)\\
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.12\columnwidth}\raggedright\strut
\(\color{red}{\text{1:}\text{Yes}}\)\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedright\strut
\(\hat{\alpha}+\hat{\gamma}_2\)\\
\(=3.055\)\\
\(-0.106\)\\
\(=2.949\)\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright\strut
\(\hat{\alpha}+\hat{\tau}_2+\hat{\gamma}_2\)\\
\(=3.055+0.016\)\\
\(-0.106\)\\
\(=2.965\)\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright\strut
\(\hat{\alpha}+\hat{\tau}_3+\hat{\gamma}_2\)\\
\(=3.055+0.175\)\\
\(-0.106\)\\
\(=3.124\)\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright\strut
\(\hat{\alpha}+\hat{\tau}_4+\hat{\gamma}_2\)\\
\(=3.055+0.169\)\\
\(-0.106\)\\
\(=3.118\)\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright\strut
\(\hat{\alpha}+\hat{\tau}_5+\hat{\gamma}_2\)\\
\(=3.055+0.468\)\\
\(-0.106\)\\
\(=3.417\)\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\normalsize

One final plot of the fitted values from this additive model in Figure
\ref{fig:Figure4-16} hopefully crystallizes the implications of an
additive model and reinforces that this model creates and assumes that
the differences across levels of one variable are the same regardless of
the level of the other variable and that this creates parallel lines.
The difference between \texttt{cigbuy} levels across all income groups
is a drop in -0.106 points. The income groups have the same differences
regardless of cigarette buying or not, with income group 5 much higher
than the other four groups.








\begin{figure}
\centering
\includegraphics{04-twoWayAnova_files/figure-latex/Figure4-16-1.pdf}
\caption{\label{fig:Figure4-16}Illustration of the results from Table \ref{tab:Table4-2}
showing the combined impacts of the components of the additive model for
\texttt{prodebt}. Panel (a) uses income groups on the x-axis and
different lines for cigarette buyers (1) or not (0). Panel (b) displays
the different income groups as lines with the cigarette buying status on
the x-axis.}
\end{figure}

\textbf{In general, we proceed through the following steps in any 2-WAY
ANOVA situation:}
>>>>>>> origin/chapter0to1_edits

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
<<<<<<< HEAD
  What is the impact of increasing the confidence level in this
  situation?
\item
  What happens to the width of the confidence interval if the size of
  the SE increases or decreases?
\item
  What about increasing the sample size -- should that increase or
  decrease the width of the interval?
\end{enumerate}

All the general results you learned before about impacts to widths of
CIs hold in this situation whether we are considering the parametric or
bootstrap methods\ldots{}

To finish this example, we will generate the comparable bootstrap 90\%
confidence interval using the bootstrap distribution in Figure
\ref{fig:Figure2-21}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Tobs <-}\StringTok{ }\KeywordTok{diffmean}\NormalTok{(GPA }\OperatorTok{~}\StringTok{ }\NormalTok{Sex, }\DataTypeTok{data=}\NormalTok{s217); Tobs}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   diffmean 
## -0.2498069
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\NormalTok{B<-}\StringTok{ }\DecValTok{1000}
\NormalTok{Tstar<-}\KeywordTok{matrix}\NormalTok{(}\OtherTok{NA}\NormalTok{,}\DataTypeTok{nrow=}\NormalTok{B)}
\ControlFlowTok{for}\NormalTok{ (b }\ControlFlowTok{in}\NormalTok{ (}\DecValTok{1}\OperatorTok{:}\NormalTok{B))\{}
\NormalTok{  Tstar[b]<-}\KeywordTok{diffmean}\NormalTok{(GPA }\OperatorTok{~}\StringTok{ }\NormalTok{Sex, }\DataTypeTok{data=}\KeywordTok{resample}\NormalTok{(s217))}
\NormalTok{  \}}
\KeywordTok{qdata}\NormalTok{(Tstar,.}\DecValTok{05}\NormalTok{)}
=======
  Make an interaction plot.
\item
  Fit the interaction model; examine the test for the interaction.
\item
  Check the residual diagnostic plots for the interaction model
  (especially normality and equal variance).

  \begin{itemize}
  \tightlist
  \item
    If there is a problem with normality or equal variance, consider a
    ``transformation'' of the response as discussed in Chapter
    \ref{chapter7} This can help make the responses have similar
    variances or responses to be more normal, but sometimes not both.
  \end{itemize}
\end{enumerate}

\newpage

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\item
  If the interaction test has a small p-value, that is your main result.
  Focus on the interaction plot from (1) to fully understand the
  results, adding Tukey's HSD results to see which means of the
  combinations of levels are detected as being different.
\item
  If the interaction is not considered important, then re-fit the model
  without the interaction (additive model) and re-check the diagnostic
  plots. If the diagnostics are reasonable to proceed:

  \begin{itemize}
  \item
    Focus on the results for each explanatory variable, using Type II
    tests especially if the design is not balanced.
  \item
    Report the initial interaction test results and the results for the
    test for each variable from the model that is re-fit without the
    interaction.
  \item
    Model coefficients are interesting as they are shifts from baseline
    for each level of each variable, controlling for the other variable
    -- interpret those differences if the number of levels is not too
    great.
  \end{itemize}
\end{enumerate}

Whether you end up favoring an additive or interaction model, all steps
of the hypothesis testing protocol should be engaged.

\section{Pushing Two-Way ANOVA to the limit: Un-replicated
designs}\label{section4-6}

In some situations, it is too expensive to replicate combinations of
treatments and only one observation at each combination of the two
explanatory variables, A and B, is possible. In these situations, even
though we have information about all combinations of A and B, it is no
longer possible to test for an interaction. Our regular rules for
degrees of freedom show that we have nothing left for the error degrees
of freedom and so we have to drop the interaction and call that
potential interaction variability ``error''.

We can still perform an analysis of the responses but an issue occurs
with trying to estimate the interaction \(F\)-test statistic -- we run
out of degrees of freedom for the error. To illustrate these methods,
the paper towel example is revisited except that only one response for
each combination is used. Now the entire data set can be displayed:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ptR<-}\KeywordTok{read.csv}\NormalTok{(}\StringTok{"http://www.math.montana.edu/courses/s217/documents/ptR.csv"}\NormalTok{)}
\NormalTok{ptR}\OperatorTok{$}\NormalTok{dropsf<-}\KeywordTok{factor}\NormalTok{(ptR}\OperatorTok{$}\NormalTok{drops)}
\NormalTok{ptR}
>>>>>>> origin/chapter0to1_edits
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
<<<<<<< HEAD
##          p   quantile 
##  0.0500000 -0.4032273
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{qdata}\NormalTok{(Tstar,.}\DecValTok{95}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           p    quantile 
##  0.95000000 -0.09521925
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{quantiles<-}\KeywordTok{qdata}\NormalTok{(Tstar,}\KeywordTok{c}\NormalTok{(.}\DecValTok{05}\NormalTok{,.}\DecValTok{95}\NormalTok{))}
\NormalTok{quantiles}
=======
##   brand drops responses dropsf
## 1    B1    10 1.9064356     10
## 2    B2    10 3.0504173     10
## 3    B1    20 0.7737965     20
## 4    B2    20 2.8384124     20
## 5    B1    30 1.5557071     30
## 6    B2    30 0.5470565     30
\end{verbatim}

Upon first inspection the interaction plot in Figure
\ref{fig:Figure4-17} looks like there might be some interesting
interactions present. But remember now that there is only a single
observation at each combination of the brands and water levels so there
is not much power to detect differences in this sort of situation and no
replicates at combination that allow estimation of SEs so no bands are
produced in the plot.




\begin{figure}
\centering
\includegraphics{04-twoWayAnova_files/figure-latex/Figure4-17-1.pdf}
\caption{\label{fig:Figure4-17}Interaction plot in paper towel data set with no
replication.}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\KeywordTok{intplot}\NormalTok{(responses}\OperatorTok{~}\NormalTok{brand}\OperatorTok{*}\NormalTok{dropsf,}\DataTypeTok{data=}\NormalTok{ptR,}\DataTypeTok{lwd=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The next step would be to assess the statistical evidence related to an
interaction between \texttt{Brand} and \texttt{Drops}. A problem will
arise in trying to form the ANOVA table as you would see this in the
console:

\small

\begin{verbatim}
> anova(lm(responses~dropsf*brand,data=ptR))
Analysis of Variance Table
Response: responses
             Df  Sum Sq Mean Sq F value Pr(>F)
dropsf        2 2.03872 1.01936               
brand         1 0.80663 0.80663               
dropsf:brand  2 2.48773 1.24386               
Residuals     0 0.00000                       
Warning message:
In anova.lm(lm(responses ~ dropsf * brand, data = ptR)) :
  ANOVA F-tests on an essentially perfect fit are unreliable
\end{verbatim}

\normalsize

Warning messages in R output show up in red after you run functions that
contain problems and are generally not a good thing, but can sometimes
be ignored. In this case, the warning message is not needed -- there are
no \(F\)-statistics or p-values in the results so we know there are some
issues with the results. The bolded line is key here squares of 0.
Without replication, there are no degrees of freedom left to estimate
the residual error. My (Greenwood's) first statistics professor, Gordon
Bril at Luther College, used to refer to this as ``shooting your load''
by fitting too many terms in the model given the number of observations
available. Maybe this is a bit graphic but hopefully will help you
remember the need for replication if you want to estimate interactions
-- it did for me. Without replication of observations, we run out of
information to estimate and test all the desired model components.

So what can we do if we can't afford replication? We can \emph{assume}
that the interaction does not exist and use those degrees of freedom and
variability as the error variability. When we drop the interaction from
Two-Way models, the interaction variability is added into the
\(\text{SS}_E\) so this is interaction between the variables. We are not
able to test for an interaction so must rely on the interaction plot to
assess whether an interaction might be present. Figure
\ref{fig:Figure4-17} suggests there might be an interaction in these
data (the two brands lines cross noticeably suggesting non-parallel
lines). So in this case, assuming no interaction is present is hard to
justify. But if we proceed under this dangerous assumption, tests for
the main effects can be developed.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{norep1<-}\KeywordTok{lm}\NormalTok{(responses}\OperatorTok{~}\NormalTok{dropsf}\OperatorTok{+}\NormalTok{brand,}\DataTypeTok{data=}\NormalTok{ptR)}
\KeywordTok{Anova}\NormalTok{(norep1)}
>>>>>>> origin/chapter0to1_edits
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
<<<<<<< HEAD
##        quantile    p
## 5%  -0.40322729 0.05
## 95% -0.09521925 0.95
\end{verbatim}

The output tells us that the 90\% confidence interval is from -0.393 to
-0.094 GPA points. The bootstrap distribution with the observed
difference in the sample means and these cut-offs is displayed in Figure
\ref{fig:Figure2-21} using this code:


=======
## Anova Table (Type II tests)
## 
## Response: responses
##            Sum Sq Df F value Pr(>F)
## dropsf    2.03872  2  0.8195 0.5496
## brand     0.80663  1  0.6485 0.5052
## Residuals 2.48773  2
\end{verbatim}

In the additive model, the last row of the ANOVA table that is called
the \texttt{Residuals} row is really the interaction row from the
interaction model ANOVA table. Neither main effect had a small p-value
(\texttt{Drops}: \(F(2,2)=0.82, \text{ p-value}=0.55\) and
\texttt{Brand}: \(F(1,2)=0.65, \text{ p-value}=0.51\)) in the additive
model. To get small p-values with small sample sizes, the differences
would need to be \textbf{very} large because the residual degrees of
freedom have become very small. The term-plots in Figure
\ref{fig:Figure4-18} show that the differences among the levels are
small relative to the residual variability as seen in the error bars
around each point estimate.
>>>>>>> origin/chapter0to1_edits




<<<<<<< HEAD
\begin{figure}
\centering
\includegraphics{02-reintroductionToStatistics_files/figure-latex/Figure2-21-1.pdf}
\caption{\label{fig:Figure2-21}Histogram and density curve of bootstrap distribution of
difference in sample mean GPAs (male minus female) with observed
difference (solid vertical line) and quantiles that delineate the 90\%
confidence intervals (dashed vertical lines).}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\KeywordTok{hist}\NormalTok{(Tstar,}\DataTypeTok{labels=}\NormalTok{T)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v=}\NormalTok{Tobs,}\DataTypeTok{col=}\StringTok{"red"}\NormalTok{,}\DataTypeTok{lwd=}\DecValTok{2}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v=}\NormalTok{quantiles}\OperatorTok{$}\NormalTok{quantile,}\DataTypeTok{col=}\StringTok{"blue"}\NormalTok{,}\DataTypeTok{lwd=}\DecValTok{3}\NormalTok{,}\DataTypeTok{lty=}\DecValTok{2}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{density}\NormalTok{(Tstar),}\DataTypeTok{main=}\StringTok{"Density curve of Tstar"}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v=}\NormalTok{Tobs,}\DataTypeTok{col=}\StringTok{"red"}\NormalTok{,}\DataTypeTok{lwd=}\DecValTok{2}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v=}\NormalTok{quantiles}\OperatorTok{$}\NormalTok{quantile,}\DataTypeTok{col=}\StringTok{"blue"}\NormalTok{,}\DataTypeTok{lwd=}\DecValTok{3}\NormalTok{,}\DataTypeTok{lty=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In the previous output, the parametric 90\% confidence interval is from
0.095 to 0.404, suggesting similar results again from the two approaches
once you account for the two different orders of differencing of the
groups. Based on the bootstrap CI, we can say that we are 90\% confident
that the difference in the true mean GPAs for STAT 217 students is
between -0.393 to -0.094 GPA points (male minus females). Because sex
cannot be assigned to the subjects, we cannot infer that sex is causing
this difference and because this was a voluntary response sample of STAT
217 students in a given semester, we cannot infer that a difference of
this size would apply to all STAT 217 students or even students in
another semester.

Throughout the semester, pay attention to the distinctions between
parameters and statistics, focusing on the differences between estimates
based on the sample and inferences for the population of interest in the
form of the parameters of interest. Remember that statistics are
summaries of the sample information and parameters are characteristics
of populations (which we rarely know). And that our inferences are
limited to the population that we randomly sampled from, if we randomly
sampled.

\section{Chapter summary}\label{section2-10}

In this chapter, we reviewed basic statistical inference methods in the
context of a two-sample mean problem. You were introduced to using R to
do permutation testing and generate bootstrap confidence intervals as
well as obtaining parametric \(t\)-test and confidence intervals in this
same situation. You should have learned how to use a \texttt{for} loop
for doing the nonparametric inferences and the \texttt{t.test} function
for generating parametric inferences. In the two examples considered,
the parametric and nonparametric methods provided similar results,
suggesting that the assumptions were at least close to being met for the
parametric procedures. When parametric and nonparametric approaches
disagree, the nonparametric methods are likely to be more trustworthy
since they have less restrictive assumptions but can still have
problems.

When the noted conditions are not met in a hypothesis testing situation,
the Type I error rates can be inflated, meaning that we reject the null
hypothesis more often than we have allowed to occur by chance.
Specifically, we could have a situation where our assumed 5\%
significance level test might actually reject the null when it is true
20\% of the time. If this is occurring, we call a procedure
\textbf{\emph{liberal}} (it rejects too easily) and if the procedure is
liberal, how could we trust a small p-value to be a ``real'' result and
not just an artifact of violating the assumptions of the procedure?
Likewise, for confidence intervals we hope that our 95\% confidence
level procedure, when repeated, will contain the true parameter 95\% of
the time. If our assumptions are violated, we might actually have an
80\% confidence level procedure and it makes it hard to trust the
reported results for our observed data set. Statistical inference relies
on a belief in the methods underlying our inferences. If we don't trust
our assumptions, we shouldn't trust the conclusions to perform the way
we want them to. As sample sizes increase and/or violations of
conditions lessen, then the procedures will perform better. In Chapter
\ref{chapter3}, we'll learn some new tools for doing diagnostics to help
us assess how much those conditions are violated.

\section{Summary of important R code}\label{section2-11}

The main components of R code used in this chapter follow with
components to modify in red, remembering that any R packages mentioned
need to be installed and loaded for this code to have a chance of
working:

\begin{itemize}
\item
  summary(\textcolor{red}{DATASETNAME})

  \begin{itemize}
  \tightlist
  \item
    Provides numerical summaries of all variables in the data set.
  \end{itemize}
\item
  t.test(\textcolor{red}{Y} \textasciitilde{} \textcolor{red}{X},
  data=\textcolor{red}{DATASETNAME}, conf.level=0.95)

  \begin{itemize}
  \tightlist
  \item
    Provides two-sample t-test test statistic, df, p-value, and 95\%
    confidence interval.
  \end{itemize}
\item
  2*pt(abs(\textcolor{red}{Tobs}), df=\textcolor{red}{DF}, lower.tail=F)
=======
\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(}\KeywordTok{allEffects}\NormalTok{(norep1))}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{04-twoWayAnova_files/figure-latex/Figure4-18-1.pdf}
\caption{\label{fig:Figure4-18}Term-plots for the additive model in paper towel data set
with no replication.}
\end{figure}

Hopefully by pushing the limits there are two conclusions available from
this section. First, replication is important, both in being able to
perform tests for interactions and for having enough power to detect
differences for the main effects. Second, dropping from the interaction
model to additive model, the variability explained by the interaction
term is pushed into the error term, whether replication is available or
not.

\section{Chapter summary}\label{section4-7}

In this chapter, methods for handling two different categorical
predictors in the same model with a continuous response were developed.
The methods build on techniques from Chapter \ref{chapter3} for the
One-Way ANOVA and there are connections between the two models. This was
most clearly seen in the guinea pig data set that was analyzed in both
chapters. When two factors are available, it is better to start with the
methods developed in this chapter because the interaction between the
factors can, potentially, be separated from their main effects. The
additive model is easier to interpret but should only be used when no
evidence of an interaction is present. When an interaction is determined
to be present, the main effects should not be interpreted and the
interaction plot in combination with Tukey's HSD provides information on
the important aspects of the results.

\begin{itemize}
\item
  If the interaction is retained in the model, there are two things you
  want to do with interpreting the interaction:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \item
    Describe the interaction, going through the changes from left to
    right in the interaction plot or term-plot for each level of the
    other variable.
  \item
    Suggest optimal combinations of the two variables to either get the
    highest or lowest possible responses.

    \begin{enumerate}
    \def\labelenumii{\alph{enumii}.}
    \tightlist
    \item
      For example, you might want to identify a dosage and delivery
      method for the guinea pigs to recommend and one to avoid if you
      want to optimize odontoblast growth.
    \end{enumerate}
  \end{enumerate}
\item
  If there is no interaction, then the additive model provides
  information on each of the variables and the differences across levels
  of each variable are the same regardless of the levels of the other
  variable.
>>>>>>> origin/chapter0to1_edits

  \begin{itemize}
  \tightlist
  \item
<<<<<<< HEAD
    Finds the two-sided test p-value for an observed 2-sample t-test
    statistic of \texttt{Tobs}.
  \end{itemize}
\item
  hist(\textcolor{red}{DATASETNAME\$Y})

  \begin{itemize}
  \tightlist
  \item
    Makes a histogram of a variable named \texttt{Y} from the data set
    of interest.
  \end{itemize}
\item
  boxplot(\textcolor{red}{Y}\textasciitilde{}\textcolor{red}{X},
  data=\textcolor{red}{DATASETNAME})
=======
    You can describe the deviations from baseline as in Chapter
    \ref{chapter3}, but for each variable, noting that you are
    controlling for the variable.
  \end{itemize}
\end{itemize}

Some statisticians might have different recommendations for dealing with
interactions and main effects, especially in the context of evidence of
an interaction. We have chosen to focus on tests for interactions to
screen for ``real'' interactions and then interpret the interaction
plots aided by the Tukey's HSD for determining which combinations of
levels are detectably different. Others might suggest exploring the main
effects tests even with interactions present. In some cases, those
results are interesting but in others the results can be misleading and
wanted to avoid trying to tell you when that might happen. Consider two
scenarios, one where the main effects have large p-values but the
interaction has a small p-value and the other where the main effects and
the interaction all have small p-values. The methods discussed in this
chapter allow us to effectively arrive at the interpretation of the
differences in the results across the combinations of the treatments due
to the interaction having a small p-value. The main effects results are
secondary results at best when the interaction is important because we
know that impacts of one explanatory variable is changing based on the
levels of the other variable.

Chapter \ref{chapter5} presents a bit of a different set of statistical
methods that allow analyses of data sets similar to those considered in
the last two chapters but with a categorical response variable. The
methods are very different but are quite similar in overall goals to
those in Chapter \ref{chapter3} where differences in responses where
explored across groups. After Chapter \ref{chapter5}, the rest of the
semester will return to fitting models using the \texttt{lm} function as
used here, but incorporating quantitative predictor variables and then
eventually incorporating both categorical and quantitative predictor
variables. The methods in Chapter \ref{chapter8} are actually quite
similar to those considered here.

\section{Important R code}\label{section4-8}

The main components of R code used in this chapter follow with
components to modify in red, remembering that any R packages mentioned
need to be installed and loaded for this code to have a chance of
working:

\begin{itemize}
\item
  tally(\textcolor{red}{A}\textasciitilde{}\textcolor{red}{B},
  data=\textcolor{red}{DATASETNAME})

  \begin{itemize}
  \item
    Requires the \texttt{mosaic} package be loaded.
  \item
    Provides the counts of observations in each combination of
    categorical predictor variables A and B, used to check for balance
    and understand sample sizes in each combination.
  \end{itemize}
\item
  \textcolor{red}{DATASETNAME}\$\textcolor{red}{VARIABLENAME}
  \texttt{\textless{}-}
  factor(\textcolor{red}{DATASETNAME}\$\textcolor{red}{VARIABLENAME})
>>>>>>> origin/chapter0to1_edits

  \begin{itemize}
  \tightlist
  \item
<<<<<<< HEAD
    Makes a boxplot of a variable named Y for groups in X from the data
    set.
  \end{itemize}
\item
  beanplot(\textcolor{red}{Y}\textasciitilde{}\textcolor{red}{X},
=======
    Use the \texttt{factor} function on any numerically coded
    explanatory variable where the numerical codes represent levels of a
    categorical variable.
  \end{itemize}
\item
  intplot(\textcolor{red}{Y}\textasciitilde{}\textcolor{red}{A}*\textcolor{red}{B},
>>>>>>> origin/chapter0to1_edits
  data=\textcolor{red}{DATASETNAME})

  \begin{itemize}
  \item
<<<<<<< HEAD
    Requires the \texttt{beanplot} package is loaded.
  \item
    Makes a beanplot of a variable named Y for groups in X from the data
    set.
  \end{itemize}
\item
  mean(\textcolor{red}{Y}\textasciitilde{}\textcolor{red}{X},
  data=\textcolor{red}{DATASETNAME});
  sd(\textcolor{red}{Y}\textasciitilde{}\textcolor{red}{X},
=======
    Download and install using:
  \item
    \texttt{source("http://www.math.montana.edu/courses/s217/documents/intplot.R")}
  \item
    Provides interaction plot.
  \end{itemize}
\item
  \textcolor{red}{INTERACTIONMODELNAME} \texttt{\textless{}-}
  lm(\textcolor{red}{Y}\textasciitilde{}\textcolor{red}{A}*\textcolor{red}{B},
>>>>>>> origin/chapter0to1_edits
  data=\textcolor{red}{DATASETNAME})

  \begin{itemize}
  \item
<<<<<<< HEAD
    This usage of \texttt{mean} and \texttt{sd} requires the
    \texttt{mosaic} package.
  \item
    Provides the mean and sd of responses of Y for each group described
    in X.
  \end{itemize}
\item
  favstats(\textcolor{red}{Y}\textasciitilde{}\textcolor{red}{X},
  data=\textcolor{red}{DATASETNAME})

  \begin{itemize}
  \tightlist
  \item
    Provides numerical summaries of Y by groups described in X.
  \end{itemize}
\item
  Tobs \texttt{\textless{}-}
  t.test(\textcolor{red}{Y}\textasciitilde{}\textcolor{red}{X},
  data=\textcolor{red}{DATASETNAME}, var.equal=T)\$statistic; Tobs\\
  B \texttt{\textless{}-} 1000\\
  Tstar \texttt{\textless{}-} matrix(NA, nrow=B)\\
  for (b in (1:B))\{\\
  Tstar{[}b{]} \texttt{\textless{}-}
  t.test(\textcolor{red}{Y}\textasciitilde{}shuffle(\textcolor{red}{X}),
  data=\textcolor{red}{DATASETNAME}, var.equal=T)\$statistic\\
  \}
=======
    Fits the interaction model with main effects for A and B and an
    interaction between them.
  \item
    This is the first model that should be fit in Two-Way ANOVA modeling
    situations.
  \end{itemize}
\item
  \textcolor{red}{ADDITIVEMODELNAME} \texttt{\textless{}-}
  lm(\textcolor{red}{Y}\textasciitilde{}\textcolor{red}{A}+\textcolor{red}{B},
  data=\textcolor{red}{DATASETNAME})

  \begin{itemize}
  \item
    Fits the additive model with only main effects for A and B but no
    interaction between them.
  \item
    Should only be used if the interaction has been decided to be
    unimportant using a test for the interaction.
  \end{itemize}
\item
  summary(\textcolor{red}{MODELNAME})
>>>>>>> origin/chapter0to1_edits

  \begin{itemize}
  \tightlist
  \item
<<<<<<< HEAD
    Code to run a \texttt{for} loop to generate 1000 permuted versions
    of the test statistic using the \texttt{shuffle} function and keep
    track of the results in \texttt{Tstar}
  \end{itemize}
\item
  pdata(Tstar, abs(\textcolor{red}{Tobs}, lower.tail=F)

  \begin{itemize}
  \tightlist
  \item
    Finds the proportion of the permuted test statistics in Tstar that
    are less than -\textbar{}Tobs\textbar{} or greater than
    \textbar{}Tobs\textbar{}, useful for finding the two-sided test
    p-value.
  \end{itemize}
\item
  Tobs \texttt{\textless{}-}
  diffmean(\textcolor{red}{Y}\textasciitilde{}\textcolor{red}{X},
  data=\textcolor{red}{DATASETNAME}, var.equal=T)\$statistic; Tobs\\
  B \texttt{\textless{}-} 1000\\
  Tstar \texttt{\textless{}-} matrix(NA, nrow=B)\\
  for (b in (1:B))\{\\
  Tstar{[}b{]} \texttt{\textless{}-}
  diffmean(\textcolor{red}{Y}\textasciitilde{}\textcolor{red}{X},
  data=resample(\textcolor{red}{DATASETNAME}))\\
  \}
=======
    Generates model summary information including the estimated model
    coefficients, SEs, t-tests, and p-values.
  \end{itemize}
\item
  Anova(\textcolor{red}{MODELNAME})

  \begin{itemize}
  \item
    Requires the \texttt{car} package to be loaded.
  \item
    Generates a Type II Sums of Squares ANOVA table that is useful for
    both additive and interaction models, but it most important to use
    when working with the additive model as it provides inferences for
    each term conditional on the other one.
  \end{itemize}
\item
  par(mfrow=c(2,2)); plot(\textcolor{red}{MODELNAME})
>>>>>>> origin/chapter0to1_edits

  \begin{itemize}
  \tightlist
  \item
<<<<<<< HEAD
    Code to run a \texttt{for} loop to generate 1000 bootstrapped
    versions of the data set using the \texttt{resample} function and
    keep track of the results of the statistic in \texttt{Tstar}.
  \end{itemize}
\item
  qdata(Tstar, c(0. 025, 0. 975))

  \begin{itemize}
  \tightlist
  \item
    Provides the values that delineate the middle 95\% of the results in
    the bootstrap distribution (\texttt{Tstar})
  \end{itemize}
\end{itemize}

\section{Practice problems}\label{section2-12}

Load the \texttt{HELPrct} data set from the \texttt{mosaicData} package
\citep{R-mosaicData} (you need to install the \texttt{mosaicData}
package once to be able to load it). The HELP study was a clinical trial
for adult inpatients recruited from a detoxification unit. Patients with
no primary care physician were randomly assigned to receive a
multidisciplinary assessment and a brief motivational intervention or
usual care and various outcomes were observed. Two of the variables in
the data set are \texttt{sex}, a factor with levels male and female and
\texttt{daysanysub} which is the time (in days) to first use of any
substance post-detox. We are interested in the difference in mean number
of days to first use of any substance post-detox between males and
females. There are some missing responses and the following code will
produce \texttt{favstats} with the missing values and then provide a
data set that for complete observations by applying the \texttt{na.omit}
function that removes any observations with missing values.

\begin{verbatim}
require(mosaicData)
data(HELPrct)
HELPrct <- HELPrct[, c("daysanysub", "sex")] #Just focus on two variables
HELPrct <- na.omit(HELPrct2) #Removes subjects with missing
favstats(daysanysub~sex, data=HELPrct2)
favstats(daysanysub~sex, data=HELPrct3)
\end{verbatim}

2.1. Based on the results provided, how many observations were missing
for males and females? Missing values here likely mean that the subjects
didn't use any substances post-detox in the time of the study but might
have at a later date -- the study just didn't run long enough. This is
called \textbf{\emph{censoring}}. What is the problem with the numerical
summaries here if the missing responses were all something larger than
the largest observation?

2.2. Make a beanplot and a boxplot of \texttt{daysanysub}
\textasciitilde{} \texttt{sex} using the \texttt{HELPrct3} data set
created above. Compare the distributions, recommending parametric or
nonparametric inferences.

2.3. Generate the permutation results and write out the 6+ steps of the
hypothesis test, making sure to note the numerical value of observed
test statistic you are using and include a discussion of the scope of
inference.

2.4. Interpret the p-value for these results.

2.5. Generate the parametric \texttt{t.test} results, reporting the
test-statistic, its distribution under the null hypothesis, and compare
the p-value to those observed using the permutation approach.

2.6. Make and interpret a 95\% bootstrap confidence interval for the
difference in the means.
=======
    Generates four diagnostic plots including the Residuals vs Fitted
    and Normal Q-Q plot.
  \end{itemize}
\item
  plot(allEffects(\textcolor{red}{MODELNAME}))

  \begin{itemize}
  \item
    Requires the \texttt{effects} package be loaded.
  \item
    Plots the results from the estimated model.
  \end{itemize}
\end{itemize}

\newpage

\section{Practice problems}\label{section4-9}

To practice the Two-Way ANOVA, consider a data set on \(N=861\) ACT
Mathematics Usage Test scores from 1987. The test was given to a sample
of high school seniors who met one of three profiles of high school
mathematics course work: (a) Algebra I only; (b) two Algebra courses and
Geometry; and (c) two Algebra courses, Geometry, Trigonometry, Advanced
Mathematics, and Beginning Calculus. These data were generated from
summary statistics for one particular form of the test as reported by
\citet{Doolittle1989}. The source of this version of the data set is
\citet{Ramsey2012} and the \texttt{Sleuth2} package \citep{R-Sleuth2}.
First install and then load that package.

\begin{verbatim}
require(Sleuth2)
require(mosaic)
math <- ex1320
names(math)
favstats(Score ~ Sex+Background, data=math)
\end{verbatim}

4.1. Use the \texttt{favstats} summary to discuss whether the design was
balanced or not.

4.2. Make a side-by-side beanplot and interaction plot of the results
and discuss the relationship between Sex, Background, and ACT Score.

4.3. Write out the interaction model in terms of the Greek letters,
making sure to define all the terms and don't forget the error terms in
the model.

4.4. Fit the interaction plot and find the ANOVA table. For the test you
should consider first (the interaction), write out the hypotheses,
report the test statistic, p-value, distribution of the test statistic
under the null, and write a conclusion related to the results of this
test.

4.5. Re-fit the model as an additive model (why is this reasonable
here?) and use \texttt{Anova} to find the Type II sums of squares ANOVA.
Write out the hypothesis for the Background variable, report the test
statistic, p-value, distribution of the test statistic under the null,
and write a conclusion related to the results of this test.

4.6. Use the \texttt{effects} package to make a term-plot from the
additive model from 4.5 and discuss the results. Specifically, discuss
what you can conclude about the average relationship across both sexes,
between Background and average ACT score?

4.7. Make our standard diagnostic plots and assess the assumptions using
these plots. Can you assess independence using these plots? Discuss this
assumption in this situation.

4.8. Use the estimated model coefficients to determine which of the
combinations of levels provides the highest estimated average score.
>>>>>>> origin/chapter0to1_edits

\bibliography{packages,references}


\end{document}
