<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>A Second Semester Statistics Course with R</title>
  <meta name="description" content="A Second Semester Statistics Course with R">
  <meta name="generator" content="bookdown 0.4 and GitBook 2.6.7">

  <meta property="og:title" content="A Second Semester Statistics Course with R" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="gpeterson406/GreenwoodBanner_Book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="A Second Semester Statistics Course with R" />
  
  
  

<meta name="author" content="Mark Greenwood and Katherine Banner">


<meta name="date" content="2017-07-03">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="chapter6.html">
<link rel="next" href="chapter8.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="chapter" data-level="1" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i><b>1</b> Preface</a><ul>
<li class="chapter" data-level="1.1" data-path="preface.html"><a href="preface.html#overview-of-methods"><i class="fa fa-check"></i><b>1.1</b> Overview of methods</a></a></li>
<li class="chapter" data-level="1.2" data-path="preface.html"><a href="preface.html#getting-started-in-r"><i class="fa fa-check"></i><b>1.2</b> Getting started in R</a></li>
<li class="chapter" data-level="1.3" data-path="preface.html"><a href="preface.html#basic-summary-statistics-histograms-and-boxplots-using-r"><i class="fa fa-check"></i><b>1.3</b> Basic summary statistics, histograms, and boxplots using R</a></li>
<li class="chapter" data-level="1.4" data-path="preface.html"><a href="preface.html#chapter-summary"><i class="fa fa-check"></i><b>1.4</b> Chapter summary</a></li>
<li class="chapter" data-level="1.5" data-path="preface.html"><a href="preface.html#important-r-code"><i class="fa fa-check"></i><b>1.5</b> Important R Code</a></li>
<li class="chapter" data-level="1.6" data-path="preface.html"><a href="preface.html#practice-problems"><i class="fa fa-check"></i><b>1.6</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chapter2.html"><a href="chapter2.html"><i class="fa fa-check"></i><b>2</b> (R)e-Introduction to statistics</a><ul>
<li class="chapter" data-level="2.1" data-path="chapter2.html"><a href="chapter2.html#section2-1"><i class="fa fa-check"></i><b>2.1</b> Histograms, boxplots, and density curves</a></li>
<li class="chapter" data-level="2.2" data-path="chapter2.html"><a href="chapter2.html#section2-2"><i class="fa fa-check"></i><b>2.2</b> Beanplots</a></li>
<li class="chapter" data-level="2.3" data-path="chapter2.html"><a href="chapter2.html#section2-3"><i class="fa fa-check"></i><b>2.3</b> Models, hypotheses, and permutations for the 2 sample mean situation</a></li>
<li class="chapter" data-level="2.4" data-path="chapter2.html"><a href="chapter2.html#section2-4"><i class="fa fa-check"></i><b>2.4</b> Permutation testing for the 2 sample mean situation</a></li>
<li class="chapter" data-level="2.5" data-path="chapter2.html"><a href="chapter2.html#section2-5"><i class="fa fa-check"></i><b>2.5</b> Hypothesis testing (general)</a></li>
<li class="chapter" data-level="2.6" data-path="chapter2.html"><a href="chapter2.html#section2-6"><i class="fa fa-check"></i><b>2.6</b> Connecting randomization (nonparametric) and parametric tests</a></li>
<li class="chapter" data-level="2.7" data-path="chapter2.html"><a href="chapter2.html#section2-7"><i class="fa fa-check"></i><b>2.7</b> Second example of permutation tests</a></li>
<li class="chapter" data-level="2.8" data-path="chapter2.html"><a href="chapter2.html#section2-8"><i class="fa fa-check"></i><b>2.8</b> Confidence intervals and bootstrapping</a></li>
<li class="chapter" data-level="2.9" data-path="chapter2.html"><a href="chapter2.html#section2-9"><i class="fa fa-check"></i><b>2.9</b> Bootstrap confidence intervals for difference in GPAs</a></li>
<li class="chapter" data-level="2.10" data-path="chapter2.html"><a href="chapter2.html#section2-10"><i class="fa fa-check"></i><b>2.10</b> Chapter summary</a></li>
<li class="chapter" data-level="2.11" data-path="chapter2.html"><a href="chapter2.html#section2-11"><i class="fa fa-check"></i><b>2.11</b> Summary of important R code</a></li>
<li class="chapter" data-level="2.12" data-path="chapter2.html"><a href="chapter2.html#section2-12"><i class="fa fa-check"></i><b>2.12</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="chapter3.html"><a href="chapter3.html"><i class="fa fa-check"></i><b>3</b> One-Way ANOVA</a><ul>
<li class="chapter" data-level="3.1" data-path="chapter3.html"><a href="chapter3.html#section3-1"><i class="fa fa-check"></i><b>3.1</b> Situation</a></li>
<li class="chapter" data-level="3.2" data-path="chapter3.html"><a href="chapter3.html#section3-2"><i class="fa fa-check"></i><b>3.2</b> Linear model for One-Way ANOVA (cell-means and reference-coding)</a></li>
<li class="chapter" data-level="3.3" data-path="chapter3.html"><a href="chapter3.html#section3-3"><i class="fa fa-check"></i><b>3.3</b> One-Way ANOVA Sums of Squares, Mean Squares, and F-test</a></li>
<li class="chapter" data-level="3.4" data-path="chapter3.html"><a href="chapter3.html#section3-4"><i class="fa fa-check"></i><b>3.4</b> ANOVA model diagnostics including QQ-plots</a></li>
<li class="chapter" data-level="3.5" data-path="chapter3.html"><a href="chapter3.html#section3-5"><i class="fa fa-check"></i><b>3.5</b> Guinea pig tooth growth One-Way ANOVA example</a></li>
<li class="chapter" data-level="3.6" data-path="chapter3.html"><a href="chapter3.html#section3-6"><i class="fa fa-check"></i><b>3.6</b> Multiple (pair-wise) comparisons using Tukey’s HSD and the compact letter display</a></li>
<li class="chapter" data-level="3.7" data-path="chapter3.html"><a href="chapter3.html#section3-7"><i class="fa fa-check"></i><b>3.7</b> Pair-wise comparisons for Prisoner Rating data</a></li>
<li class="chapter" data-level="3.8" data-path="chapter3.html"><a href="chapter3.html#section3-8"><i class="fa fa-check"></i><b>3.8</b> Chapter Summary</a></li>
<li class="chapter" data-level="3.9" data-path="chapter3.html"><a href="chapter3.html#section3-9"><i class="fa fa-check"></i><b>3.9</b> Summary of important R code</a></li>
<li class="chapter" data-level="3.10" data-path="chapter3.html"><a href="chapter3.html#section3-10"><i class="fa fa-check"></i><b>3.10</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="chapter4.html"><a href="chapter4.html"><i class="fa fa-check"></i><b>4</b> Two-Way ANOVA</a><ul>
<li class="chapter" data-level="4.1" data-path="chapter4.html"><a href="chapter4.html#section4-1"><i class="fa fa-check"></i><b>4.1</b> Situation</a></li>
<li class="chapter" data-level="4.2" data-path="chapter4.html"><a href="chapter4.html#section4-2"><i class="fa fa-check"></i><b>4.2</b> Designing a two-way experiment and visualizing results</a></li>
<li class="chapter" data-level="4.3" data-path="chapter4.html"><a href="chapter4.html#section4-3"><i class="fa fa-check"></i><b>4.3</b> Two-Way ANOVA models and hypothesis tests</a></li>
<li class="chapter" data-level="4.4" data-path="chapter4.html"><a href="chapter4.html#section4-4"><i class="fa fa-check"></i><b>4.4</b> Guinea pig tooth growth analysis with Two-Way ANOVA</a></li>
<li class="chapter" data-level="4.5" data-path="chapter4.html"><a href="chapter4.html#section4-5"><i class="fa fa-check"></i><b>4.5</b> Observational study example: The Psychology of Debt</a></li>
<li class="chapter" data-level="4.6" data-path="chapter4.html"><a href="chapter4.html#section4-6"><i class="fa fa-check"></i><b>4.6</b> Pushing Two-Way ANOVA to the limit: Un-replicated designs</a></li>
<li class="chapter" data-level="4.7" data-path="chapter4.html"><a href="chapter4.html#section4-7"><i class="fa fa-check"></i><b>4.7</b> Chapter summary</a></li>
<li class="chapter" data-level="4.8" data-path="chapter4.html"><a href="chapter4.html#section4-8"><i class="fa fa-check"></i><b>4.8</b> Important R code</a></li>
<li class="chapter" data-level="4.9" data-path="chapter4.html"><a href="chapter4.html#section4-9"><i class="fa fa-check"></i><b>4.9</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="chapter5.html"><a href="chapter5.html"><i class="fa fa-check"></i><b>5</b> Chi-square tests</a><ul>
<li class="chapter" data-level="5.1" data-path="chapter5.html"><a href="chapter5.html#section5-1"><i class="fa fa-check"></i><b>5.1</b> Situation, contingency tables, and plots</a></li>
<li class="chapter" data-level="5.2" data-path="chapter5.html"><a href="chapter5.html#section5-2"><i class="fa fa-check"></i><b>5.2</b> Homogeneity Test Hypotheses</a></li>
<li class="chapter" data-level="5.3" data-path="chapter5.html"><a href="chapter5.html#section5-3"><i class="fa fa-check"></i><b>5.3</b> Independence Test Hypotheses</a></li>
<li class="chapter" data-level="5.4" data-path="chapter5.html"><a href="chapter5.html#section5-4"><i class="fa fa-check"></i><b>5.4</b> Models for R by C tables</a></li>
<li class="chapter" data-level="5.5" data-path="chapter5.html"><a href="chapter5.html#section5-5"><i class="fa fa-check"></i><b>5.5</b> Permutation tests for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li class="chapter" data-level="5.6" data-path="chapter5.html"><a href="chapter5.html#section5-6"><i class="fa fa-check"></i><b>5.6</b> Chi-square distribution for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li class="chapter" data-level="5.7" data-path="chapter5.html"><a href="chapter5.html#section5-7"><i class="fa fa-check"></i><b>5.7</b> Examining residuals for the source of differences</a></li>
<li class="chapter" data-level="5.8" data-path="chapter5.html"><a href="chapter5.html#section5-8"><i class="fa fa-check"></i><b>5.8</b> General Protocol for <span class="math inline">\(X^2\)</span> tests</a></li>
<li class="chapter" data-level="5.9" data-path="chapter5.html"><a href="chapter5.html#section5-9"><i class="fa fa-check"></i><b>5.9</b> Political Party and Voting results: Complete Analysis</a></li>
<li class="chapter" data-level="5.10" data-path="chapter5.html"><a href="chapter5.html#section5-10"><i class="fa fa-check"></i><b>5.10</b> Is cheating and lying related in students?</a></li>
<li class="chapter" data-level="5.11" data-path="chapter5.html"><a href="chapter5.html#section5-11"><i class="fa fa-check"></i><b>5.11</b> Analyzing a stratified random sample of California schools</a></li>
<li class="chapter" data-level="5.12" data-path="chapter5.html"><a href="chapter5.html#section5-12"><i class="fa fa-check"></i><b>5.12</b> Chapter summary</a></li>
<li class="chapter" data-level="5.13" data-path="chapter5.html"><a href="chapter5.html#section5-13"><i class="fa fa-check"></i><b>5.13</b> Review of Important R commands</a></li>
<li class="chapter" data-level="5.14" data-path="chapter5.html"><a href="chapter5.html#section5-14"><i class="fa fa-check"></i><b>5.14</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="chapter6.html"><a href="chapter6.html"><i class="fa fa-check"></i><b>6</b> Correlation and Simple Linear Regression</a><ul>
<li class="chapter" data-level="6.1" data-path="chapter6.html"><a href="chapter6.html#section6-1"><i class="fa fa-check"></i><b>6.1</b> Relationships between two quantitative variables</a></li>
<li class="chapter" data-level="6.2" data-path="chapter6.html"><a href="chapter6.html#section6-2"><i class="fa fa-check"></i><b>6.2</b> Estimating the correlation coefficient</a></li>
<li class="chapter" data-level="6.3" data-path="chapter6.html"><a href="chapter6.html#section6-3"><i class="fa fa-check"></i><b>6.3</b> Relationships between variables by groups</a></li>
<li class="chapter" data-level="6.4" data-path="chapter6.html"><a href="chapter6.html#section6-4"><i class="fa fa-check"></i><b>6.4</b> Inference for the correlation coefficient (Optional Section)</a></li>
<li class="chapter" data-level="6.5" data-path="chapter6.html"><a href="chapter6.html#section6-5"><i class="fa fa-check"></i><b>6.5</b> Are tree diameters related to tree heights?</a></li>
<li class="chapter" data-level="6.6" data-path="chapter6.html"><a href="chapter6.html#section6-6"><i class="fa fa-check"></i><b>6.6</b> Describing relationships with a regression model</a></li>
<li class="chapter" data-level="6.7" data-path="chapter6.html"><a href="chapter6.html#section6-7"><i class="fa fa-check"></i><b>6.7</b> Least Squares Estimation</a></li>
<li class="chapter" data-level="6.8" data-path="chapter6.html"><a href="chapter6.html#section6-8"><i class="fa fa-check"></i><b>6.8</b> Measuring the strength of regressions: R2</a></li>
<li class="chapter" data-level="6.9" data-path="chapter6.html"><a href="chapter6.html#section6-9"><i class="fa fa-check"></i><b>6.9</b> Outliers: leverage and influence</a></li>
<li class="chapter" data-level="6.10" data-path="chapter6.html"><a href="chapter6.html#section6-10"><i class="fa fa-check"></i><b>6.10</b> Residual diagnostics – setting the stage for inference</a></li>
<li class="chapter" data-level="6.11" data-path="chapter6.html"><a href="chapter6.html#section6-11"><i class="fa fa-check"></i><b>6.11</b> Old Faithful discharge and waiting times</a></li>
<li class="chapter" data-level="6.12" data-path="chapter6.html"><a href="chapter6.html#section6-12"><i class="fa fa-check"></i><b>6.12</b> Chapter summary</a></li>
<li class="chapter" data-level="6.13" data-path="chapter6.html"><a href="chapter6.html#section6-13"><i class="fa fa-check"></i><b>6.13</b> Important R code</a></li>
<li class="chapter" data-level="6.14" data-path="chapter6.html"><a href="chapter6.html#section6-14"><i class="fa fa-check"></i><b>6.14</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="chapter7.html"><a href="chapter7.html"><i class="fa fa-check"></i><b>7</b> Simple linear regression inference</a><ul>
<li class="chapter" data-level="7.1" data-path="chapter7.html"><a href="chapter7.html#section7-1"><i class="fa fa-check"></i><b>7.1</b> Model</a></li>
<li class="chapter" data-level="7.2" data-path="chapter7.html"><a href="chapter7.html#section7-2"><i class="fa fa-check"></i><b>7.2</b> Confidence Interval and Hypothesis tests for the slope and intercept</a></li>
<li class="chapter" data-level="7.3" data-path="chapter7.html"><a href="chapter7.html#section7-3"><i class="fa fa-check"></i><b>7.3</b> Bozeman temperature trend</a></li>
<li class="chapter" data-level="7.4" data-path="chapter7.html"><a href="chapter7.html#section7-4"><i class="fa fa-check"></i><b>7.4</b> Randomizing inferences for the slope coefficient</a></li>
<li class="chapter" data-level="7.5" data-path="chapter7.html"><a href="chapter7.html#section7-5"><i class="fa fa-check"></i><b>7.5</b> Transformations part I: Linearizing relationships</a></li>
<li class="chapter" data-level="7.6" data-path="chapter7.html"><a href="chapter7.html#section7-6"><i class="fa fa-check"></i><b>7.6</b> Transformations part II: Impacts on SLR interpretations: log(y), log(x), &amp; both log(y) &amp; log(x)</a></li>
<li class="chapter" data-level="7.7" data-path="chapter7.html"><a href="chapter7.html#confidence-interval-for-the-mean-and-prediction-intervals-for-a-new-observation-270"><i class="fa fa-check"></i><b>7.7</b> Confidence Interval for the mean and prediction Intervals for a new observation 270</a></li>
<li class="chapter" data-level="7.8" data-path="chapter7.html"><a href="chapter7.html#section7-7"><i class="fa fa-check"></i><b>7.8</b> Chapter summary</a></li>
<li class="chapter" data-level="7.9" data-path="chapter7.html"><a href="chapter7.html#section7-8"><i class="fa fa-check"></i><b>7.9</b> Important R code</a></li>
<li class="chapter" data-level="7.10" data-path="chapter7.html"><a href="chapter7.html#section7-9"><i class="fa fa-check"></i><b>7.10</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="chapter8.html"><a href="chapter8.html"><i class="fa fa-check"></i><b>8</b> Multiple linear regression</a><ul>
<li class="chapter" data-level="8.1" data-path="chapter8.html"><a href="chapter8.html#section8-1"><i class="fa fa-check"></i><b>8.1</b> Going from SLR to MLR</a></li>
<li class="chapter" data-level="8.2" data-path="chapter8.html"><a href="chapter8.html#section8-2"><i class="fa fa-check"></i><b>8.2</b> Validity conditions in MLR</a></li>
<li class="chapter" data-level="8.3" data-path="chapter8.html"><a href="chapter8.html#section8-3"><i class="fa fa-check"></i><b>8.3</b> Interpretation of MLR terms</a></li>
<li class="chapter" data-level="8.4" data-path="chapter8.html"><a href="chapter8.html#section8-4"><i class="fa fa-check"></i><b>8.4</b> Comparing multiple regression models</a></li>
<li class="chapter" data-level="8.5" data-path="chapter8.html"><a href="chapter8.html#section8-5"><i class="fa fa-check"></i><b>8.5</b> General recommendations for MLR interpretations and VIFs</a></li>
<li class="chapter" data-level="8.6" data-path="chapter8.html"><a href="chapter8.html#section8-6"><i class="fa fa-check"></i><b>8.6</b> MLR Inference: Parameter inferences using the t-distribution</a></li>
<li class="chapter" data-level="8.7" data-path="chapter8.html"><a href="chapter8.html#section8-7"><i class="fa fa-check"></i><b>8.7</b> Overall F-test in Multiple Linear Regression</a></li>
<li class="chapter" data-level="8.8" data-path="chapter8.html"><a href="chapter8.html#section8-8"><i class="fa fa-check"></i><b>8.8</b> Case Study: First year college GPA and SATs</a></li>
<li class="chapter" data-level="8.9" data-path="chapter8.html"><a href="chapter8.html#section8-9"><i class="fa fa-check"></i><b>8.9</b> Different intercepts for different groups: MLR with Indicator variables</a></li>
<li class="chapter" data-level="8.10" data-path="chapter8.html"><a href="chapter8.html#section8-10"><i class="fa fa-check"></i><b>8.10</b> Additive MLR with more than two groups: Headache example</a></li>
<li class="chapter" data-level="8.11" data-path="chapter8.html"><a href="chapter8.html#section8-11"><i class="fa fa-check"></i><b>8.11</b> Different slopes and different intercepts</a></li>
<li class="chapter" data-level="8.12" data-path="chapter8.html"><a href="chapter8.html#section8-12"><i class="fa fa-check"></i><b>8.12</b> F-tests for MLR models with quantitative and categorical variables and interactions</a></li>
<li class="chapter" data-level="8.13" data-path="chapter8.html"><a href="chapter8.html#section8-13"><i class="fa fa-check"></i><b>8.13</b> AICs for model selection</a></li>
<li class="chapter" data-level="8.14" data-path="chapter8.html"><a href="chapter8.html#section8-14"><i class="fa fa-check"></i><b>8.14</b> Forced Expiratory Volume model selection using AICs</a></li>
<li class="chapter" data-level="8.15" data-path="chapter8.html"><a href="chapter8.html#section8-15"><i class="fa fa-check"></i><b>8.15</b> Chapter summary</a></li>
<li class="chapter" data-level="8.16" data-path="chapter8.html"><a href="chapter8.html#section8-16"><i class="fa fa-check"></i><b>8.16</b> Important R code</a></li>
<li class="chapter" data-level="8.17" data-path="chapter8.html"><a href="chapter8.html#section8-17"><i class="fa fa-check"></i><b>8.17</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="chapter9.html"><a href="chapter9.html"><i class="fa fa-check"></i><b>9</b> Case studies</a><ul>
<li class="chapter" data-level="9.1" data-path="chapter9.html"><a href="chapter9.html#section9-1"><i class="fa fa-check"></i><b>9.1</b> Overview of material covered</a></li>
<li class="chapter" data-level="9.2" data-path="chapter9.html"><a href="chapter9.html#section9-2"><i class="fa fa-check"></i><b>9.2</b> The impact of simulated chronic nitrogen deposition on the biomass and N2-fixation activity of two boreal feather moss–cyanobacteria associations</a></li>
<li class="chapter" data-level="9.3" data-path="chapter9.html"><a href="chapter9.html#section9-3"><i class="fa fa-check"></i><b>9.3</b> Ants learn to rely on more informative attributes during decision-making</a></li>
<li class="chapter" data-level="9.4" data-path="chapter9.html"><a href="chapter9.html#section9-4"><i class="fa fa-check"></i><b>9.4</b> Multi-variate models are essential for understanding vertebrate diversification in deep time</a></li>
<li class="chapter" data-level="9.5" data-path="chapter9.html"><a href="chapter9.html#section9-5"><i class="fa fa-check"></i><b>9.5</b> General summary</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Second Semester Statistics Course with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chapter7" class="section level1">
<h1><span class="header-section-number">Chapter 7</span> Simple linear regression inference</h1>
<div id="section7-1" class="section level2">
<h2><span class="header-section-number">7.1</span> Model</h2>
<p>In Chapter <a href="chapter6.html#chapter6">6</a>, we learned how to estimate and interpret correlations and regression equations with a single predictor variable (<strong><em>simple linear regression</em></strong> or SLR). We carefully explored the variety of things that could go wrong and how to check for problems in regression situations. In this chapter, that work provides the basis of performing statistical inference that mainly focuses on the population slope coefficient based on the sample slope coefficient. As a reminder, the estimated regression model is <span class="math inline">\(\hat{y}_i = b_0 + b_1x_i\)</span>. the population regression equation is <span class="math inline">\(y_i = \beta_0 + \beta_1x_i + \epsilon_i\)</span>. Where <span class="math inline">\(\beta_0\)</span> is the <strong><em>population</em></strong> (or true) <strong><em>y-intercept</em></strong> and <span class="math inline">\(\beta_1\)</span> is the <strong><em>population</em></strong> (or true) <strong><em>slope coefficient</em></strong>. These are population parameters (fixed but typically unknown). This model can be re-written to think about different components and their roles. The mean of a random variable is statistically denoted as <span class="math inline">\(E(y_i)\)</span>, the <strong><em>expected value of</em></strong> <span class="math inline">\(\mathbf{y_i}\)</span>, or as <span class="math inline">\(\mu_{y_i}\)</span> and the mean of the response variable in a simple linear model is specified by <span class="math inline">\(E(y_i) = \mu_{y_i} = \beta_0 + \beta_1x_i\)</span>. This uses the true regression line to define the model for the mean of the responses as a function of the value of the explanatory variable.</p>
<p>The other part of any statistical model is specifying a model for the variability around the mean. There are two aspects to the variability to specify here – the shape of the distribution and the spread of the distribution. This is where the normal distribution and our “normality assumption” re-appears. And for normal distributions, we need to define a variance parameter, <span class="math inline">\(\sigma^2\)</span>. Combined, the complete regression model is</p>
<p><span class="math display">\[y_i \sim N(\mu_{y_i},\sigma^2), \text{ with } 
\mu_{y_i} = \beta_0 + \beta_1x_i,\]</span></p>
<p>which can be read as “y follows a normal distribution with mean mu-y and variance sigma-squared”. This also implies that the random variability around the true mean, the errors, follow a normal distribution with mean 0 and that same variance, <span class="math inline">\(\epsilon_i \sim N(0,\sigma^2)\)</span>. The true deviations (<span class="math inline">\(\epsilon_i\)</span>) are once again estimated by the residuals, <span class="math inline">\(e_i = y_i - \hat{y}_i\)</span> = observed response – predicted response. We can use the residuals to estimate <span class="math inline">\(\sigma\)</span>, which is also called the <strong><em>residual standard error</em></strong>, <span class="math inline">\(\hat{\sigma} = \sqrt{\Sigma e^2_i / (n-2)}\)</span>. We will find this quantity near the end of the regression output as discussed below so the formula is not heavily used here. This provides us with the three parameters that are estimated as part of our SLR model: <span class="math inline">\(\beta_0, \beta_1,\text{ and } \sigma\)</span>.</p>
<p>These definitions also formalize the assumptions implicit in the regression model:</p>
<ol style="list-style-type: decimal">
<li><p>The errors follow a normal distribution (<strong><em>Normality assumption</em></strong>).</p></li>
<li><p>The errors have the same variance (<strong><em>Constant variance assumption</em></strong>).</p></li>
<li><p>The observations are independent (<strong><em>Independence assumption</em></strong>).</p></li>
<li><p>The model for the mean is “correct” (<strong><em>Linearity, No Influential points, Only one group</em></strong>).</p></li>
</ol>
<p>The diagnostics described at the end of Chapter <a href="chapter6.html#chapter6">6</a> provide techniques for checking these assumptions – meeting these assumptions is fundamental to having a regression line that we trust and inferences from it that we also can trust.</p>
<p>To make this clearer, suppose that in the <em>Beers </em>and <em>BAC</em> study that they had randomly assigned 20 students to consume each number of beers. We would expect some variation in the <em>BAC</em> for each group of 20 at each level of <em>Beers</em> but that each group of observations will be centered at the true mean <em>BAC</em> for each number of <em>Beers</em>. The regression model assumes that the <em>BAC</em> values are normally distributed around the mean for each <em>Beer</em> level, <span class="math inline">\(\text{BAC}_i \sim N(\beta_0 + \beta_1\text{ Beers}_i,\sigma^2)\)</span>, with the mean defined by the regression equation. We actually do not need to obtain more than one observation at each <span class="math inline">\(x\)</span> value to make this assumption or assess it, but the plots below show you what this could look like. The sketch in Figure <a href="chapter7.html#fig:Figure7-1">7.1</a> attempts to show the idea of normal distributions that are centered at the true regression line, all with the same shape and variance that is an assumption of the regression model.</p>

<div class="figure" style="text-align: center"><span id="fig:Figure7-1"></span>
<img src="chapter7_files/image029.png" alt="Sketch of assumed normal distributions for the responses centered at the regression line."  />
<p class="caption">
Figure 7.1: Sketch of assumed normal distributions for the responses centered at the regression line.
</p>
</div>
<p>Figure <a href="chapter7.html#fig:Figure7-2">7.2</a> contains simulated realizations from a normal distribution of 20 subjects at each <em>Beer</em> level around the assumed true regression line with two different residual SEs of 0.02 and 0.06. The original BAC model has a residual SE of 0.02 but had many fewer observations at each <em>Beer</em> value.</p>

<div class="figure"><span id="fig:Figure7-2"></span>
<img src="07-simpleLinearRegressionInference_files/figure-html/Figure7-2-1.png" alt="Simulated data for Beers and BAC assuming two different residual standard errors (0.02 and 0.06)." width="672" />
<p class="caption">
Figure 7.2: Simulated data for Beers and BAC assuming two different residual standard errors (0.02 and 0.06).
</p>
</div>
<p>Along with getting the idea that regression models define normal distributions in the y-direction that are centered at the regression line, you can also get a sense of how variable samples from a normal distribution can appear. Each distribution of 20 subjects at each <span class="math inline">\(x\)</span> value came from a normal distribution but there are some of those distributions that might appear to generate small outliers and have slightly different variances. This can help us to remember to not be too particular when assessing assumptions and allow for some variability in spreads and a few observations from the tails of the distribution to occasionally arise.</p>
<p>In sampling from the population, we expect some amount of variability of each estimator around its true value. This variability leads to the potential variability in estimated regression lines (think of a suite of potential estimated regression lines that would be created by different random samples from the same population). Figure <a href="chapter7.html#fig:Figure7-3">7.3</a> contains the true regression line (bold, red) and realizations of the estimated regression line in simulated data based on results similar to the real data set.</p>

<div class="figure"><span id="fig:Figure7-3"></span>
<img src="chapter7_files/image034.png" alt="Variability in realized regression lines based on sampling variation."  />
<p class="caption">
Figure 7.3: Variability in realized regression lines based on sampling variation.
</p>
</div>
<p>This variability due to random sampling is something that needs to be properly accounted for to take the <strong>single</strong> estimated regression line to make inferences about the true line and parameters based on the sample-based estimates. The next sections develop those inferential tools.</p>
</div>
<div id="section7-2" class="section level2">
<h2><span class="header-section-number">7.2</span> Confidence Interval and Hypothesis tests for the slope and intercept</h2>
<p>Our inference techniques will resemble previous material with an interest in forming confidence intervals and doing hypothesis testing, although the interpretation of confidence intervals for slope coefficients take some extra care. Remember that the general form of any parametric confidence interval is</p>
<p><span class="math display">\[\text{estimate} \mp t^*\text{SE}_{estimate},\]</span></p>
<p>so we need to obtain the appropriate standard error for regression model coefficients and the degrees of freedom to define the <span class="math inline">\(t\)</span>-distribution to look up <span class="math inline">\(t^*\)</span>. We will find the <span class="math inline">\(\text{SE}_{b_0}\)</span> and <span class="math inline">\(\text{SE}_{b_1}\)</span> in the model summary. The degrees of freedom for the <span class="math inline">\(t\)</span>-distribution in simple linear regression are <span class="math inline">\(\mathbf{df=n-2}\)</span>. Putting this together, the confidence interval for the true y-intercept, <span class="math inline">\(\beta_0\)</span>, is <span class="math inline">\(\mathbf{b_0 \mp t^*_{n-2}}\textbf{SE}_{\mathbf{b_0}}\)</span> although this confidence interval is rarely of interest. The confidence interval that is almost always of interest is for the true slope coefficient, <span class="math inline">\(\beta_1\)</span>, that is <span class="math inline">\(\mathbf{b_1 \mp t^*_{n-2}}\textbf{SE}_{\mathbf{b_1}}\)</span>. The slope confidence interval is used to do two things: (1) inference for the amount of change in the mean of <span class="math inline">\(y\)</span> for a unit change in <span class="math inline">\(x\)</span> in the population and (2) to potentially do hypothesis testing by checking whether 0 is in the CI or not. The sketch in Figure <a href="chapter7.html#fig:Figure7-3b">7.4</a> illustrates the roles of the CI for the slope in terms of determining where the population slope coefficient might be – centered at the sample slope coefficient – our best guess for the true slope. This sketch informs an <strong><em>interpretation of the slope coefficient confidence interval</em></strong>:</p>

<div class="figure" style="text-align: center"><span id="fig:Figure7-3b"></span>
<img src="chapter7_files/image045.png" alt="Sketch of the role of the CI for slope in terms of determining where the population slope coefficient might be."  />
<p class="caption">
Figure 7.4: Sketch of the role of the CI for slope in terms of determining where the population slope coefficient might be.
</p>
</div>
<blockquote>
<p>For a 1 <strong><em>[units of X]</em></strong> increase in <strong>X</strong>, we are ___ % confident that the <strong>true change in the mean of</strong> <strong><em>Y</em></strong> will be between <strong>LL</strong> and <strong>UL</strong> <strong><em>[units of Y]</em></strong>.</p>
</blockquote>
<p>In this interpretation, LL and UL are the calculated lower and upper limits of the confidence interval. This builds on our previous interpretation of the slope coefficient, adding in the information about pinning down the true change (population change) in the mean of the response variable. The interpretation of the y-intercept CI is:</p>
<blockquote>
<p>For an <strong><em>x</em></strong> of 0 <strong><em>[units of X]</em></strong>, we are 95% confident that the true mean of <strong><em>Y</em></strong> will be between <strong>LL</strong> and <strong>UL</strong> <strong><em>[units of Y]</em></strong>.</p>
</blockquote>
<p>This is really only interesting if we’ll see a method for generating CIs for the true mean at potentially more interesting values of <span class="math inline">\(x\)</span> in Section <a href="chapter7.html#section7-7">7.8</a>. To trust the results from these confidence intervals, all the regression validity conditions need to be met (or at least close to met).</p>
<p>The only hypothesis test of interest in this situation is for the slope coefficient. To develop the hypotheses of interest in SLR, note the effect of having <span class="math inline">\(\beta_1=0\)</span> in the mean of the regression equation, <span class="math inline">\(\mu_{y_i} = \beta_0 + \beta_1x_i = \beta_0 + 0x_i = \beta_0\)</span>. This is the “intercept-only” or “mean-only” model that suggests that the mean of <span class="math inline">\(y\)</span> does not vary with different values of <span class="math inline">\(x\)</span> as it is always <span class="math inline">\(\beta_0\)</span>. We saw this model in the ANOVA material as the reduced model when the null hypothesis of no difference in the true means across the groups was true. Here, this is the same as saying that there is no linear relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, or that <span class="math inline">\(x\)</span> is of no use in predicting <span class="math inline">\(y\)</span>, or that we make the same prediction for <span class="math inline">\(y\)</span> for every value of <span class="math inline">\(x\)</span>. Thus</p>
<p><span class="math display">\[\boldsymbol{H_0: \beta_1=0}\]</span></p>
<p>is a test for <strong>no linear relationship between</strong> <span class="math inline">\(\mathbf{x}\)</span> <strong>and</strong> <span class="math inline">\(\mathbf{y}\)</span> <strong>in the population</strong>. The alternative of <span class="math inline">\(\boldsymbol{H_A: \beta_1\ne 0}\)</span> that there is <strong>some</strong> linear relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> in the population, is our main test of interest in these situations. It is also possible to test greater than or less than alternatives in certain situations.</p>
<p>Test statistics for regression coefficients are developed, if assumptions are met, using the <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n-2\)</span> degrees of freedom. The <span class="math inline">\(t\)</span>-test statistic is generally</p>
<p><span class="math display">\[t=\frac{b_i}{\text{SE}_{b_i}}\]</span></p>
<p>with the main interest in the test for <span class="math inline">\(\beta_1\)</span> based on <span class="math inline">\(b_1\)</span> for now. The p-value would be calculated using the two-tailed area from the <span class="math inline">\(t_{n-2}\)</span> distribution calculated using the <code>pt</code> function. The p-value to test these hypotheses is also provided in the model summary as we will see below.</p>
<p>The greater than or less than alternatives can have interesting interpretations in certain situations. For example, the greater than alternative <span class="math inline">\(\left(\boldsymbol{H_A: \beta_1 &gt; 0}\right)\)</span> tests an alternative of a positive linear relationship, with the p-value extracted just from the right tail of the same <span class="math inline">\(t\)</span>-distribution. This could be used when a researcher would only find a result “interesting” if a positive relationship is detected, such as in the study of tree height and tree diameter where a researcher might be justified in deciding to test only for a positive linear relationship. Similarly, the left-tailed alternative is also possible, <span class="math inline">\(\boldsymbol{H_A: \beta_1 &lt; 0}\)</span>. To get one-tailed p-values from two-tailed results (the default), first check that the observed test statistic is in the direction of the alternative (<span class="math inline">\(t&gt;0\)</span> for <span class="math inline">\(H_A:\beta_1&gt;0\)</span> or <span class="math inline">\(t&lt;0\)</span> for <span class="math inline">\(H_A:\beta_1&lt;0\)</span>). <strong>If these conditions are met, then the p-value for the one-sided test from the two-sided version is found by dividing the reported p-value by 2</strong>. If <span class="math inline">\(t&gt;0\)</span> for <span class="math inline">\(H_A:\beta_1&gt;0\)</span> or <span class="math inline">\(t&lt;0\)</span> for <span class="math inline">\(H_A:\beta_1&lt;0\)</span> are not met, then the p-value would be greater than 0.5 and it would be easiest to look it up directly using <code>pt</code>.</p>
<p>We can revisit a couple of examples for a last time with these ideas in hand to complete the analyses.</p>
<ul>
<li>For the <em>Beers, BAC</em> data, the 95% confidence for the true slope coefficient, <span class="math inline">\(\beta_1\)</span>, is</li>
</ul>
<p><span class="math display">\[\begin{array}
  \boldsymbol{b_1 \mp t^*_{n-2}} \textbf{SE}_{\boldsymbol{b_1}}
  &amp; \boldsymbol{= 0.01796 \mp 2.144787 * 0.002402} \\
  &amp; \boldsymbol{= 0.01796 \mp 0.00515} \\
  &amp; \boldsymbol{\rightarrow (0.0128, 0.0231).}
  \end{array}\]</span></p>
<p>You can find the components of this calculation in the model summary and from <code>qt(0.975, df=n-2)</code> which was 2.145 for the <span class="math inline">\(t^*\)</span>-multiplier. Be careful not to use the <span class="math inline">\(t\)</span>-value of 7.48 in the model summary to make confidence intervals – that is the test statistic used below. The related calculations are shown at the bottom of the following code:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m1 &lt;-<span class="st"> </span><span class="kw">lm</span>(BAC<span class="op">~</span>Beers, <span class="dt">data=</span>BB)
<span class="kw">summary</span>(m1)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = BAC ~ Beers, data = BB)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.027118 -0.017350  0.001773  0.008623  0.041027 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) -0.012701   0.012638  -1.005    0.332
## Beers        0.017964   0.002402   7.480 2.97e-06
## 
## Residual standard error: 0.02044 on 14 degrees of freedom
## Multiple R-squared:  0.7998, Adjusted R-squared:  0.7855 
## F-statistic: 55.94 on 1 and 14 DF,  p-value: 2.969e-06</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qt</span>(<span class="fl">0.975</span>, <span class="dt">df=</span><span class="dv">14</span>) <span class="co"># t* multiplier for 95% CI</span></code></pre></div>
<pre><code>## [1] 2.144787</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="fl">0.017964</span> <span class="op">+</span><span class="st"> </span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="op">*</span><span class="kw">qt</span>(<span class="fl">0.975</span>, <span class="dt">df=</span><span class="dv">14</span>)<span class="op">*</span><span class="fl">0.002402</span></code></pre></div>
<pre><code>## [1] 0.01281222 0.02311578</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qt</span>(<span class="fl">0.975</span>, <span class="dt">df=</span><span class="dv">14</span>)<span class="op">*</span><span class="fl">0.002402</span></code></pre></div>
<pre><code>## [1] 0.005151778</code></pre>
<p>We can also get the confidence interval directly from the <code>confint</code> function run on our regression model, saving some calculation effort and providing both the CI for the y-intercept and the slope coefficient.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(m1)</code></pre></div>
<pre><code>##                   2.5 %     97.5 %
## (Intercept) -0.03980535 0.01440414
## Beers        0.01281262 0.02311490</code></pre>
<p>We interpret the 95% CI for the slope coefficient as follows: For a 1 <strong>beer</strong> increase in number of beers consumed, we are 95% confident that the <strong>true</strong> change in the <strong>mean</strong> <em>BAC</em> will be between 0.0128 and 0.0231 g/dL. While the estimated slope is our best guess of the impacts of an extra beer consumed based on our sample, this CI provides information about the likely range of potential impacts on the mean in the population. It also could be used to test the two-sided hypothesis test and would suggest that we should reject the null hypothesis since the confidence interval does not contain 0.</p>
<p>The width of the CI, loosely the precision of the estimated slope, is impacted by the variability of the observations around the estimated regression line, the overall sample size, and the positioning of the x-observations. Basically all those aspects relate to how “clearly” known the regression line is and that determines the estimated precision in the slope. For example, the more variability around the line that is present, the more uncertainty there is about the correct line to use (Least Squares (LS) can still find an estimated line but there are other lines that might be “close” to its optimizing choice). Similarly, more observations help us a better estimate of the mean – an idea that permeates all statistical methods. Finally, the location of x-values can impact the precision in a slope coefficient. We’ll revisit this in the context of <strong><em>multi-collinearity</em></strong> in the next chapter, and often we have no control of x-values, but just note that different patterns of x-values can lead to different precision of estimated slope coefficients<a href="#fn60" class="footnoteRef" id="fnref60"><sup>60</sup></a>.</p>
<p>For hypothesis testing, we will almost always stick with two-sided tests in regression modeling as it is a more conservative approach and does not require us to have an expectation of a direction for relationships <em>a priori</em>. In this example, the null hypothesis for the slope coefficient is that there is no linear relationship between <em>Beers</em> and <em>BAC</em> in the population. The alternative hypothesis is that there is some linear relationship between <em>Beers</em> and <em>BAC</em> in the population. The test statistic is <span class="math inline">\(t=0.01796/0.002402 =7.48\)</span> which, if assumptions hold, follows a <span class="math inline">\(t(14)\)</span> distribution. The model summary provides the calculation of the test statistic and the two-sided test p-value of <span class="math inline">\(2.97\text{e-6} = 0.00000297\)</span>. So we would just report p-value &lt; 0. 0001. This suggests we should reject the null hypothesis and conclude that there is evidence at the 5% significance level of a linear relationship between <em>Beers</em> and <em>BAC</em> in the population. Because of the random assignment, we can also say that drinking beers causes changes in BAC but, because the sample was of volunteers, we cannot infer that these results would hold in the general population of OSU students or more generally.</p>
<p>There are also results for the y-intercept in the output. The 95% CI is from -0.0398 to 0.0144, that the true mean <em>BAC</em> for a 0 beer consuming subject is between -0.0398 to 0.01445. This is really not a big surprise but possibly is comforting to know that these results would fail to reject the null hypothesis that the true mean <em>BAC</em> for 0 <em>Beers</em> is 0. Finding no evidence of a difference from 0 makes sense and makes the estimated y-intercept of -0.013 not so problematic. In other situations, the results for the y-intercept may be more illogical but this will often be because the y-intercept is extrapolating far beyond the scope of observations. The y-intercept’s main function in regression models is to be at the right level for the slope to “work” to make a line that describes the responses and thus is usually of lesser interest.</p>
<p>As a second example, we can revisit modeling the <em>Hematocrit</em> of female Australian athletes as a function of <em>body fat %</em>. The sample size is <span class="math inline">\(n=99\)</span> so the <em>df</em> are 97 in the analysis. In Chapter <a href="chapter6.html#chapter6">6</a>, the relationship between <em>Hematocrit</em> and <em>body fat %</em> for females appeared to be a weak negative linear association. The 95% confidence interval for the slope is -0.187 to 0.0155. For a 1 % increase in body fat %, we are 95% confident that the change in the true mean Hematocrit is between -0.187 and 0.0155 % of blood. This suggests that we would fail to reject the null hypothesis of no linear relationship at the 5% significance level because this CI contains 0 – we can’t reject the null that the true slope is 0. In fact the p-value is 0.0965 which is larger than 0.05 which provides a consistent conclusion with using the 95% confidence interval to perform a hypothesis test. Either way, we would conclude that there is not enough evidence at the 5% significance level to conclude that there is some linear relationship between bodyfat and Hematocrit in the population of female Australian athletes. If your standards were different, say if you had elected to test at the 10% significance level, you might have a different opinion about the evidence against the null hypothesis here. For this reason, we sometimes interpret this sort of marginal result as having some evidence against the null but certainly not strong evidence.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(alr3)
<span class="kw">data</span>(ais)
aisR2&lt;-ais[<span class="op">-</span><span class="kw">c</span>(<span class="dv">56</span>,<span class="dv">166</span>),<span class="kw">c</span>(<span class="st">&quot;Ht&quot;</span>,<span class="st">&quot;Hc&quot;</span>,<span class="st">&quot;Bfat&quot;</span>,<span class="st">&quot;Sex&quot;</span>)]
m2 &lt;-<span class="st"> </span><span class="kw">lm</span>(Hc<span class="op">~</span>Bfat, <span class="dt">data=</span>aisR2[aisR2<span class="op">$</span>Sex<span class="op">==</span><span class="dv">1</span>,]) <span class="co"># Results for Females </span>
<span class="kw">summary</span>(m2)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Hc ~ Bfat, data = aisR2[aisR2$Sex == 1, ])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -5.2399 -2.2132 -0.1061  1.8917  6.6453 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) 42.01378    0.93269  45.046   &lt;2e-16
## Bfat        -0.08504    0.05067  -1.678   0.0965
## 
## Residual standard error: 2.598 on 97 degrees of freedom
## Multiple R-squared:  0.02822,    Adjusted R-squared:  0.0182 
## F-statistic: 2.816 on 1 and 97 DF,  p-value: 0.09653</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(m2)</code></pre></div>
<pre><code>##                  2.5 %      97.5 %
## (Intercept) 40.1626516 43.86490713
## Bfat        -0.1856071  0.01553165</code></pre>
<p>One more worked example is provided from the Montana fire data. In this example pay particular attention to how we are handling the units of the response variable, log-hectares, and to the changes to doing inferences at the 1% significance and 99% confidence levels, and where you can find the needed results in the following output:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mtfires &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;http://www.math.montana.edu/courses/s217/documents/climateR2.csv&quot;</span>)
mtfires<span class="op">$</span>loghectacres &lt;-<span class="st"> </span><span class="kw">log</span>(mtfires<span class="op">$</span>hectacres)
fire1 &lt;-<span class="st"> </span><span class="kw">lm</span>(loghectacres<span class="op">~</span>Temperature, <span class="dt">data=</span>mtfires)
<span class="kw">summary</span>(fire1)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = loghectacres ~ Temperature, data = mtfires)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.0822 -0.9549  0.1210  1.0007  2.4728 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) -69.7845    12.3132  -5.667 1.26e-05
## Temperature   1.3884     0.2165   6.412 2.35e-06
## 
## Residual standard error: 1.476 on 21 degrees of freedom
## Multiple R-squared:  0.6619, Adjusted R-squared:  0.6458 
## F-statistic: 41.12 on 1 and 21 DF,  p-value: 2.347e-06</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(fire1, <span class="dt">level=</span><span class="fl">0.99</span>)</code></pre></div>
<pre><code>##                    0.5 %     99.5 %
## (Intercept) -104.6477287 -34.921286
## Temperature    0.7753784   2.001499</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qt</span>(<span class="fl">0.995</span>, <span class="dt">df=</span><span class="dv">21</span>)</code></pre></div>
<pre><code>## [1] 2.83136</code></pre>
<ul>
<li><p>Based on the estimated regression model, we can say that if the average temperature is 0, we expect that, on average, the log-area burned would be -69.8 log-hectares.</p></li>
<li><p>From the regression model summary, <span class="math inline">\(b_1=1.39\)</span> with <span class="math inline">\(\text{SE}_{b_1}=0.2165\)</span> and <span class="math inline">\(\mathbf{t=6.41}\)</span></p></li>
<li><p>There were <span class="math inline">\(n=23\)</span> measurements taken, so <span class="math inline">\(\mathbf{df=n-2=23-3=21}\)</span></p></li>
<li><p>Suppose that we want to test for a linear relationship between temperature and log-hectares burned:</p>
<p><span class="math display">\[H_0: \beta_1=0\]</span></p>
<ul>
<li>In words, the true slope coefficient between <em>Temperature</em> and <em>log-area burned</em> is 0 OR there is no linear relationship between <em>Temperature</em> and <em>log-area burned</em> in the population.</li>
</ul>
<p><span class="math display">\[H_A: \beta_1\ne 0\]</span></p>
<ul>
<li>In words, the alternative states that the true slope coefficient between <em>Temperature</em> and <em>log-area burned</em> is not 0 OR there is a linear relationship between <em>Temperature</em> and <em>log-area burned</em> in the population.</li>
</ul></li>
</ul>
<p>Test statistic: <span class="math inline">\(t = 1.39/0.217 = 6.41\)</span></p>
<ul>
<li>Assuming the null hypothesis to be true (no linear relationship), the <span class="math inline">\(t\)</span>-statistic follows a <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n-2 = 23-2=21\)</span> degrees of freedom.</li>
</ul>
<p>p-value:</p>
<ul>
<li><p>From the model summary, the <strong>p-value is</strong> <span class="math inline">\(\mathbf{2.35*10^{-6}}\)</span></p>
<ul>
<li>Interpretation: There is less than a 0.01% chance that we would observe slope coefficient like we did or something more extreme (greater than 1.39 log(hectares)/<span class="math inline">\(^\circ F\)</span>) if there were in fact no linear relationship between temperature (<span class="math inline">\(^\circ F\)</span>) and log-area burned (log-hectares) in the population.</li>
</ul></li>
</ul>
<p>Decision: At the 1% significance level (<span class="math inline">\(\alpha=0.01\)</span>), the p-value is less than <span class="math inline">\(\alpha\)</span>, so reject <span class="math inline">\(H_0\)</span>.</p>
<p>Conclusion: There is strong evidence to reject the null hypothesis of no linear relationship and conclude that there is, in fact, a linear relationship between Temperature and log(Hectares) burned. Since we have a time series of results, our inferences pertain to the results we could have observed for these years but not for years we did not observe – so just for the true slope for this sample of years. Because we can’t randomly assign the amount of area burned, we cannot make causal inferences – there are many reasons why both the average temperature and area burned would vary together that would not involve a direct connection between them.</p>
<p><span class="math display">\[\text{99% CI for } \beta_1: \boldsymbol{b_1 \mp
t^*_{n-2}}\textbf{SE}_{\boldsymbol{b_1}} \rightarrow 1.39 \mp 2.831\bullet 0.217
\rightarrow (0.78, 2.00)\]</span></p>
<p>Interpretation of 99% CI for slope coefficient:</p>
<ul>
<li><p>For a 1 degree F increase in <em>Temperature</em>, we are 99% confident that the change in the true mean log-area burned is between 0.78 and 2.00 log(Hectares).</p></li>
<li><p>Another way to interpret this is:</p>
<ul>
<li>For a 1 degree F increase in <em>Temperature</em>, we are 99% confident that the mean Area Burned will change by between 0.78 and 2.00 log(Hectares) <strong>in the population</strong>.</li>
</ul></li>
<li><p>Also <span class="math inline">\(R^2\)</span> is 66.2%, which tells us that <em>Temperature</em> explains 66.2% of the variation in <em>log(Hectares) burned</em>. Or that the linear regression model built using <em>Temperature</em> explains 66.2% of the variation in yearly <em>log(Hectares) burned</em>.</p></li>
</ul>
</div>
<div id="section7-3" class="section level2">
<h2><span class="header-section-number">7.3</span> Bozeman temperature trend</h2>
<p>For a new example, consider the yearly average maximum temperatures in Bozeman, MT. For over 100 years, daily measurements have been taken of the minimum and maximum temperatures at hundreds of weather stations across the US. In early years, this involved manual recording of the temperatures and resetting the thermometer to track the extremes for the following day. More recently, these measures have been replaced by digital temperature recording devices that continue to track this sort of information with much less human effort and, possibly, errors. This sort of information is often aggregated to monthly or yearly averages to be able to see “on average” changes from month-to-month or year-to-year as opposed to the day-to-day variation in the temperature - something that we are all too familiar with in our part of the country (see <a href="http://fivethirtyeight.com/features/which-city-has-the-most-unpredictable-weather/" class="uri">http://fivethirtyeight.com/features/which-city-has-the-most-unpredictable-weather/</a> for an interesting discussion of weather variability where Great Falls, MT had a very high rating on “unpredictability”). Often the local information is aggregated further to provide regional, hemispheric, or even global average temperatures. Climate change research involves attempting to quantify the changes over time in these sorts of records.</p>
<p>These data were extracted from the National Oceanic and Atmospheric Administration’s National Centers for Environmental Information’s database (<a href="http://www.ncdc.noaa.gov/cdo-web/" class="uri">http://www.ncdc.noaa.gov/cdo-web/</a>) and we will focus on the yearly average of the monthly averages of the daily maximum temperature (we can call them yearly average maximum temperatures but note that it was a little more complicated than that to arrive at the response variable) in Bozeman in degrees F from 1901 to 2014.</p>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">bozemantemps &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;http://www.math.montana.edu/courses/s217/documents/BozemanMeanMax.csv&quot;</span>)
<span class="kw">summary</span>(bozemantemps)</code></pre></div>
<pre><code>##     meanmax           Year     
##  Min.   :49.75   Min.   :1901  
##  1st Qu.:53.97   1st Qu.:1930  
##  Median :55.43   Median :1959  
##  Mean   :55.34   Mean   :1958  
##  3rd Qu.:57.02   3rd Qu.:1986  
##  Max.   :60.05   Max.   :2014</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">length</span>(bozemantemps<span class="op">$</span>Year) <span class="co">#Some years are missing (1905, 1906, 1948, 1950,1995)</span></code></pre></div>
<pre><code>## [1] 109</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(car)
<span class="kw">scatterplot</span>(meanmax<span class="op">~</span>Year, <span class="dt">data=</span>bozemantemps, 
            <span class="dt">ylab=</span><span class="st">&quot;Mean Maximum Temperature (degrees F)&quot;</span>, <span class="dt">spread=</span>F,
            <span class="dt">main=</span><span class="st">&quot;Scatterplot of Bozeman Yearly Average Max Temperatures&quot;</span>)</code></pre></div>
<div class="figure"><span id="fig:Figure7-4"></span>
<img src="07-simpleLinearRegressionInference_files/figure-html/Figure7-4-1.png" alt="Scatterplot of average yearly maximum temperatures in Bozeman from 1900 to 2014." width="768" />
<p class="caption">
Figure 7.5: Scatterplot of average yearly maximum temperatures in Bozeman from 1900 to 2014.
</p>
</div>
<p>The scatterplot in Figure <a href="chapter7.html#fig:Figure7-4">7.5</a> shows the results between 1901 and 2014 based on a sample of <span class="math inline">\(n=109\)</span> years because four years had too many missing months to fairly include in the responses. Missing values occur for many reasons and in this case were likely just machine or human error<a href="#fn61" class="footnoteRef" id="fnref61"><sup>61</sup></a>. These are time series data and in time series analysis we assume that the population of interest for inference is all possible realizations from the underlying process over this timeframe even though we only ever get to observe one realization. In terms of climate change research, we would want to (a) assess evidence for a trend over time (hopefully assessing whether any observed trend is clearly different from a result that could have been observed by chance if there really is no change over time in the true process) and (b) quantify the size of the change over time along with the uncertainty in that estimate relative to the underlying true mean change over time. The hypothesis test for the slope answers (a) and the confidence interval for the slope addresses (b). We also should be concerned about problematic (influential) points, changing variance, and potential nonlinearity in the trend over time causing problems for the SLR inferences. The scatterplot suggests that there is a moderate or strong positive linear relationship between <em>temperatures</em> and <em>year</em> with some “wiggles” in the smoothing line at the beginning and end of the record. Smoothing lines can become quite untrustworthy at the edges of the data set, so discount the curving at the edges a bit. If the curving is real, it would suggest a less steep change before 1920, relatively linear change from 1930 to 1970, a small increase in slope through the mid-90s, and then a leveling off after that point. There also appears to be one potential large outlier in the late 1930s.</p>
<p>We’ll perform all 6+ steps of the hypothesis test for the slope coefficient and add a confidence interval interpretation for this example. First, we have to decide on our significance level (5% is a typical choice), our hypotheses (the 2-sided test would be a <strong><em>conservative</em></strong> choice and no one that does climate change research wants to be accused of taking a <strong><em>liberal</em></strong> approach in their analyses<a href="#fn62" class="footnoteRef" id="fnref62"><sup>62</sup></a>) and our test statistic, <span class="math inline">\(t=\frac{b_1}{\text{SE}_{b_1}}\)</span>.</p>
<ol style="list-style-type: decimal">
<li><strong>Hypotheses for the slope coefficient test:</strong></li>
</ol>
<p><span class="math display">\[H_0: \beta_1=0 \text{ vs } H_A: \beta_1 \ne 0\]</span> 2. <strong>Validity conditions:</strong></p>
<ul>
<li><p><strong>Quantitative variables condition</strong></p>
<ul>
<li>Both <code>Year</code> and yearly average <code>Temperature</code> are quantitative variables so are suitable for an SLR analysis.</li>
</ul></li>
<li><p><strong>Independence of observations</strong></p>
<ul>
<li>There may be a lack of independence among years since a warm year might be followed by another warmer than average year. It would take more sophisticated models to account for this and the standard error on the slope coefficient could either get larger or smaller depending on the type of <strong><em>autocorrelation</em></strong> (correlation between neighboring time points or correlation with oneself at some time lag) present. This creates a caveat on these results but this model is often the first one researchers fit in these situations and often is reasonably correct even in the presence of some autocorrelation.</li>
</ul>
<p>To assess the remaining conditions, we need to fit the regression model and use the diagnostic plots in Figure <a href="chapter7.html#fig:Figure7-5">7.6</a> to aid our assessment:</p>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">temp1 &lt;-<span class="st"> </span><span class="kw">lm</span>(meanmax<span class="op">~</span>Year, <span class="dt">data=</span>bozemantemps)
<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))
<span class="kw">plot</span>(temp1, <span class="dt">add.smooth=</span>F)</code></pre></div>
<div class="figure"><span id="fig:Figure7-5"></span>
<img src="07-simpleLinearRegressionInference_files/figure-html/Figure7-5-1.png" alt="Diagnostic plots of the Bozeman yearly temperature simple linear regression model." width="672" />
<p class="caption">
Figure 7.6: Diagnostic plots of the Bozeman yearly temperature simple linear regression model.
</p>
</div></li>
<li><p><strong>Linearity of relationship</strong></p>
<ul>
<li><p>Examine the Residuals vs Fitted plot:</p>
<ul>
<li>There does not appear to be a clear curve remaining in the residuals so that initial curving in the smoothing line is not clearly showing up in the diagnostics so we should be able to proceed without worrying too much about the slight nonlinearity detected in the initial scatterplot.</li>
</ul></li>
</ul></li>
<li><p><strong>Equal (constant) variance</strong></p>
<ul>
<li>Examining the Residuals vs Fitted and the “Scale-Location” plots provide little to no evidence of changing variance. The variability does decrease slightly in the middle fitted values but those changes are really minor and present no real evidence of changing variability.</li>
</ul></li>
<li><p><strong>Normality of residuals</strong></p>
<ul>
<li>Examining the Normal QQ-plot for violations of the normality assumption shows only one real problem in the outlier from the 32<sup>nd</sup> observation in the data set (1934) which was flagged as a large outlier in the original scatterplot. We should be careful about inferences that assume normality and contain this point in the analysis. We might consider running the analysis with it and without that point to see how much it impacts the results just to be sure it isn’t creating evidence of a trend because of a violation of the normality assumption. The next check reassures us that re-running the model without this point would only result in slightly changing the SEs and not the slopes.</li>
</ul></li>
<li><p><strong>No influential points:</strong></p>
<ul>
<li><p>There are no influential points displayed in the Residuals vs Leverage plot since the Cook’s D contours are not displayed.</p>
<ul>
<li>Note: by default this plot contains a smoothing line that is relatively meaningless, so ignore it if is displayed. We suppressed it using the <code>add.smooth=F</code> option in <code>plot(temp1)</code> but if you forget to do that, just ignore the smoothers in the diagnostic plots especially in the Residuals vs Leverage plot.</li>
</ul></li>
<li><p>This results tells us that the outlier was not influential. If you look back at the scatterplot, it was located near the middle of the observed <span class="math inline">\(x\text{&#39;s}\)</span> so its potential leverage is low. You can find its leverage based on the plot to be around 0.12 when there are observations in the data set with leverages over 0.3. The high leverage points occur at the beginning and the end of the record because they are at the edges of the observed <span class="math inline">\(x\text{&#39;s}\)</span> and most of these points follow the overall pattern fairly well.</p></li>
</ul>
<p>So the main issues are with independence of observations and one non-influential outlier that might be compromising our normality assumption a bit.</p></li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li><p><strong>Calculate the test statistic:</strong></p>
<p><span class="math inline">\(t=0.05244/0.00476 = 11.02\)</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(temp1)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = meanmax ~ Year, data = bozemantemps)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.3779 -0.9300  0.1078  1.1960  5.8698 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) -47.35123    9.32184   -5.08 1.61e-06
## Year          0.05244    0.00476   11.02  &lt; 2e-16
## 
## Residual standard error: 1.624 on 107 degrees of freedom
## Multiple R-squared:  0.5315, Adjusted R-squared:  0.5271 
## F-statistic: 121.4 on 1 and 107 DF,  p-value: &lt; 2.2e-16</code></pre></li>
<li><p>Find the p-value:</p></li>
</ol>
<ul>
<li><p>From the model summary: p-value &lt; 2e-16 or just &lt; 0.0001</p></li>
<li><p>The test statistic is assumed to follow a <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n-2=109-2=107\)</span> degrees of freedom. The p-value can be calculated as:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="dv">2</span><span class="op">*</span><span class="kw">pt</span>(<span class="fl">11.02</span>, <span class="dt">df=</span><span class="dv">107</span>, <span class="dt">lower.tail=</span>F)</code></pre></div>
<pre><code>## [1] 2.498481e-19</code></pre></li>
<li><p>Which is then reported as &lt; 0.0001, which means that the chances of observing a slope coefficient as extreme or more extreme than 0.052 if the null hypothesis of no linear relationship is true is less than 0.01%.</p></li>
</ul>
<ol start="5" style="list-style-type: decimal">
<li><p><strong>Make a decision:</strong></p>
<p>Reject the null hypothesis because the p-value is less than 0.05.</p></li>
<li><p><strong>Write a conclusion:</strong></p>
<p>There is strong evidence against the null hypothesis of no linear relationship between <em>Year</em> and yearly mean <em>Temperature</em> so we can conclude that there is, in fact, some linear relationship between <em>Year</em> and yearly mean maximum <em>Temperature</em> in Bozeman. We can conclude that this detected trend pertains to the Bozeman area in the years 1901 to 2014 but not outside of this area or time frame. We cannot say that time caused the observed changes since it was not randomly assigned and we cannot attribute the changes to any other factors because we did not consider them. But knowing that there was a trend toward increasing temperatures is an intriguing first step in a more complete analysis of changing climate in the area.</p></li>
</ol>
<p>It is also good to report the percentage of variation that the model explains: <em>Year</em> explains 54.91% of the variation in yearly average maximum <em>Temperature</em>. If this value had been very small, we might discount the previous result. Since it is moderately large, that suggests that just by using a linear trend over time we can account for quite a bit of the variation in yearly average maximum temperatures in Bozeman. Note that the percentage of variation explained would get much worse if we tried to analyze the monthly or original daily maximum temperature data.</p>
<p>Interpreting a confidence interval provides more useful information than the hypothesis test here – instead of just assessing evidence against the null hypothesis, we can actually provide our best guess at the true change in the mean of <span class="math inline">\(y\)</span> for a change in <span class="math inline">\(x\)</span>. Here, the 95% CI is (0.043, 0.062). This tells us that for a 1 year increase in change in the true mean of the yearly average maximum <em>Temperatures</em> in Bozeman is between 0.043 and 0.062 degrees F.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(temp1)</code></pre></div>
<pre><code>##                    2.5 %       97.5 %
## (Intercept) -65.83068375 -28.87177785
## Year          0.04300681   0.06187746</code></pre>
<p>Sometimes the scale of the x-variable makes interpretation a little difficult, so we can re-scale it to make it more interpretable. One option is to re-scale the variable and re-fit the regression model and the other (easier) option is to re-scale our interpretation. The idea here is that a 100-year change might be easier and more meaningful scale to interpret than a single year change. If we have a slope in the model of 0.052 (for a 1 year change), we can also say that a 100 year change in the mean is estimated to be 0.052*100 = 0.52<span class="math inline">\(^\circ F\)</span>. Similarly, the 95% CI for the population mean 100-year change would be from 0.43<span class="math inline">\(^\circ F\)</span> to 0.62<span class="math inline">\(^\circ F\)</span>. In 2007, the IPCC (Intergovernmental Panel on Climate Change; <a href="http://www.ipcc.ch/publications_and_data/ar4/wg1/en/tssts-3-1-1.html" class="uri">http://www.ipcc.ch/publications_and_data/ar4/wg1/en/tssts-3-1-1.html</a>) estimated the global temperature change from 1906 to 2005 to be 0.74<span class="math inline">\(^\circ C\)</span> per decade or, scaled up, 7.4<span class="math inline">\(^\circ C\)</span> per century (1.33<span class="math inline">\(^\circ F\)</span>). There are many reasons why our local temperature trend might differ, including that our analysis was of average maximum temperatures and the IPCC was considering the average temperature (which was not measured locally or in most places in a good way until digital instrumentation was installed) and that local trends are likely to vary around the global average change based on localized environmental conditions.</p>
<p>One issue that arises in local studies of climate change is that researchers often consider these sorts of tests at many locations and on many response variables (if I did the maximum temperature, why not also do the same analysis of the minimum temperature time series as well? And if I did the analysis for Bozeman, what about Butte and Helena and…?). Remember our discussion of multiple testing issues in an ANOVA context when we compared lots of groups? This issue can arise when regression modeling is repeated in many similar data sets, say different sites or different response variables or both, in one study. Moore, Harper, and Greenwood (2007) considered the impacts on the assessment of evidence of trends of earlier spring onset timing in the Mountain West when the number of tests across many sites is accounted for. We found that the evidence for time trends decreases substantially but does not disappear. In a related study, Greenwood, Harper, and Moore (2011) found evidence for regional trends to earlier spring onset using more sophisticated statistical models. The main point here is to <strong>be careful when using simple statistical methods repeatedly if you are not accounting for the number of tests performed</strong><a href="#fn63" class="footnoteRef" id="fnref63"><sup>63</sup></a>.</p>
<p>Along with the confidence interval, we can also plot the estimated model (Figure <a href="chapter7.html#fig:Figure7-6">7.7</a> using a term-plot from the <code>effects</code> package (Fox, 2003). This is the same function we used for visualizing results in the ANOVA models and in its basic application you just need <code>plot(allEffects(modelname))</code> but we enhanced our version a little. In regression models, we get to see the regression line along with bounds for 95% confidence intervals for the mean at every value of <span class="math inline">\(x\)</span> that was observed (explained in next section). Note that there is also a rugplot on the x-axis showing you where values of the explanatory variable were obtained, which is useful to understanding how much information is available for different aspects of the line. Here it provides gaps for missing years of observations as sort of broken teeth in a comb.</p>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(effects)
<span class="kw">plot</span>(<span class="kw">allEffects</span>(temp1, <span class="dt">xlevels=</span><span class="kw">list</span>(<span class="dt">Year=</span>bozemantemps<span class="op">$</span>Year)),
     <span class="dt">grid=</span>T)</code></pre></div>
<div class="figure"><span id="fig:Figure7-6"></span>
<img src="07-simpleLinearRegressionInference_files/figure-html/Figure7-6-1.png" alt="Term-plot for the Bozeman mean yearly maximum temperature linear regression model with 95% confidence interval bands for the mean in each year." width="672" />
<p class="caption">
Figure 7.7: Term-plot for the Bozeman mean yearly maximum temperature linear regression model with 95% confidence interval bands for the mean in each year.
</p>
</div>
<p>If we extended the plot for the model to <code>Year</code>=0, we could see the reason that the y-intercept in this model is -47.4<span class="math inline">\(^\circ F\)</span>. This is obviously a large extrapolation for these data and provides a silly result. However, in paleoclimate data that goes back thousands of years using tree rings, ice cores, or sea sediments, the estimated mean in year 0 might be interesting and within the scope of observed values. It all depends on the application.</p>
<p>To make the y-intercept more interesting for our data set, we can re-scale the <span class="math inline">\(x\text{&#39;s}\)</span> before we fit the model to have the first year in the data set (1901) be “0”. This is accomplished by calculating <span class="math inline">\(\text{Year2} = \text{Year}-1901\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">bozemantemps<span class="op">$</span>Year2 &lt;-<span class="st"> </span>bozemantemps<span class="op">$</span>Year <span class="op">-</span><span class="st"> </span><span class="dv">1901</span>
<span class="kw">summary</span>(bozemantemps<span class="op">$</span>Year2)</code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    0.00   29.00   58.00   57.27   85.00  113.00</code></pre>
<p>The new estimated regression equation is <span class="math inline">\(\widehat{\text{Temp}}_i = 52.34 + 0.052*\text{Year2}_i\)</span>. The slope and its test statistic are the same as in the previous model. The y-intercept has changed dramatically with a 95% from 51.72<span class="math inline">\(^\circ F\)</span> to 52.96<span class="math inline">\(^\circ F\)</span> for <code>Year2</code>=0. But we know that <code>Year2</code> has a 0 value for 1901 because of our subtraction. That means that this CI is for the true mean in 1901 and is now at least somewhat interesting. If you revisit Figure <a href="chapter7.html#fig:Figure7-6">7.7</a> you will actually see that the displayed confidence intervals provide upper and lower bounds that match this result for 1901 – the y-intercept CI matches the 95% CI for the true mean.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">temp2 &lt;-<span class="st"> </span><span class="kw">lm</span>(meanmax<span class="op">~</span>Year2, <span class="dt">data=</span>bozemantemps)
<span class="kw">summary</span>(temp2)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = meanmax ~ Year2, data = bozemantemps)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.3779 -0.9300  0.1078  1.1960  5.8698 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) 52.34126    0.31383  166.78   &lt;2e-16
## Year2        0.05244    0.00476   11.02   &lt;2e-16
## 
## Residual standard error: 1.624 on 107 degrees of freedom
## Multiple R-squared:  0.5315, Adjusted R-squared:  0.5271 
## F-statistic: 121.4 on 1 and 107 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(temp2)</code></pre></div>
<pre><code>##                   2.5 %      97.5 %
## (Intercept) 51.71913822 52.96339150
## Year2        0.04300681  0.06187746</code></pre>
<p>Ideally, we want to find a regression model that does not violate any assumptions, has a high <span class="math inline">\(\mathbf{R^2}\)</span> value, and a slope coefficient with a small p-value. If any of these are not the case, then we are not completely satisfied with the regression and <strong>should be suspicious of any inference we perform</strong>. We can sometimes resolve some of the systematic issues noted above using <strong><em>transformations</em></strong>, discussed in Sections <a href="chapter7.html#section7-5">7.5</a> and <a href="chapter7.html#section7-6">7.6</a>.</p>
</div>
<div id="section7-4" class="section level2">
<h2><span class="header-section-number">7.4</span> Randomizing inferences for the slope coefficient</h2>
<p>Exploring permutation testing in SLR provides an opportunity to gauge the observed relationship against the sorts of relationships we would expect to see if there was no linear relationship between the variables. If the relationship is linear (not curvilinear) and the null hypothesis of <span class="math inline">\(\beta_1=0\)</span> is true, then any configuration of the responses relative to the predictor variables is a good as any other. Consider the four scatterplots of the Bozeman temperature data versus <code>Year</code> and permuted versions of <code>Year</code> in Figure <a href="chapter7.html#fig:Figure7-7">7.8</a>. First, think about which of the panels you think presents the most evidence of a linear relationship between <code>Year</code> and <code>Temperature</code>?</p>

<div class="figure"><span id="fig:Figure7-7"></span>
<img src="07-simpleLinearRegressionInference_files/figure-html/Figure7-7-1.png" alt="Plot of the Temperature responses versus four versions of Year, three of which are permutations of the Year variable relative to the Temperatures." width="672" />
<p class="caption">
Figure 7.8: Plot of the <code>Temperature</code> responses versus four versions of <code>Year</code>, three of which are permutations of the Year variable relative to the Temperatures.
</p>
</div>
<p>Hopefully you can see that panel (c) contains the most clear linear relationship among the choices. The plot in panel (c) is actually the real data set and pretty clearly presents itself as “different” from the other results. When we have small p-values, the real data set will be clearly different from the permuted results because it will be almost impossible to find a permuted data set that can attain as large a slope coefficient as was observed in the real data set<a href="#fn64" class="footnoteRef" id="fnref64"><sup>64</sup></a>. This result ties back into our original interests in this climate change research situation – does our result look like it is different from what could have been observed just by chance if there were no linear relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>? It seems unlikely…</p>
<p>Repeating this permutation process and tracking the estimated slope coefficients, as <span class="math inline">\(T^*\)</span>, provides another method to obtain a p-value in SLR applications. This could also be performed on the <span class="math inline">\(t\)</span>-statistic for the slope coefficient and would provide the same p-values but the sampling distribution would have a different x-axis scaling. In this situation, the observed slope of 0.052 is really far from any possible values that can be obtained using permutations as shown in Figure <a href="chapter7.html#fig:Figure7-8">7.9</a>. The p-value would be reported as &lt; 0.0001 for the two-sided test.</p>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Tobs &lt;-<span class="st"> </span><span class="kw">lm</span>(meanmax<span class="op">~</span>Year, <span class="dt">data=</span>bozemantemps)<span class="op">$</span>coef[<span class="dv">2</span>]
Tobs</code></pre></div>
<pre><code>##       Year 
## 0.05244213</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))
B &lt;-<span class="st"> </span><span class="dv">1000</span>
Tstar &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>,<span class="dt">nrow=</span>B)
<span class="cf">for</span> (b <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>B)){
  Tstar[b] &lt;-<span class="st"> </span><span class="kw">lm</span>(meanmax<span class="op">~</span><span class="kw">shuffle</span>(Year), <span class="dt">data=</span>bozemantemps)<span class="op">$</span>coef[<span class="dv">2</span>]
}
<span class="kw">pdata</span>(<span class="kw">abs</span>(Tstar), <span class="kw">abs</span>(Tobs), <span class="dt">lower.tail=</span>F)</code></pre></div>
<pre><code>## Year 
##    0</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hist</span>(Tstar, <span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="op">*</span>Tobs)
<span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="op">*</span>Tobs, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>)
<span class="kw">plot</span>(<span class="kw">density</span>(Tstar), <span class="dt">main=</span><span class="st">&quot;Density curve of Tstar&quot;</span>, <span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="op">*</span>Tobs)
<span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="op">*</span>Tobs, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</code></pre></div>
<div class="figure"><span id="fig:Figure7-8"></span>
<img src="07-simpleLinearRegressionInference_files/figure-html/Figure7-8-1.png" alt="Permutation distribution of the slope coefficient in the Bozeman temperature linear regression model with bold vertical lines at \(\pm b_1=0.56\)." width="672" />
<p class="caption">
Figure 7.9: Permutation distribution of the slope coefficient in the Bozeman temperature linear regression model with bold vertical lines at <span class="math inline">\(\pm b_1=0.56\)</span>.
</p>
</div>
<p>One other interesting aspect of exploring the permuted data sets as in Figure <a href="chapter7.html#fig:Figure7-7">7.8</a> is that the outlier in the late 1930s “disappears” in the permuted data sets because there were many other observations that were that warm, just none that happened around that time of the century in the real data set. This reinforces the evidence for changes over time that seem to be present in these data that old unusual years don’t look unusual in more recent years.</p>
<p>The permutation approach can be useful in situations where the normality assumption is compromised, but there are no influential points. In these situations, we might find more trustworthy p-values using permutations but only if we are working with an initial estimated regression equation that we generally trust. I personally like the permutation approach as a way of explaining what a p-value is actually measuring – the chance of seeing something like what we saw, or more extreme, if the null is true. And the previous scatterplots show what the “by chance” versions of this relationship might look like.</p>
<p>In a similar situation where we want to focus on confidence intervals for slope coefficients but are not completely comfortable with the normality assumption, it is also possible to generate bootstrap confidence intervals by sampling with replacement from the data set. This idea was introduced in Sections <a href="chapter2.html#section2-8">2.8</a> and <a href="chapter2.html#section2-9">2.9</a>. This provides a 95% bootstrap confidence interval from 0.433 to 0.62, which almost exactly matches the parametric <span class="math inline">\(t\)</span>-based confidence interval. The bootstrap distributions are very symmetric (Figure <a href="chapter7.html#fig:Figure7-9">7.10</a>). The interpretation is the same and this result reinforces the other assessments that the parametric approach is not unreasonable except possibly for the independence assumption.</p>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Tobs &lt;-<span class="st"> </span><span class="kw">lm</span>(meanmax<span class="op">~</span>Year, <span class="dt">data=</span>bozemantemps)<span class="op">$</span>coef[<span class="dv">2</span>]
Tobs</code></pre></div>
<pre><code>##       Year 
## 0.05244213</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))
B &lt;-<span class="st"> </span><span class="dv">1000</span>
Tstar &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>,<span class="dt">nrow=</span>B)
<span class="cf">for</span> (b <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>B)){
  Tstar[b] &lt;-<span class="st"> </span><span class="kw">lm</span>(meanmax<span class="op">~</span>Year, <span class="dt">data=</span><span class="kw">resample</span>(bozemantemps))<span class="op">$</span>coef[<span class="dv">2</span>]
}
quantiles &lt;-<span class="st"> </span><span class="kw">qdata</span>(Tstar, <span class="kw">c</span>(.<span class="dv">025</span>,.<span class="dv">975</span>))
quantiles</code></pre></div>
<pre><code>##         quantile     p
## 2.5%  0.04331326 0.025
## 97.5% 0.06195345 0.975</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hist</span>(Tstar,<span class="dt">labels=</span>T)
<span class="kw">abline</span>(<span class="dt">v=</span>Tobs,<span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">lwd=</span><span class="dv">2</span>)
<span class="kw">abline</span>(<span class="dt">v=</span>quantiles<span class="op">$</span>quantile,<span class="dt">col=</span><span class="st">&quot;blue&quot;</span>,<span class="dt">lwd=</span><span class="dv">3</span>)
<span class="kw">plot</span>(<span class="kw">density</span>(Tstar),<span class="dt">main=</span><span class="st">&quot;Density curve of Tstar&quot;</span>)
<span class="kw">abline</span>(<span class="dt">v=</span>Tobs,<span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">lwd=</span><span class="dv">2</span>)
<span class="kw">abline</span>(<span class="dt">v=</span>quantiles<span class="op">$</span>quantile,<span class="dt">col=</span><span class="st">&quot;blue&quot;</span>,<span class="dt">lwd=</span><span class="dv">3</span>)</code></pre></div>
<div class="figure"><span id="fig:Figure7-9"></span>
<img src="07-simpleLinearRegressionInference_files/figure-html/Figure7-9-1.png" alt="Bootstrap distribution of the slope coefficient in the Bozeman temperature linear regression model with bold vertical lines delineating 95% confidence interval and observed slope of 0.52." width="672" />
<p class="caption">
Figure 7.10: Bootstrap distribution of the slope coefficient in the Bozeman temperature linear regression model with bold vertical lines delineating 95% confidence interval and observed slope of 0.52.
</p>
</div>
</div>
<div id="section7-5" class="section level2">
<h2><span class="header-section-number">7.5</span> Transformations part I: Linearizing relationships</h2>
</div>
<div id="section7-6" class="section level2">
<h2><span class="header-section-number">7.6</span> Transformations part II: Impacts on SLR interpretations: log(y), log(x), &amp; both log(y) &amp; log(x)</h2>
</div>
<div id="confidence-interval-for-the-mean-and-prediction-intervals-for-a-new-observation-270" class="section level2">
<h2><span class="header-section-number">7.7</span> Confidence Interval for the mean and prediction Intervals for a new observation 270</h2>
</div>
<div id="section7-7" class="section level2">
<h2><span class="header-section-number">7.8</span> Chapter summary</h2>
</div>
<div id="section7-8" class="section level2">
<h2><span class="header-section-number">7.9</span> Important R code</h2>
</div>
<div id="section7-9" class="section level2">
<h2><span class="header-section-number">7.10</span> Practice problems</h2>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="60">
<li id="fn60"><p>There is an area of statistical research on how to optimally choose x-values to get the most precise estimate of a slope coefficient. In observational studies we have to deal with whatever pattern of <span class="math inline">\(x\text{&#39;s}\)</span> we ended up with. If you can choose, generate an even spread of <span class="math inline">\(x\text{&#39;s}\)</span> over some range of interest similar to what was used in the <em>Beers</em> vs <em>BAC</em> study to provide the best distribution of values to discover the relationship across the selected range of x-values.<a href="chapter7.html#fnref60">↩</a></p></li>
<li id="fn61"><p>It is actually pretty amazing that there are hundreds of locations with nearly complete daily records for over 100 years.<a href="chapter7.html#fnref61">↩</a></p></li>
<li id="fn62"><p>All joking aside, if researchers can find evidence of climate change using <strong><em>conservative</em></strong> methods (methods that reject the null hypothesis when it is true less often than stated), then their results are even harder to ignore.<a href="chapter7.html#fnref62">↩</a></p></li>
<li id="fn63"><p>The simplest adjustment for multiple testing is using a Bonferroni adjustment, where you multiply all the p-values by the number of tests performed. It controls the chances of at least one error to be same as your Type I error rate for one test.<a href="chapter7.html#fnref63">↩</a></p></li>
<li id="fn64"><p>It took many permutations to get competitor plots this close to the real data set and they really aren’t that close.<a href="chapter7.html#fnref64">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="chapter6.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="chapter8.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
