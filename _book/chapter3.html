<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>A Second Semester Statistics Course with R</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="A Second Semester Statistics Course with R">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="A Second Semester Statistics Course with R" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="gpeterson406/GreenwoodBanner_Book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="A Second Semester Statistics Course with R" />
  
  
  

<meta name="author" content="Mark Greenwood and Katherine Banner">


<meta name="date" content="2017-06-23">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="chapter2.html">
<link rel="next" href="chapter4.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="chapter" data-level="1" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i><b>1</b> Preface</a><ul>
<li class="chapter" data-level="1.1" data-path="preface.html"><a href="preface.html#overview-of-methods"><i class="fa fa-check"></i><b>1.1</b> Overview of methods</a></a></li>
<li class="chapter" data-level="1.2" data-path="preface.html"><a href="preface.html#getting-started-in-r"><i class="fa fa-check"></i><b>1.2</b> Getting started in R</a></li>
<li class="chapter" data-level="1.3" data-path="preface.html"><a href="preface.html#basic-summary-statistics-histograms-and-boxplots-using-r"><i class="fa fa-check"></i><b>1.3</b> Basic summary statistics, histograms, and boxplots using R</a></li>
<li class="chapter" data-level="1.4" data-path="preface.html"><a href="preface.html#chapter-summary"><i class="fa fa-check"></i><b>1.4</b> Chapter summary</a></li>
<li class="chapter" data-level="1.5" data-path="preface.html"><a href="preface.html#important-r-code"><i class="fa fa-check"></i><b>1.5</b> Important R Code</a></li>
<li class="chapter" data-level="1.6" data-path="preface.html"><a href="preface.html#practice-problems"><i class="fa fa-check"></i><b>1.6</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chapter2.html"><a href="chapter2.html"><i class="fa fa-check"></i><b>2</b> (R)e-Introduction to statistics</a><ul>
<li class="chapter" data-level="2.1" data-path="chapter2.html"><a href="chapter2.html#section2-1"><i class="fa fa-check"></i><b>2.1</b> Histograms, boxplots, and density curves</a></li>
<li class="chapter" data-level="2.2" data-path="chapter2.html"><a href="chapter2.html#section2-2"><i class="fa fa-check"></i><b>2.2</b> Beanplots</a></li>
<li class="chapter" data-level="2.3" data-path="chapter2.html"><a href="chapter2.html#section2-3"><i class="fa fa-check"></i><b>2.3</b> Models, hypotheses, and permutations for the 2 sample mean situation</a></li>
<li class="chapter" data-level="2.4" data-path="chapter2.html"><a href="chapter2.html#section2-4"><i class="fa fa-check"></i><b>2.4</b> Permutation testing for the 2 sample mean situation</a></li>
<li class="chapter" data-level="2.5" data-path="chapter2.html"><a href="chapter2.html#section2-5"><i class="fa fa-check"></i><b>2.5</b> Hypothesis testing (general)</a></li>
<li class="chapter" data-level="2.6" data-path="chapter2.html"><a href="chapter2.html#section2-6"><i class="fa fa-check"></i><b>2.6</b> Connecting randomization (nonparametric) and parametric tests</a></li>
<li class="chapter" data-level="2.7" data-path="chapter2.html"><a href="chapter2.html#section2-7"><i class="fa fa-check"></i><b>2.7</b> Second example of permutation tests</a></li>
<li class="chapter" data-level="2.8" data-path="chapter2.html"><a href="chapter2.html#section2-8"><i class="fa fa-check"></i><b>2.8</b> Confidence intervals and bootstrapping</a></li>
<li class="chapter" data-level="2.9" data-path="chapter2.html"><a href="chapter2.html#section2-9"><i class="fa fa-check"></i><b>2.9</b> Bootstrap confidence intervals for difference in GPAs</a></li>
<li class="chapter" data-level="2.10" data-path="chapter2.html"><a href="chapter2.html#section2-10"><i class="fa fa-check"></i><b>2.10</b> Chapter summary</a></li>
<li class="chapter" data-level="2.11" data-path="chapter2.html"><a href="chapter2.html#section2-11"><i class="fa fa-check"></i><b>2.11</b> Summary of important R code</a></li>
<li class="chapter" data-level="2.12" data-path="chapter2.html"><a href="chapter2.html#section2-12"><i class="fa fa-check"></i><b>2.12</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="chapter3.html"><a href="chapter3.html"><i class="fa fa-check"></i><b>3</b> One-Way ANOVA</a><ul>
<li class="chapter" data-level="3.1" data-path="chapter3.html"><a href="chapter3.html#section3-1"><i class="fa fa-check"></i><b>3.1</b> Situation</a></li>
<li class="chapter" data-level="3.2" data-path="chapter3.html"><a href="chapter3.html#section3-2"><i class="fa fa-check"></i><b>3.2</b> Linear model for One-Way ANOVA (cell-means and reference-coding)</a></li>
<li class="chapter" data-level="3.3" data-path="chapter3.html"><a href="chapter3.html#section3-3"><i class="fa fa-check"></i><b>3.3</b> One-Way ANOVA Sums of Squares, Mean Squares, and F-test</a></li>
<li class="chapter" data-level="3.4" data-path="chapter3.html"><a href="chapter3.html#section3-4"><i class="fa fa-check"></i><b>3.4</b> ANOVA model diagnostics including QQ-plots</a></li>
<li class="chapter" data-level="3.5" data-path="chapter3.html"><a href="chapter3.html#section3-5"><i class="fa fa-check"></i><b>3.5</b> Guinea pig tooth growth One-Way ANOVA example</a></li>
<li class="chapter" data-level="3.6" data-path="chapter3.html"><a href="chapter3.html#section3-6"><i class="fa fa-check"></i><b>3.6</b> Multiple (pair-wise) comparisons using Tukey’s HSD and the compact letter display</a></li>
<li class="chapter" data-level="3.7" data-path="chapter3.html"><a href="chapter3.html#section3-7"><i class="fa fa-check"></i><b>3.7</b> Pair-wise comparisons for Prisoner Rating data</a></li>
<li class="chapter" data-level="3.8" data-path="chapter3.html"><a href="chapter3.html#section3-8"><i class="fa fa-check"></i><b>3.8</b> Chapter Summary</a></li>
<li class="chapter" data-level="3.9" data-path="chapter3.html"><a href="chapter3.html#section3-9"><i class="fa fa-check"></i><b>3.9</b> Summary of important R code</a></li>
<li class="chapter" data-level="3.10" data-path="chapter3.html"><a href="chapter3.html#section3-10"><i class="fa fa-check"></i><b>3.10</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="chapter4.html"><a href="chapter4.html"><i class="fa fa-check"></i><b>4</b> Two-Way ANOVA</a><ul>
<li class="chapter" data-level="4.1" data-path="chapter4.html"><a href="chapter4.html#section4-1"><i class="fa fa-check"></i><b>4.1</b> Situation</a></li>
<li class="chapter" data-level="4.2" data-path="chapter4.html"><a href="chapter4.html#section4-2"><i class="fa fa-check"></i><b>4.2</b> Designing a two-way experiment and visualizing results</a></li>
<li class="chapter" data-level="4.3" data-path="chapter4.html"><a href="chapter4.html#section4-3"><i class="fa fa-check"></i><b>4.3</b> Two-Way ANOVA models and hypothesis tests</a></li>
<li class="chapter" data-level="4.4" data-path="chapter4.html"><a href="chapter4.html#section4-4"><i class="fa fa-check"></i><b>4.4</b> Guinea pig tooth growth analysis with Two-Way ANOVA</a></li>
<li class="chapter" data-level="4.5" data-path="chapter4.html"><a href="chapter4.html#section4-5"><i class="fa fa-check"></i><b>4.5</b> Observational study example: The Psychology of Debt</a></li>
<li class="chapter" data-level="4.6" data-path="chapter4.html"><a href="chapter4.html#section4-6"><i class="fa fa-check"></i><b>4.6</b> Pushing Two-Way ANOVA to the limit: Un-replicated designs</a></li>
<li class="chapter" data-level="4.7" data-path="chapter4.html"><a href="chapter4.html#section4-7"><i class="fa fa-check"></i><b>4.7</b> Chapter summary</a></li>
<li class="chapter" data-level="4.8" data-path="chapter4.html"><a href="chapter4.html#section4-8"><i class="fa fa-check"></i><b>4.8</b> Important R code</a></li>
<li class="chapter" data-level="4.9" data-path="chapter4.html"><a href="chapter4.html#section4-9"><i class="fa fa-check"></i><b>4.9</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="chapter5.html"><a href="chapter5.html"><i class="fa fa-check"></i><b>5</b> Chi-square tests</a><ul>
<li class="chapter" data-level="5.1" data-path="chapter5.html"><a href="chapter5.html#section5-1"><i class="fa fa-check"></i><b>5.1</b> Situation, contingency tables, and plots</a></li>
<li class="chapter" data-level="5.2" data-path="chapter5.html"><a href="chapter5.html#section5-2"><i class="fa fa-check"></i><b>5.2</b> Homogeneity Test Hypotheses</a></li>
<li class="chapter" data-level="5.3" data-path="chapter5.html"><a href="chapter5.html#section5-3"><i class="fa fa-check"></i><b>5.3</b> Independence Test Hypotheses</a></li>
<li class="chapter" data-level="5.4" data-path="chapter5.html"><a href="chapter5.html#section5-4"><i class="fa fa-check"></i><b>5.4</b> Models for R by C tables</a></li>
<li class="chapter" data-level="5.5" data-path="chapter5.html"><a href="chapter5.html#section5-5"><i class="fa fa-check"></i><b>5.5</b> Permutation tests for the X2 statistic</a></li>
<li class="chapter" data-level="5.6" data-path="chapter5.html"><a href="chapter5.html#section5-6"><i class="fa fa-check"></i><b>5.6</b> Chi-square distribution for the X2 statistic</a></li>
<li class="chapter" data-level="5.7" data-path="chapter5.html"><a href="chapter5.html#section5-7"><i class="fa fa-check"></i><b>5.7</b> Examining residuals for the source of differences</a></li>
<li class="chapter" data-level="5.8" data-path="chapter5.html"><a href="chapter5.html#section5-8"><i class="fa fa-check"></i><b>5.8</b> General Protocol for X2 tests</a></li>
<li class="chapter" data-level="5.9" data-path="chapter5.html"><a href="chapter5.html#section5-9"><i class="fa fa-check"></i><b>5.9</b> Political Party and Voting results: Complete Analysis</a></li>
<li class="chapter" data-level="5.10" data-path="chapter5.html"><a href="chapter5.html#section5-10"><i class="fa fa-check"></i><b>5.10</b> Is cheating and lying related in students?</a></li>
<li class="chapter" data-level="5.11" data-path="chapter5.html"><a href="chapter5.html#section5-11"><i class="fa fa-check"></i><b>5.11</b> Analyzing a stratified random sample of California schools</a></li>
<li class="chapter" data-level="5.12" data-path="chapter5.html"><a href="chapter5.html#section5-12"><i class="fa fa-check"></i><b>5.12</b> Chapter summary</a></li>
<li class="chapter" data-level="5.13" data-path="chapter5.html"><a href="chapter5.html#section5-13"><i class="fa fa-check"></i><b>5.13</b> Review of Important R commands</a></li>
<li class="chapter" data-level="5.14" data-path="chapter5.html"><a href="chapter5.html#section5-14"><i class="fa fa-check"></i><b>5.14</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="chapter6.html"><a href="chapter6.html"><i class="fa fa-check"></i><b>6</b> Correlation and Simple Linear Regression</a><ul>
<li class="chapter" data-level="6.1" data-path="chapter6.html"><a href="chapter6.html#section6-1"><i class="fa fa-check"></i><b>6.1</b> Relationships between two quantitative variables</a></li>
<li class="chapter" data-level="6.2" data-path="chapter6.html"><a href="chapter6.html#section6-2"><i class="fa fa-check"></i><b>6.2</b> Estimating the correlation coefficient</a></li>
<li class="chapter" data-level="6.3" data-path="chapter6.html"><a href="chapter6.html#section6-3"><i class="fa fa-check"></i><b>6.3</b> Relationships between variables by groups</a></li>
<li class="chapter" data-level="6.4" data-path="chapter6.html"><a href="chapter6.html#section6-4"><i class="fa fa-check"></i><b>6.4</b> Inference for the correlation coefficient (Optional Section)</a></li>
<li class="chapter" data-level="6.5" data-path="chapter6.html"><a href="chapter6.html#section6-5"><i class="fa fa-check"></i><b>6.5</b> Are tree diameters related to tree heights?</a></li>
<li class="chapter" data-level="6.6" data-path="chapter6.html"><a href="chapter6.html#section6-6"><i class="fa fa-check"></i><b>6.6</b> Describing relationships with a regression model</a></li>
<li class="chapter" data-level="6.7" data-path="chapter6.html"><a href="chapter6.html#section6-7"><i class="fa fa-check"></i><b>6.7</b> Least Squares Estimation</a></li>
<li class="chapter" data-level="6.8" data-path="chapter6.html"><a href="chapter6.html#section6-8"><i class="fa fa-check"></i><b>6.8</b> Measuring the strength of regressions: R2</a></li>
<li class="chapter" data-level="6.9" data-path="chapter6.html"><a href="chapter6.html#section6-9"><i class="fa fa-check"></i><b>6.9</b> Outliers: leverage and influence</a></li>
<li class="chapter" data-level="6.10" data-path="chapter6.html"><a href="chapter6.html#section6-10"><i class="fa fa-check"></i><b>6.10</b> Residual diagnostics – setting the stage for inference</a></li>
<li class="chapter" data-level="6.11" data-path="chapter6.html"><a href="chapter6.html#section6-11"><i class="fa fa-check"></i><b>6.11</b> Old Faithful discharge and waiting times</a></li>
<li class="chapter" data-level="6.12" data-path="chapter6.html"><a href="chapter6.html#section6-12"><i class="fa fa-check"></i><b>6.12</b> Chapter summary</a></li>
<li class="chapter" data-level="6.13" data-path="chapter6.html"><a href="chapter6.html#section6-13"><i class="fa fa-check"></i><b>6.13</b> Important R code</a></li>
<li class="chapter" data-level="6.14" data-path="chapter6.html"><a href="chapter6.html#section6-14"><i class="fa fa-check"></i><b>6.14</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="chapter7.html"><a href="chapter7.html"><i class="fa fa-check"></i><b>7</b> Simple linear regression inference</a><ul>
<li class="chapter" data-level="7.1" data-path="chapter7.html"><a href="chapter7.html#section7-1"><i class="fa fa-check"></i><b>7.1</b> Model</a></li>
<li class="chapter" data-level="7.2" data-path="chapter7.html"><a href="chapter7.html#section7-2"><i class="fa fa-check"></i><b>7.2</b> Confidence Interval and Hypothesis tests for the slope and intercept</a></li>
<li class="chapter" data-level="7.3" data-path="chapter7.html"><a href="chapter7.html#section7-3"><i class="fa fa-check"></i><b>7.3</b> Bozeman temperature trend</a></li>
<li class="chapter" data-level="7.4" data-path="chapter7.html"><a href="chapter7.html#section7-4"><i class="fa fa-check"></i><b>7.4</b> Randomizing inferences for the slope coefficient</a></li>
<li class="chapter" data-level="7.5" data-path="chapter7.html"><a href="chapter7.html#section7-5"><i class="fa fa-check"></i><b>7.5</b> Transformations part I: Linearizing relationships</a></li>
<li class="chapter" data-level="7.6" data-path="chapter7.html"><a href="chapter7.html#section7-6"><i class="fa fa-check"></i><b>7.6</b> Transformations part II: Impacts on SLR interpretations: log(y), log(x), &amp; both log(y) &amp; log(x)</a></li>
<li class="chapter" data-level="7.7" data-path="chapter7.html"><a href="chapter7.html#confidence-interval-for-the-mean-and-prediction-intervals-for-a-new-observation-270"><i class="fa fa-check"></i><b>7.7</b> Confidence Interval for the mean and prediction Intervals for a new observation 270</a></li>
<li class="chapter" data-level="7.8" data-path="chapter7.html"><a href="chapter7.html#section7-7"><i class="fa fa-check"></i><b>7.8</b> Chapter summary</a></li>
<li class="chapter" data-level="7.9" data-path="chapter7.html"><a href="chapter7.html#section7-8"><i class="fa fa-check"></i><b>7.9</b> Important R code</a></li>
<li class="chapter" data-level="7.10" data-path="chapter7.html"><a href="chapter7.html#section7-9"><i class="fa fa-check"></i><b>7.10</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="chapter8.html"><a href="chapter8.html"><i class="fa fa-check"></i><b>8</b> Multiple linear regression</a><ul>
<li class="chapter" data-level="8.1" data-path="chapter8.html"><a href="chapter8.html#section8-1"><i class="fa fa-check"></i><b>8.1</b> Going from SLR to MLR</a></li>
<li class="chapter" data-level="8.2" data-path="chapter8.html"><a href="chapter8.html#section8-2"><i class="fa fa-check"></i><b>8.2</b> Validity conditions in MLR</a></li>
<li class="chapter" data-level="8.3" data-path="chapter8.html"><a href="chapter8.html#section8-3"><i class="fa fa-check"></i><b>8.3</b> Interpretation of MLR terms</a></li>
<li class="chapter" data-level="8.4" data-path="chapter8.html"><a href="chapter8.html#section8-4"><i class="fa fa-check"></i><b>8.4</b> Comparing multiple regression models</a></li>
<li class="chapter" data-level="8.5" data-path="chapter8.html"><a href="chapter8.html#section8-5"><i class="fa fa-check"></i><b>8.5</b> General recommendations for MLR interpretations and VIFs</a></li>
<li class="chapter" data-level="8.6" data-path="chapter8.html"><a href="chapter8.html#section8-6"><i class="fa fa-check"></i><b>8.6</b> MLR Inference: Parameter inferences using the t-distribution</a></li>
<li class="chapter" data-level="8.7" data-path="chapter8.html"><a href="chapter8.html#section8-7"><i class="fa fa-check"></i><b>8.7</b> Overall F-test in Multiple Linear Regression</a></li>
<li class="chapter" data-level="8.8" data-path="chapter8.html"><a href="chapter8.html#section8-8"><i class="fa fa-check"></i><b>8.8</b> Case Study: First year college GPA and SATs</a></li>
<li class="chapter" data-level="8.9" data-path="chapter8.html"><a href="chapter8.html#section8-9"><i class="fa fa-check"></i><b>8.9</b> Different intercepts for different groups: MLR with Indicator variables</a></li>
<li class="chapter" data-level="8.10" data-path="chapter8.html"><a href="chapter8.html#section8-10"><i class="fa fa-check"></i><b>8.10</b> Additive MLR with more than two groups: Headache example</a></li>
<li class="chapter" data-level="8.11" data-path="chapter8.html"><a href="chapter8.html#section8-11"><i class="fa fa-check"></i><b>8.11</b> Different slopes and different intercepts</a></li>
<li class="chapter" data-level="8.12" data-path="chapter8.html"><a href="chapter8.html#section8-12"><i class="fa fa-check"></i><b>8.12</b> F-tests for MLR models with quantitative and categorical variables and interactions</a></li>
<li class="chapter" data-level="8.13" data-path="chapter8.html"><a href="chapter8.html#section8-13"><i class="fa fa-check"></i><b>8.13</b> AICs for model selection</a></li>
<li class="chapter" data-level="8.14" data-path="chapter8.html"><a href="chapter8.html#section8-14"><i class="fa fa-check"></i><b>8.14</b> Forced Expiratory Volume model selection using AICs</a></li>
<li class="chapter" data-level="8.15" data-path="chapter8.html"><a href="chapter8.html#section8-15"><i class="fa fa-check"></i><b>8.15</b> Chapter summary</a></li>
<li class="chapter" data-level="8.16" data-path="chapter8.html"><a href="chapter8.html#section8-16"><i class="fa fa-check"></i><b>8.16</b> Important R code</a></li>
<li class="chapter" data-level="8.17" data-path="chapter8.html"><a href="chapter8.html#section8-17"><i class="fa fa-check"></i><b>8.17</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="chapter9.html"><a href="chapter9.html"><i class="fa fa-check"></i><b>9</b> Case studies</a><ul>
<li class="chapter" data-level="9.1" data-path="chapter9.html"><a href="chapter9.html#section9-1"><i class="fa fa-check"></i><b>9.1</b> Overview of material covered</a></li>
<li class="chapter" data-level="9.2" data-path="chapter9.html"><a href="chapter9.html#section9-2"><i class="fa fa-check"></i><b>9.2</b> The impact of simulated chronic nitrogen deposition on the biomass and N2-fixation activity of two boreal feather moss–cyanobacteria associations</a></li>
<li class="chapter" data-level="9.3" data-path="chapter9.html"><a href="chapter9.html#section9-3"><i class="fa fa-check"></i><b>9.3</b> Ants learn to rely on more informative attributes during decision-making</a></li>
<li class="chapter" data-level="9.4" data-path="chapter9.html"><a href="chapter9.html#section9-4"><i class="fa fa-check"></i><b>9.4</b> Multi-variate models are essential for understanding vertebrate diversification in deep time</a></li>
<li class="chapter" data-level="9.5" data-path="chapter9.html"><a href="chapter9.html#section9-5"><i class="fa fa-check"></i><b>9.5</b> General summary</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Second Semester Statistics Course with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chapter3" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> One-Way ANOVA</h1>
<p>In Chapter <a href="chapter2.html#chapter2">2</a>, tools for comparing the means of two groups were considered. More generally, these methods are used for a quantitative response and a categorical explanatory variable (group) which had two and only two levels. The full prisoner rating data set actually contained three groups (Figure <a href="chapter3.html#fig:Figure3-1">3.1</a> with <em>Beautiful</em>, <em>Average</em>, and <em>Unattractive</em> rated pictures randomly assigned to the subjects for sentence ratings. In a situation with more than two groups, we have two choices. First, we could rely on our two group comparisons, performing tests for every possible pair (<em>Beautiful</em> vs<em>Average</em>, <em>Beautiful</em> vs <em>Unattractive</em>, and <em>Average</em> vs <em>Unattractive</em>). We spent Chapter <a href="chapter2.html#chapter2">2</a> doing inferences for differences between <em>Average</em> and <em>Unattractive</em>. The other two comparisons would lead us to initially end up with three p-values and no direct answer about our initial question of interest – is there some overall difference in the average sentences provided across the groups? In this chapter, we will learn a new method, called <strong><em>Analysis of Variance</em></strong>, or <strong><em>One-Way ANOVA</em></strong> since there is just one<a href="#fn31" class="footnoteRef" id="fnref31"><sup>31</sup></a> grouping variable. After we perform our One-Way ANOVA test for overall evidence of a difference, we will revisit the comparisons similar to those considered in Chapter <a href="chapter2.html#chapter2">2</a> to get more details on specific differences among <em>all</em> the pairs of groups – what we call <strong><em>pair-wise comparisons</em></strong>. An issue is created when you perform many tests simultaneously and we will augment our previous methods with an adjusted method for pairwise comparisons to make our results valid called <strong><em>Tukey’s Honest Significant Difference</em></strong>.</p>
<p>To make this more concrete, we return to the original MockJury data, making side-by-side boxplots and beanplots (Figure <a href="chapter3.html#fig:Figure3-1">3.1</a> as well summarizing the sentences for the three groups using <code>favstats</code>.</p>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(heplots)
<span class="kw">require</span>(mosaic)
<span class="kw">data</span>(MockJury)
<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))
<span class="kw">boxplot</span>(Years<span class="op">~</span>Attr,<span class="dt">data=</span>MockJury)
<span class="kw">require</span>(beanplot)
<span class="kw">beanplot</span>(Years<span class="op">~</span>Attr,<span class="dt">data=</span>MockJury,<span class="dt">log=</span><span class="st">&quot;&quot;</span>,<span class="dt">col=</span><span class="st">&quot;bisque&quot;</span>,<span class="dt">method=</span><span class="st">&quot;jitter&quot;</span>)</code></pre></div>
<div class="figure"><span id="fig:Figure3-1"></span>
<img src="GreenwoodBanner_files/figure-html/Figure3-1-1.png" alt="Boxplot and beanplot of the sentences (years) for the three treatment groups." width="672" />
<p class="caption">
Figure 3.1: Boxplot and beanplot of the sentences (years) for the three treatment groups.
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">favstats</span>(Years<span class="op">~</span>Attr,<span class="dt">data=</span>MockJury)</code></pre></div>
<pre><code>##           Attr min Q1 median   Q3 max     mean       sd  n missing
## 1    Beautiful   1  2      3  6.5  15 4.333333 3.405362 39       0
## 2      Average   1  2      3  5.0  12 3.973684 2.823519 38       0
## 3 Unattractive   1  2      5 10.0  15 5.810811 4.364235 37       0</code></pre>
<p>There are slight differences in the sample sizes in the three groups with 37 <em>Unattractive</em>, 38 <em>Average</em> and 39 <em>Beautiful</em> group responses, providing a data set has a total sample size of <span class="math inline">\(N=114\)</span>. The <em>Beautiful</em> and <em>Average</em> groups do not appear to be very different with means of 4.33 and 3.97 years. In Chapter <a href="chapter2.html#chapter2">2</a>, we found moderate evidence regarding the difference in <em>Average</em>and <em>Unattractive</em>. It is less clear whether we might find evidence of a difference between <em>Beautiful</em> and <em>Unattractive</em> groups since we are comparing means of 5.81 and 4.33 years. All the distributions appear to be right skewed with relatively similar shapes. The variability in <em>Average</em> and <em>Unattractive</em> groups seems like it could be slightly different leading to an overall concern of whether the variability is the same in all the groups.</p>
<div id="section3-1" class="section level2">
<h2><span class="header-section-number">3.1</span> Situation</h2>
<p>Weintroduced the statistical model <span class="math inline">\(y_{ij} = \mu_j+\epsilon_j\)</span> in Chapter <a href="chapter2.html#chapter2">2</a> for the situation with <span class="math inline">\(j = 1 \text{ or } 2\)</span> to denote a situation where there were two groups and, for the model that is consistent with the alternative hypothesis, the means differed. Now we have three groups and the previous model can be extended to this new situation by allowing <span class="math inline">\(j\)</span> to be 1, 2, or 3. Now that we have more than two groups, we need to admit that what we were doing in Chapter <a href="chapter2.html#chapter2">2</a> was actually fitting what is called a <strong><em>linear model</em></strong>. The linear model assumes that the responses follow a normal distribution with the linear model defining the mean, all observations have the same variance, and the parameters for the mean in the model enter linearly. This last condition is hard to explain at this level of material – it is sufficient to know that there models where the parameters enter the model nonlinearly and that they are beyond the scope of this course. The result of this constraint is that we will be able to use the same general modeling framework for the methods introduced in Chapters <a href="chapter3.html#chapter3">3</a>, <a href="chapter4.html#chapter4">4</a>, <a href="chapter6.html#chapter6">6</a>, <a href="chapter7.html#chapter7">7</a>, and <a href="chapter8.html#chapter8">8</a>.</p>
<p>As in Chapter <a href="chapter2.html#chapter2">2</a>, we have a null hypothesis that defines a situation (and model) where all the groups have the same mean. Specifically, the <strong><em>null hypothesis</em></strong> in the general situation with <span class="math inline">\(J\)</span> groups (<span class="math inline">\(J\ge 2\)</span>) is to have all the <strong>true</strong> group means equal,</p>
<p><span class="math display">\[H_0:\mu_1 = \ldots \mu_J.\]</span></p>
<p>This defines a model where all the groups have the same mean so it can be defined in terms of a single mean, <span class="math inline">\(\mu\)</span>, for the <span class="math inline">\(i^{th}\)</span> observation from the <span class="math inline">\(j^{th}\)</span> group as <span class="math inline">\(y_{ij} = \mu+\epsilon_{ij}\)</span>. This is not the model that most researchers want to be the final description of their study as it implies no difference in the groups. There is more caution required to specify the alternative hypothesis with more than two groups. The <strong><em>alternative hypothesis</em></strong> needs to be the logical negation of this null hypothesis of all groups having equal means; to make the null hypothesis false, we only need one group to differ but more than one group could differ from the others. Essentially, there are many ways to “violate” the null hypothesis so we choose some delicate wording for the alternative hypothesis when there are more than 2 groups. Specifically, we state the alternative as</p>
<p><span class="math display">\[H_A: \text{ Not all } \mu_j \text{ are equal}\]</span></p>
<p>or, in words, <strong>at least one of the true means differs among the J groups</strong>. You will be attracted to trying to say that all means are different in the alternative but we do not put this strict a requirement in place to reject the null hypothesis. The alternative model allows all the true group means to differ but does require that they differ with</p>
<p><span class="math display">\[{\color{red}{\mu_j}}+\epsilon_{ij}.\]</span></p>
<p>This linear model states that the response for the <span class="math inline">\(i^{th}\)</span> observation in the <span class="math inline">\(j^{th}\)</span> group, <span class="math inline">\(\mathbf{y_{ij}}\)</span>, is modeled with a group <span class="math inline">\(j\)</span> (<span class="math inline">\(j=1, \ldots, J\)</span>) population mean, <span class="math inline">\(\mu_j\)</span>, and a random error for each subject in each group <span class="math inline">\(\epsilon_{ij}\)</span>, that we assume follows a normal distribution and that all the random errors have the same variance, <span class="math inline">\(\sigma^2\)</span>. We can write the assumption aboutthe random errors, often called the <strong><em>normality assumption</em></strong>, as <span class="math inline">\(\epsilon_{ij} \sim N(0,\sigma^2)\)</span>. There is a second way to write out this model that allows extension to more complex models discussed below, so we need a name for this version of the model. The model written in terms of the <span class="math inline">\({\color{red}{\mu_j}}\text{&#39;s}\)</span> is called the <b><font color='red'>cell means model</font></b> and is the easier version of this model to understand.</p>
<p>One of the reasons we learned about beanplots is that it helps us visually consider all the aspects of this model. In the right panel of Figure <a href="chapter3.html#fig:Figure3-1">3.1</a>, we can see the wider, bold horizontal lines that provide the estimated group means. The bigger the differences in the sample means, the more likely we are to find evidence against the null hypothesis. You can also see the null model on the plot that assumes all the groups have the same as displayed in the dashed horizontal line at 4.7 years (the R code below shows the overall mean of <em>Years</em> is 4.7). While the hypotheses focus on the means, the model also contains assumptions about the distribution of the responses – specifically that the distributions are normal and that all the groups have the same variability. As discussed previously, it appears that the distributions are right skewed and the variability might not be the same for all the groups. The boxplot provides the information about the skew and variability but since it doesn’t display the means it is not directly related to the linear model and hypotheses we are considering.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(MockJury<span class="op">$</span>Years)</code></pre></div>
<pre><code>## [1] 4.692982</code></pre>
<p>There is a second way to write out the One-Way ANOVA model that provides a framework for extensions to more complex models described in Chapter <a href="chapter4.html#chapter4">4</a> and beyond. The other <strong><em>parameterization</em></strong> (way of writing out or defining) of the model is called the <b><font color='purple'>reference-coded model</font></b> since it writes out the model in terms of a<br />
<strong><em>baseline group</em></strong> and deviations from that baseline or reference level. The reference-coded model for the <span class="math inline">\(i^{th}\)</span> subject in the <span class="math inline">\(j^{th}\)</span> group is <span class="math inline">\(y_{ij} =\color{purple}{\boldsymbol{\alpha + \tau_j}}+\epsilon_{ij}\)</span> where <span class="math inline">\(\color{purple}{\boldsymbol{\alpha}}\)</span> (alpha) is the true mean for the baseline group (first alphabetically) and the <span class="math inline">\(\color{purple}{\boldsymbol{\tau_j}}\)</span> (tau <span class="math inline">\(j\)</span>) are the deviations from the baseline group for group <span class="math inline">\(j\)</span>. The deviation for the baseline group, <span class="math inline">\(\color{purple}{\boldsymbol{\tau_1}}\)</span>, is always set to 0 so there are really just deviations for groups 2 through <span class="math inline">\(J\)</span>. The equivalence between the two models can be seen by considering the mean for the first, second, and <span class="math inline">\(J^{th}\)</span> groups in both models:</p>
<table>
<thead>
<tr class="header">
<th></th>
<th align="left">Cell means:</th>
<th align="left">Reference-coded:</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Group 1:</td>
<td align="left"><span class="math inline">\({\color{red}{\mu_1}}\)</span></td>
<td align="left"><span class="math inline">\({\color{purple}{\boldsymbol{\alpha}}}\)</span></td>
</tr>
<tr class="even">
<td>Group 2:</td>
<td align="left"><span class="math inline">\({\color{red}{\mu_2}}\)</span></td>
<td align="left"><span class="math inline">\({\color{red}{\boldsymbol{\tau_2}}}\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\ldots\)</span></td>
<td align="left"><span class="math inline">\(\ldots\)</span></td>
<td align="left"><span class="math inline">\(\ldots\)</span></td>
</tr>
<tr class="even">
<td>Group <span class="math inline">\(J\)</span>:</td>
<td align="left"><span class="math inline">\({\color{red}{\mu_J}}\)</span></td>
<td align="left"><span class="math inline">\({\color{purple}{\boldsymbol{\tau_J}}}\)</span></td>
</tr>
</tbody>
</table>
<p>The hypotheses for the reference-coded model are similar to those in the cell-means coding except that they are defined in terms of the deviations, <span class="math inline">\({\color{purple}{\boldsymbol{\tau_j}}}\)</span>. The null hypothesis is that there is no deviation from the baseline for any group – that all the <span class="math inline">\({\color{purple}{\boldsymbol{\tau_j\text{&#39;s}}}}=0\)</span>,</p>
<p><span class="math display">\[\boldsymbol{H_0: \tau_2=\ldots=\tau_J=0}.\]</span></p>
<p>The alternative hypothesis is that at least one of the deviations is not 0,</p>
<p><span class="math display">\[\boldsymbol{H_A:} \textbf{ Not all } \boldsymbol{\tau_j} \textbf{ equal } \bf{0}.\]</span></p>
<p>In this chapter, you are welcome to use either version (unless we instruct you otherwise) but we have to use the reference-coding in subsequent chapters. The next task is to learn how to use R’s linear model <code>lm</code> function to get estimates of the parameters in each model, but first a quick review of these new ideas:</p>
<p><b><font color='red'>Cell Means Version</font></b></p>
<ul>
<li><p><span class="math inline">\(H_0: {\color{red}{\mu_1=\ldots\mu_J}}\)</span>             <span class="math inline">\(H_A: {\color{red}{\text{ Not all } \mu_j \text{ equal}}}\)</span></p></li>
<li><p>Null hypothesis in words: No difference in the true means between the groups.</p></li>
<li><p>Null model <span class="math inline">\(y_{ij} = \mu_j+\epsilon_{ij}\)</span></p></li>
<li><p>Alternative hypothesis in words: At least one of the true means differs between the groups.</p></li>
<li><p>Alternative model: <span class="math inline">\(y_{ij} = \color{red}{\mu_j}+\epsilon_{ij}.\)</span></p></li>
</ul>
<p><b><font color='purple'>Reference-coded Version</font></b></p>
<ul>
<li><p><span class="math inline">\(H_0: \color{purple}{\boldsymbol{\tau_2 \ldots \tau_J = 0}}\)</span>          <span class="math inline">\(H_A: \color{purple}{\text{ Not all } \tau_j \text{ equal}}\)</span></p></li>
<li><p>Null hypothesis in words: No deviation of the true mean for any groups from the baseline group.</p></li>
<li><p>Null model: <span class="math inline">\(y_{ij} =\boldsymbol{\alpha} + \tau_j+\epsilon_{ij}\)</span></p></li>
<li><p>Alternative hypothesis in words: At least one of the true deviations is different from 0 or that at least one group has a different true mean than the baseline group.</p></li>
<li><p>Alternative model: <span class="math inline">\(y_{ij} =\color{purple}{\boldsymbol{\alpha + \tau_j}}+\epsilon_{ij}\)</span></p></li>
</ul>
<p>In order to estimate the models discussed above, the <code>lm</code> function is used. If you look closely in the code for the rest of the book, any model for a quantitative response will use this function, suggesting a common thread in the most commonly used statistical models. The <code>lm</code> function continues to use the same format as previous functions, <code>lm(Y~X, data=datasetname)</code>. It ends up that this code will give you the reference-coded version of the model by default (R thinks it is that important!). We want to start with the cell-means version of the model, so we have to override the standard technique and add a “<code>-1</code>” to the formula interface to tell R that we want to the cell-means coding. Generally, this looks like <code>lm(Y~X-1 , data=datasetname).</code> Once we fit a model in R, the <code>summary</code> function run on the model provides a useful “summary” of the model coefficients and a suite of other potentially interesting information. When fitting this version of the One-Way ANOVA model, you will find a row of output for each group relating the <span class="math inline">\(\mu_j\text{&#39;s}\)</span>. The output contains columns for an estimate (<code>Estimate</code>), standard error (<code>Std.Error</code>), <span class="math inline">\(t\)</span>-value (<code>t value</code>), and p-value (<code>Pr(&gt;|t|)</code>). We’ll learn to use all the output in the following material, but for now just focus on the estimates of the parameters that the function provides that we put in bold.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lm1 &lt;-<span class="st"> </span><span class="kw">lm</span>(Years <span class="op">~</span><span class="st"> </span>Attr<span class="op">-</span><span class="dv">1</span>, <span class="dt">data=</span>MockJury)
<span class="kw">summary</span>(lm1)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Years ~ Attr - 1, data = MockJury)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.8108 -2.8108 -0.9737  2.1892 10.6667 
## 
## Coefficients:
##                  Estimate Std. Error t value Pr(&gt;|t|)    
## AttrBeautiful      4.3333     0.5730   7.563 1.23e-11 ***
## AttrAverage        3.9737     0.5805   6.845 4.41e-10 ***
## AttrUnattractive   5.8108     0.5883   9.878  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3.578 on 111 degrees of freedom
## Multiple R-squared:  0.6449, Adjusted R-squared:  0.6353 
## F-statistic: 67.21 on 3 and 111 DF,  p-value: &lt; 2.2e-16</code></pre>
<table style="width:93%;">
<colgroup>
<col width="31%" />
<col width="15%" />
<col width="18%" />
<col width="13%" />
<col width="13%" />
</colgroup>
<thead>
<tr class="header">
<th align="center"> </th>
<th align="center">Estimate</th>
<th align="center">Std. Error</th>
<th align="center">t value</th>
<th align="center">Pr(&gt;|t|)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>AttrBeautiful</strong></td>
<td align="center"><strong>4.33</strong></td>
<td align="center">0.573</td>
<td align="center">7.56</td>
<td align="center">1.23e-11</td>
</tr>
<tr class="even">
<td align="center"><strong>AttrAverage</strong></td>
<td align="center"><strong>3.97</strong></td>
<td align="center">0.58</td>
<td align="center">6.85</td>
<td align="center">4.41e-10</td>
</tr>
<tr class="odd">
<td align="center"><strong>AttrUnattractive</strong></td>
<td align="center"><strong>5.81</strong></td>
<td align="center">0.588</td>
<td align="center">9.88</td>
<td align="center">6.86e-17</td>
</tr>
</tbody>
</table>
<p>In general, we denote estimated parameters with a hat over the parameter of interest to show that it is an estimate. For the true mean of group <span class="math inline">\(j\)</span>, <span class="math inline">\(\mu_j\)</span>, we estimate it with <span class="math inline">\(\hat{\mu}_j\)</span>, which is just the sample mean for group <span class="math inline">\(j\)</span>, <span class="math inline">\(\bar{x}_j\)</span>. The model suggests an estimate for each observation that we denote as <span class="math inline">\(\hat{y}_{ij}\)</span> that we will also call a <strong><em>fitted value</em></strong> based on the model being considered. The three estimates are bolded in the previous output, with the same estimate used for all observations in the same group. R tries to help you to sort out which row of output corresponds to which group by appending the group name l with the variable name. Here, the variable name was <code>Attr</code> and the first group alphabetically was <em>Beautiful</em>, so R provides a row labeled <code>AttrBeautiful</code> with an estimate of 4.3333. The sample means from the three groups can be seen to directly match that and the other two results.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(Years <span class="op">~</span><span class="st"> </span>Attr, <span class="dt">data=</span>MockJury)</code></pre></div>
<pre><code>##    Beautiful      Average Unattractive 
##     4.333333     3.973684     5.810811</code></pre>
<p>The reference-coded version of the same model is more complicated but ends up giving the same results once we understand what it is doing. It uses a different parameterization to accomplish this so has different model output. Here is the model summary:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lm2 &lt;-<span class="st"> </span><span class="kw">lm</span>(Years <span class="op">~</span><span class="st"> </span>Attr, <span class="dt">data=</span>MockJury)
<span class="kw">summary</span>(lm2)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Years ~ Attr, data = MockJury)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.8108 -2.8108 -0.9737  2.1892 10.6667 
## 
## Coefficients:
##                  Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)        4.3333     0.5730   7.563 1.23e-11 ***
## AttrAverage       -0.3596     0.8157  -0.441   0.6601    
## AttrUnattractive   1.4775     0.8212   1.799   0.0747 .  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3.578 on 111 degrees of freedom
## Multiple R-squared:  0.04754,    Adjusted R-squared:  0.03038 
## F-statistic:  2.77 on 2 and 111 DF,  p-value: 0.067</code></pre>
<p>The estimated model coefficients are <span class="math inline">\(\hat{\alpha} = 4.333\)</span> years, <span class="math inline">\(\hat{tau}_2 =-0.3596\)</span> years, <span class="math inline">\(\hat{\tau}_3=1.4775\)</span> years where R selected group 1 for <em>Beautiful</em>, 2 for <em>Average</em>, and 3 for <em>Unattractive</em>. The way you can figure out the baseline group (group 1 is <em>Beautiful</em> here) is to see which category label is <em>not present</em> in the output. <strong>The baseline level is typically the first group label alphabetically</strong>, but you should always check this. Based on these definitions, there are interpretations available for each coefficient. For <span class="math inline">\(\hat{\alpha} = 4.333\)</span> years, this is an estimate of the mean sentencing time for the <em>Beautiful</em>group. <span class="math inline">\(\hat{\tau}_2 =-0.3596\)</span> years is the deviation of the <em>Average</em> group’s mean from the <em>Beautiful</em> groups mean (specifically, it is <span class="math inline">\(0.36\)</span> years lower). Finally, <span class="math inline">\(\hat{\tau}_3=1.4775\)</span> years tells us that the <em>Unattractive</em> group mean sentencing time is 1.48 years higher than time. These interpretations lead directly to reconstructing the estimated means for each group by combining the baseline and pertinent deviations as shown in Table <a href="chapter3.html#tab:Table3-1">3.1</a>.</p>

<table>
<caption><span id="tab:Table3-1">Table 3.1: </span>Constructing group mean estimates from the reference-coded linear model estimates.</caption>
<thead>
<tr class="header">
<th align="left">Group</th>
<th align="left">Formula</th>
<th align="left">Estimates</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Beautiful</td>
<td align="left"><span class="math inline">\(\hat{\alpha}\)</span></td>
<td align="left"><strong>4.3333</strong> years</td>
</tr>
<tr class="even">
<td align="left">Average</td>
<td align="left"><span class="math inline">\(\hat{\alpha}+\hat{\tau}_2\)</span></td>
<td align="left">4.3333 - 0.3596 = <strong>3.974</strong> years</td>
</tr>
<tr class="odd">
<td align="left">Unattractive</td>
<td align="left"><span class="math inline">\(\hat{\alpha}+\hat{\tau}_3\)</span></td>
<td align="left">4.3333 + 1.4775 = <strong>5.811</strong> years</td>
</tr>
</tbody>
</table>
<p>We can also visualize the results of our linear models using what are called <strong><em>term-plots</em></strong> or <strong><em>effect-plots</em></strong> (from the <code>effects</code> package; Fox, 2003) as displayed in Figure <a href="chapter3.html#fig:Figure3-2">3.2</a>. We don’t want to use the word “effect” for these model components unless we have random assignment in the study design so we generically call these <strong><em>term-plots</em></strong> as they display terms or components from the model in hopefully useful ways to aid in model interpretation even in the presence of complicated model parameterizations. Specifically, these plots take an estimated model and show you its estimates along with 95% confidence intervals generated by the linear model. To make this plot, you need to install and load the <code>effects</code> package and then use <code>plot(allEffects(...))</code> functions together on the <code>lm</code> object called <code>lm2</code> that was estimated above. You can find the correspondence between the displayed means and the estimates that were constructed in Table <a href="chapter3.html#tab:Table3-1">3.1</a>.</p>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(effects)
<span class="kw">plot</span>(<span class="kw">allEffects</span>(lm2))</code></pre></div>
<div class="figure"><span id="fig:Figure3-2"></span>
<img src="GreenwoodBanner_files/figure-html/Figure3-2-1.png" alt="Plot of the estimated group mean sentences from the reference-coded model for the MockJury data." width="672" />
<p class="caption">
Figure 3.2: Plot of the estimated group mean sentences from the reference-coded model for the MockJury data.
</p>
</div>
<p>In order to assess evidence for having different means for the groups, we will compare either of the previous models (cell-means or reference-coded) to a null model based on the null hypothesis (<span class="math inline">\(H_0: \mu_1 = \ldots = \mu_J\)</span>) which implies a model of <span class="math inline">\(\color{red}{y_{ij} = \mu_j}+\epsilon_{ij}\)</span> in the cell-means version where <span class="math inline">\({\color{red}{\mu}}\)</span> is a common mean for all the observations. We will call this the <b><font color='red'>mean-only</font></b> model since it only has a single mean in it. In the reference-coding version of the model, we have a null hypothesis that <span class="math inline">\(H_0: \tau_2 = \ldots = \tau_J = 0\)</span>, so the “mean-only” model is <span class="math inline">\(\color{purple}{y_{ij} =\boldsymbol{\alpha}+\epsilon_{ij}}\)</span> with <span class="math inline">\(\color{purple}{\boldsymbol{\alpha}}\)</span> having the same definition as <span class="math inline">\(\color{red}{\mu}\)</span> for the cell means model – it forces a common value for the mean for all the groups. Moving from the <em>reference-coded</em> model to the <em>mean-only</em> model is also an example of a situation where we move from a “full” model to a “reduced” model by setting some coefficients in the “full” model to 0 and, by doing this, get a simpler or “reduced” model. Simple models can be good as they are easier to groups that suggests no difference in the groups is not a very exciting result in most, but not all, situations<a href="#fn32" class="footnoteRef" id="fnref32"><sup>32</sup></a>. In order for R to provide results for the mean-only model, we remove the grouping variable, <code>Attr</code>, from the model formula and just include a “1”. The <code>(Intercept)</code> row of the output provides the estimate for the mean-only model as a reduced model from either the cell-means or reference-coded models when we assume that the mean is the same for all groups:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lm3 &lt;-<span class="st"> </span><span class="kw">lm</span>(Years <span class="op">~</span><span class="st"> </span><span class="dv">1</span>, <span class="dt">data=</span>MockJury)
<span class="kw">summary</span>(lm3)</code></pre></div>
<pre><code>## $coefficients
##             Estimate Std. Error  t value     Pr(&gt;|t|)
## (Intercept) 4.692982  0.3403532 13.78857 5.765681e-26</code></pre>
<p>This model provides an estimate of the common mean for all observations of <span class="math inline">\(4.693 = \hat{\mu} = \hat{\alpha}\)</span> years. This value also is the dashed, horizontal line in the beanplot in Figure <a href="chapter3.html#fig:Figure3-1">3.1</a>. Some people call this mean-only estimate the grand or overall mean.</p>
</div>
<div id="section3-2" class="section level2">
<h2><span class="header-section-number">3.2</span> Linear model for One-Way ANOVA (cell-means and reference-coding)</h2>
<p>The previous discussion showed two ways of parameterizing models for the One-Way ANOVA model and getting estimates from output but still hasn’t addressed how to assess evidence related to whether the observed differences in the means among the groups is “real”. In this section, we develop what is called the <strong><em>ANOVA F-test</em></strong> that provides a method of aggregating the differences among the means of 2 or more groups and testing our null hypothesis of no difference in the means vs the alternative. In order to develop the test, some additional notation is needed. The sample size in each group is denoted <span class="math inline">\(n_j\)</span> and the total sample size is <span class="math inline">\(\boldsymbol{N=\Sigma n_j = n_1 + n_2 + \ldots + n_J}\)</span> where <span class="math inline">\(\Sigma\)</span> (capital sigma) means “add up over whatever follows”. An estimated <strong><em>residual</em></strong> (<span class="math inline">\(e_{ij}\)</span>) is the difference between an observation, <span class="math inline">\(y_{ij}\)</span>, and the model estimate, <span class="math inline">\(\hat{y}_{ij} = \hat{\mu}_j\)</span>, for that observation, <span class="math inline">\(y_{ij}-\hat{y}_{ij} = e_{ij}\)</span>. It is basically what is left over that the mean part of the model (<span class="math inline">\(\hat{\mu}_{j}\)</span>) does not explain. It is also a window into how “good” the model might be.</p>
<p>Consider the four different fake results for a situation with four groups (<span class="math inline">\(J=4\)</span>) displayed in Figure <a href="#fig:Figure3-3"><strong>??</strong></a>. Which of the different results shows the most and least evidence of differences in the means? In trying to answer this, think about both how different the means are (obviously important) and how variable the results are around the mean. These situations were created to have the same means in Scenarios 1 and 2 as well as matching means in Scenarios 3 and 4. The variability around the means matches by shading (lighter or darker). In Scenarios 1 and 2, the differences in the means is smaller than in the other two results. But Scenario 2 should provide more evidence of what little difference in present than Scenario 1 because it has less variability around the means. The best situation for finding group differences here is Scenario 4 since it has the largest difference in the means and the least variability around those means. Our test statistic somehow needs to allow a comparison of the variability in the means to the overall variability to help us get results that reflect that Scenario 4 has the strongest evidence of a difference and Scenario 1 would have the least.</p>

<p><img src="GreenwoodBanner_files/figure-html/unnamed-chunk-81-1.png" width="672" /></p>
<p>The statistic that allows the comparison of relative amounts of variation is called the <strong><em>ANOVA F-statistic</em></strong> . It is developed using <strong><em>sums of squares</em></strong> which are measures of total variation like are used in the numerator of the standard deviation (<span class="math inline">\(\Sigma_1^N(y_i-\bar{y})^2\)</span>) that took all the observations, subtracted the mean, squared the differences, and then added up the results over all the observations to generate a measure of total variability. With multiple groups, we will focus on decomposing that total variability (<strong><em>Total Sums of Squares</em></strong>) into variability among the means (we’ll call this <strong><em>Explanatory Variable</em></strong> <span class="math inline">\(\mathbf{A}\textbf{&#39;s}\)</span> <strong><em>Sums of Squares</em></strong>) and variability in the residuals or errors ( <strong><em>Error Sums of Squares</em></strong>). We define each of these quantities in the One-Way ANOVA situation as follows:</p>
<ul>
<li><p><span class="math inline">\(\textbf{SS}_{\textbf{Total}} =\)</span> Total Sums of Squares <span class="math inline">\(= \Sigma^J_{j=1}\Sigma^{n_j}_{i=1}(y_{ij}-\bar{\bar{y}})^2\)</span></p>
<ul>
<li><p>This is the total variation in the responses around the overall or <strong><em>grand mean</em></strong> (<span class="math inline">\(\bar{\bar{y}}\)</span>, the estimated mean for all the observations and available from the mean-only model).</p></li>
<li><p>By summing over all <span class="math inline">\(n_j\)</span> observations in each group, <span class="math inline">\(\Sigma^{n_j}_{i=1}(\ )\)</span>, and then adding those results up across the groups, <span class="math inline">\(\Sigma^J_{j=1}(\ )\)</span>, we accumulate the variation across all <span class="math inline">\(N\)</span> observations.</p></li>
<li><p>Note: this is the residual variation if the null model is used, so there is no further decomposition possible for that model.</p></li>
<li><p>This is also equivalent to the numerator of the sample variance, <span class="math inline">\(\Sigma^{N}_{1}(y_{i}-\bar{y})^2\)</span> which is what you get when you ignore the information on the potential differences in the groups.</p></li>
</ul></li>
<li><p><span class="math inline">\(\textbf{SS}_{\textbf{A}} =\)</span> Explanatory Variable <em>A</em>’s Sums of Squares <span class="math inline">\(=\Sigma^J_{j=1}\Sigma^{n_j}_{i=1}(\bar{y}_{i}-\bar{\bar{y}})^2 =\Sigma^J_{j=1}n_j(\bar{y}_{i}-\bar{\bar{y}})^2\)</span></p>
<ul>
<li><p>This is the variation in the group means around the grand mean based on the explanatory variable <span class="math inline">\(A\)</span>.</p></li>
<li><p>Also called sums of squares for the treatment, regression, or model.</p></li>
</ul></li>
<li><p><span class="math inline">\(\textbf{SS}_E =\)</span> Error (Residual) Sums of Squares <span class="math inline">\(=\Sigma^J_{j=1}\Sigma^{n_j}_{i=1}(y_{ij}-\bar{y})^2 =\Sigma^J_{j=1}\Sigma^{n_j}_{i=1}(e_{ij})^2\)</span></p>
<ul>
<li><p>This is the variation in the responses around the group means.</p></li>
<li><p>Also called the sums of squares for the residuals, with the second version of the formula showing that it is just the squared residuals added up across all the observations.</p></li>
</ul></li>
</ul>
<p>The possibly surprising result given the mass of notation just presented is that the total sums of squares is <strong>ALWAYS</strong> equal to the sum of explanatory variable <span class="math inline">\(A\text{&#39;s}\)</span> sum of squares and the error sums of squares,</p>
<p><span class="math display">\[\textbf{SS}_{\textbf{Total}} \mathbf{=} \textbf{SS}_\textbf{A} \mathbf{+} \textbf{SS}_\textbf{E}.\]</span></p>
</div>
<div id="section3-3" class="section level2">
<h2><span class="header-section-number">3.3</span> One-Way ANOVA Sums of Squares, Mean Squares, and F-test</h2>
</div>
<div id="section3-4" class="section level2">
<h2><span class="header-section-number">3.4</span> ANOVA model diagnostics including QQ-plots</h2>
</div>
<div id="section3-5" class="section level2">
<h2><span class="header-section-number">3.5</span> Guinea pig tooth growth One-Way ANOVA example</h2>
</div>
<div id="section3-6" class="section level2">
<h2><span class="header-section-number">3.6</span> Multiple (pair-wise) comparisons using Tukey’s HSD and the compact letter display</h2>
</div>
<div id="section3-7" class="section level2">
<h2><span class="header-section-number">3.7</span> Pair-wise comparisons for Prisoner Rating data</h2>
</div>
<div id="section3-8" class="section level2">
<h2><span class="header-section-number">3.8</span> Chapter Summary</h2>
</div>
<div id="section3-9" class="section level2">
<h2><span class="header-section-number">3.9</span> Summary of important R code</h2>
</div>
<div id="section3-10" class="section level2">
<h2><span class="header-section-number">3.10</span> Practice problems</h2>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="31">
<li id="fn31"><p>In Chapter <a href="chapter4.html#chapter4">4</a>, methods are discussed for when there are two categorical explanatory variables that is called the Two-Way ANOVA and related ANOVA tests are used in Chapter <a href="chapter8.html#chapter8">8</a> for working with extensions of these models.<a href="chapter3.html#fnref31">↩</a></p></li>
<li id="fn32"><p>Suppose we were doing environmental monitoring and were studying asbestos levels in soils. We might be hoping that the mean-only model were reasonable to use if the groups being compared were in remediated areas and in areas known to have never been contaminated.<a href="chapter3.html#fnref32">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="chapter2.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="chapter4.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": ["GreenwoodBanner.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
